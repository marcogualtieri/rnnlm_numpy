
gate engineering

in engineering a gate is a rotating or sliding structure supported by hinges or by a rotating horizontal or vertical axis that can be located at an extreme of a large pipe or canal in order to control the flow of water or any fluid from one side to the other it is usually placed at the mouth of irrigation channels to avoid water loss or at the end of drainage channels to elude water entrance

engineering statistics

engineering statistics combines engineering and statistics

design choice

in engineering a design choice is a possible solution to a problem given a design task and a governing set of criteria design specifications several conceptual designs may be drafted each of these preliminary concepts is a potential design choice many never advance beyond the preliminary phase those that are developed to the point where they could actually be applied become the pool from which the final selection is made this process stems from the principle that there is usually no uniquely right way of accomplishing any task the final selection is often made on a financial basis i e the least expensive design is chosen in a bid process
in civil engineering design choices usually derive from basic principles of materials science and structural design a suspension bridge for example uses the fact that steel is extremely efficient in tension while a prestressed concrete bridge takes advantage of concrete's relatively low cost by weight and its ability to sustain high compressive loading see compression

index of engineering articles

this page is a list of engineering topics
a
acceleration
adaptive control
aerodynamics
aerospace engineering
agricultural engineering
aircraft maintenance engineer
algorithm of inventive problems solving
ampere
amp re's law
analog circuit
analytical mechanics
applied engineering
architectural engineering
artificial intelligence
asymptotic stability
asynchronous circuit
automotive engineering
avalanche diode
b
base isolation
bending
bibo stability
biomechanics
biomedical engineering
bore gauge
brittle
buckling
buttress
biological engineering
c
cad
caid
calculus
calculus topics
caliper
capacitance
capacitor
centennial challenges
chamfer
chartered engineer
chemical engineering
chemistry
chemistry topics
civil engineering
classical limit
classical mechanics
clean room design
closed loop controller
coefficient of thermal expansion
compression physical
compressive strength
computational fluid dynamics
computer
computer aided design
computer aided industrial design
computer aided manufacturing
computer engineering
computer science
constraint
continuum mechanics
contradiction
control engineering
control theory
controllability
corrosion
cost
coulomb damping
coulomb's law
creep
critical path method
current
d
damping
damping ratio
decision tree
deformation
design
diac
dial indicator
digital circuit
dimensionless number
diode
displacement current
distillation
dynamics
dyne
e
earthquake engineering
elasticity
electric charge
electric current
electric field
electric motor
electric potential
electrical circuit
electrical engineering
electrical circuit
electrical network
electrical resistance
electricity
electrodynamics
electromagnetic field
electromagnetic induction
electromagnetic radiation
electromagnetic spectrum
electromagnetism
electromotive force
electronic amplifier
electronic circuit
electronics
electrostatics
engine
engineer
engineer's scale
engineering
engineering design process
engineering drawing
engineering economics
engineering ethics
engineering management
engineering science and mechanics
engineering society
engineering statistics
environmental engineering
exploratory engineering
external electric load
f
factor of safety
faraday lenz law
faraday's law of induction
fast fracture
fatigue
feasibility study
fields of engineering
finite element analysis
fluid dynamics
force
force density
fourier series
fourier theorem
fourier transform
fracture
frequency
friction
fundamentals of engineering exam
g
gauge
gauss's law
general relativity
gravity
h
hall effect sensor
hamilton jacobi equation
heat transfer
hertz
hooke's law
hydraulic press
hydraulics
hydrostatics
i
ideal final result
impedance
inductance
inductor
industrial engineering
instrumentation
integrated circuit
intelligent control
ieee
isolation transformer
international conference on green computing and engineering technology
k
kelvin
kilogram force
kinematics
kirchhoff's circuit laws
knurling
l
laser diode
law of universal gravitation
liability
life cycle cost analysis
light emitting diode
lorentz force law
m
machine
management
magnetic circuit
magnetic field
magnetic flux
magnetic moment
magnetism
magnetostatics
margin of safety
marginal stability
mass transfer
materials
materials engineering
materials science
mathematics articles
mathematics
maxwell's equations
measurement
mechanical engineering
mechanics
mechatronics
megascale engineering
melting
metallography
microcontroller
micromachinery
microprocessor
modulus of elasticity
moment
n
nanoengineering
nanotechnology
negative feedback
new product development
nonlinear control
nuclear engineering
o
observability
ohm's law
operational amplifier
optical spectrum
optimal control
optimization computer science
overall equipment effectiveness
p
petroleum engineering
phase angle
photometer
physics
physics topics
pid controller
pin diode
piston
plasticity
plumb bob
plumb line
pneumatics
plm
poisson's ratio
pound force
poundal
positive feedback
potential difference
pressure
printed circuit
process
process control
product lifecycle management
professional engineer
project management
q
quality
quality control
quantum hydrodynamics
quantum mechanics
r
reaction kinetics
reliability
resistor
resonant cavity
reverse engineering
rheology
robotics
rotating
s
seismic loading
seismic performance
semiconductor
series and parallel circuits
schottky diode
shear strength
shear stress
si units
signal processing
silicon controlled rectifier
simple machine
simulation
software
software engineering
solid mechanics
solid modeling
sports engineering
state observer
statics
stress strain curve
structural load
student design competition
superconductivity
superfluid
switch
synchronous circuit
systems engineering
t
technical drawing
technology
tensile strength
tensile stress
theory of elasticity
theory of inventive problems solving
thermal shock
thermodynamics
thyristor
time and methods engineering
torque
toughness
transformer
transils
transistor
turbine
u
uncertainty principle
unreinforced masonry building
v
validation
varistor
vector
vibration control
volt
w
washington accord
waveguide
wear
wind engineering
welding metallurgy
y
y delta transform
yield strength
young's modulus
z
zener diode

outline of engineering

the following outline is provided as an overview of and topical guide to engineering
engineering discipline art skill and profession of acquiring and applying scientific mathematical economic social and practical knowledge in order to design and build structures machines devices systems materials and processes that safely realize improvements to the lives of people
branches
aerospace engineering
aerospace engineering is the branch of engineering behind the design construction and science of aircraft and spacecraft it is broken into two major and overlapping branches aeronautical engineering and astronautical engineering the former deals with craft that stay within earth's atmosphere and the latter deals with craft that operates outside of earth's atmosphere
applied engineering
applied engineering is the application of management design and technical skills for the design and integration of systems the execution of new product designs the improvement of manufacturing processes and the management and direction of physical and or technical functions of a firm or organization
planetary engineering climate engineering geoengineering
planetary engineering is the application of technology for the purpose of influencing the global properties of a planet the goal of this theoretical task is usually to make other worlds habitable for life
perhaps the best known type of planetary engineering is terraforming by which a planet's surface conditions are altered to be more like those of earth other types of planetary engineering include ecopoiesis the introduction of an ecology to a lifeless environment planetary engineering is largely the realm of science fiction at present although some types of climate change on earth are recent evidence that humans can cause change on a global scale
systems engineering
systems engineering covers analysis design and control of engineering systems it focuses on the science and technology of industrial systems it emphasizes the analysis and design of systems to produce goods and services efficiently two unique features set systems engineering apart from other engineering disciplines the particular attention devoted to both the physical processes involved and to the decision making components of the industrial environment and the wide scope applicability of its systems methodology not limited to manufacturing industries but effectively used in all kinds of business organizations
textile engineering
the textile engineering program is accredited by abet inc textile engineering courses deal with the application of scientific and engineering principles to the design and control of all aspects of fiber textile and apparel processes products and machinery these include natural and man made materials interaction of materials with machines safety and health energy conservation and waste and pollution control additionally students are given experience in plant design and layout machine and wet process design and improvement and designing and creating textile products throughout the textile engineering curriculum students take classes from other engineering and disciplines including mechanical chemical materials and industrial engineering departments

hydrostatic test

a hydrostatic test is a way in which pressure vessels such as pipelines plumbing gas cylinders boilers and fuel tanks can be tested for strength and leaks the test involves filling the vessel or pipe system with a liquid usually water which may be dyed to aid in visual leak detection and pressurization of the vessel to the specified test pressure pressure tightness can be tested by shutting off the supply valve and observing whether there is a pressure loss the location of a leak can be visually identified more easily if the water contains a colorant strength is usually tested by measuring permanent deformation of the container hydrostatic testing is the most common method employed for testing pipes and pressure vessels using this test helps maintain safety standards and durability of a vessel over time newly manufactured pieces are initially qualified using the hydrostatic test they are then re qualified at regular intervals using the proof pressure test which is also called the modified hydrostatic test testing of pressure vessels for transport and storage of gases is very important because such containers can explode if they fail under pressure
testing procedures
hydrostatic tests are conducted under the constraints of either the industry's or the customer's specifications or may be required by law the vessel is filled with a nearly incompressible liquid usually water or oil and examined for leaks or permanent changes in shape red or fluorescent dyes may be added to the water to make leaks easier to see the test pressure is always considerably higher than the operating pressure to give a factor of safety this factor of safety is typically or of the designed working pressure depending on the regulations that apply for example if a cylinder was rated to dot psi approximately bar it would be tested at around psi approximately bar water is commonly used because it is cheap and easily available and is usually harmless to the system to be tested hydraulic fluids and oils may be specified where contamination with water could cause problems these fluids are nearly incompressible therefore requiring relatively little work to develop a high pressure and is therefore also only able to release a small amount of energy in case of a failure only a small volume will escape under high pressure if the container fails if high pressure gas were used then the gas would expand to v nrt p with its compressed volume resulting in an explosion with the attendant risk of damage or injury this is the risk which the testing is intended to mitigate
small pressure vessels are normally tested using a water jacket test the vessel is visually examined for defects and then placed in a container filled with water and in which the change in volume of the vessel can be measured usually by monitoring the water level in a calibrated tube the vessel is then pressurized for a specified period usually or more seconds and if specified the expansion will be measured by reading off the amount of liquid the has been forced into the measuring tube by the volume increase of the pressurized vessel the vessel is then depressurized and the permanent volume increase due to plastic deformation while under pressure is measured by comparing the final volume in the measuring tube with the volume before pressurization a leak will give a similar result to permanent set but will be detectable by holding the volume in the pressurized vessel by closing the inlet valve for a period before depressurizing as the pressure will drop steadily during this period if there is a leak in most cases a permanent set that exceeds the specified maximum will indicate failure a leak may also be a failure criterion but it may be that the leak is due to poor sealing of the test equipment if the vessel fails it will normally go through a condemning process marking the cylinder as unsafe
the information needed to specify the test is stamped onto the cylinder this includes the design standard serial number manufacturer and manufacture date other information is stamped as needed such as the ree or how much the manufacturing standard specifies the cylinder may expand before it is considered unsafe after testing the vessel or its nameplate will usually be stamp marked with the date of the successful test and the test facility's identification mark
a simpler test that is also considered a hydrostatic test but can be performed by anyone who has a garden hose is to pressurize the vessel by filling it with water and to physically examine the outside for leaks this type of test is suitable for containers such as boat fuel tanks which are not pressure vessels but must work under the hydrostatic pressure of the contents a hydrostatic test head is usually specified as a height above the tank top the tank is pressurized by filling water to the specified height through a temporary standpipe if necessary it may be necessary to seal vents and other outlets during the test
examples
portable fire extinguishers are safety tools that are required to be on hand in almost every public building fire extinguishers are also highly recommended in every home over time the conditions in which they are housed and the manner in which they are handled have an impact on the structural integrity of the extinguisher a structurally weakened fire extinguisher can malfunction or even burst when it is needed the most to maintain the quality and safety of this product hydrostatic testing is utilized all critical components of the fire extinguisher should be tested to ensure proper function the cylinder is generally tested by using the water jacket test
as previously mentioned the water pressure inside the tank will usually be of the normal operating pressure the change in volume of the cylinder is calculated by measuring the change in the water levels outside the cylinder this can be done with a digital scale capable of detecting the slightest changes almost always in grams in addition the cylinder can also be visually checked for leaks or the pressure drop method can be utilized to measure the overall efficiency of the cylinder
pipeline testing
hydrotesting of pipes pipelines and vessels is performed to expose defective materials that have missed prior detection ensure that any remaining defects are insignificant enough to allow operation at design pressures expose possible leaks and serve as a final validation of the integrity of the constructed system asme b requires this testing to ensure tightness and strength
buried high pressure oil and gas pipelines are tested for strength by pressurizing them to at least of their maximum allowable operating pressure maop at any point along their length since many long distance transmission pipelines are designed to have a steel hoop stress of of specified minimum yield strength smys at maop this means that the steel is stressed to smys and above during the testing and test sections must be selected to ensure that excessive plastic deformation does not occur
test pressures need not exceed a value that would produce a stress higher than yield stress at test temperature asme b section c
other codes require a more onerous approach bs pd requires testing to of the design pressure which should not be less than the maop plus surge and other incidental effects that will occur during normal operation
leak testing is performed by balancing changes in the measured pressure in the test section against the theoretical pressure changes calculated from changes in the measured temperature of the test section
australian standard as pipelines gas and liquid petroleum part field pressure testing gives an excellent explanation of the factors involved
testing frequency
most countries have legislation or building code that requires pressure vessels to be regularly tested for example every two years with a visual inspection annually for high pressure gas cylinders and every five or ten years for lower pressure ones such as used in fire extinguishers gas cylinders which fail are normally destroyed as part of the testing protocol to avoid the dangers inherent in them being subsequently used
these common gas cylinders have the following requirements
typically organizations such as iso astm and asme specify the guidelines for the different types of pressure vessels

building engineering physics

the term building engineering physics was introduced in a report released in january commissioned by the royal academy of engineering the report entitled engineering a low carbon built environment the discipline of building engineering physics presents the initiative of many at the royal academy of engineering in developing a field that addresses our fossil fuel dependence while working towards a more sustainably built environment for the future
the field of building engineering physics combines the existing professions of building services engineering applied physics and building construction engineering into a single field designed to investigate the energy efficiency of old and new buildings the application of building engineering physics allows the construction and renovation of high performance energy efficient buildings while minimizing their environmental impacts
building engineering physics addresses several different areas in building performance including air movement thermal performance control of moisture ambient energy acoustics light climate and biology this field employs creative ways of manipulating these principal aspects of a building s indoor and outdoor environments so that a more eco friendly standard of living is obtained building engineering physics is unique from other established applied sciences or engineering professions as it combines the sciences of architecture engineering and human biology and physiology building engineering physics not only addresses energy efficiency and building sustainability but also a building's internal environment conditions that affect the comfort and performance levels of its occupants
throughout the th century a large percentage of buildings were constructed completely dependent on fossil fuels rather than focusing on energy efficiency architects and engineers were more concerned with experimenting with new materials and structural forms to further aesthetic ideals now in the st century building energy performance standards are pushing towards a zero carbon standard in old and new buildings alike the threat of global change and the need for energy independence and sustainability has prompted governments across the globe to adopt firm carbon reducing standards a significant way to meet these stringent standards is in the construction of buildings that minimize environmental impacts as well as the refurbishing of older buildings to meet carbon emission standards the application of building engineering physics can aid in this transition to reduce energy dependent buildings provide for the demands of a growing population and better standard of living growth in the application of this field is due in large part to the introduction of regulations requiring the calculation of carbon emissions to demonstrate compliance principally the energy performance of buildings directive epbd
unfortunately the discipline of building engineering physics is severely impaired by lack of diverse education in the construction industry very few in the construction industry know how to apply the principles of building engineering physics and do not have interdisciplinary experience

digital materialization

digital materialization dm
can loosely be defined as two way direct communication or conversion between matter and information that enables people to exactly describe monitor manipulate and create any arbitrary real object dm is a general paradigm alongside a specified framework that is suitable for computer processing and includes holistic coherent volumetric modeling systems symbolic languages that are able to handle infinite degrees of freedom and detail in a compact format and the direct interaction and or fabrication of any object at any spatial resolution without the need for lossy or intermediate formats
dm systems possess the following attributes
such an approach can not only be applied to tangible objects but can include the conversion of things such as light and sound to from information and matter systems to digitally materialize light and sound already largely exist now e g photo editing audio mixing etc and have been quite effective but the representation control and creation of tangible matter is poorly support by computational and digital systems
commonplace computer aided design and manufacturing systems currently represent real objects as dimensional shells in contrast dm proposes a deeper understanding and sophisticated manipulation of matter by directly using rigorous mathematics as complete volumetric descriptions of real objects by utilizing technologies such as function representation frep it becomes possible to compactly describe and understand the surface and internal structures or properties of an object at an infinite resolution thus models can accurately represent matter across all scales making it possible to capture the complexity and quality of natural and real objects and ideally suited for digital fabrication and other kinds of real world interactions dm surpasses the previous limitations of static disassociated languages and simple human made objects to propose systems that are heterogeneous interacting directly and more naturally with the complex world
digital and computer based languages and processes unlike the analogue counterparts can computationally and spatially describe and control matter in an exact constructive and accessible manner however this requires approaches that can handle the complexity of natural objects and materials

fractography

fractography is the study of the fracture surfaces of materials fractographic methods are routinely used to determine the cause of failure in engineering structures especially in product failure and the practice of forensic engineering or failure analysis in material science research fractography is used to develop and evaluate theoretical models of crack growth behavior
one of the aims of fractographic examination is to determine the cause of failure by studying the characteristics of a fractured surface different types of crack growth e g fatigue stress corrosion cracking hydrogen embrittlement produce characteristic features on the surface which can be used to help identify the failure mode the overall pattern of cracking can be more important than a single crack however especially in the case of brittle materials like ceramics and glasses
methods
an important aim of fractography is to establish and examine the origin of cracking as examination at the origin may reveal the cause of crack initiation initial fractographic examination is commonly carried out on a macro scale utilising low power optical microscopy and oblique lighting techniques to identify the extent of cracking possible modes and likely origins optical microscopy or macrophotography are often enough to pinpoint the nature of the failure and the causes of crack initiation and growth if the loading pattern is known
common features that may cause crack initiation are inclusions voids or empty holes in the material contamination and stress concentrations hachures are the lines on fracture surfaces which show crack direction the broken crankshaft shown at right failed from a surface defect near the bulb at lower centre the single brittle crack growing up into the bulk material by small steps a problem known as fatigue the crankshaft also shows hachures which point back to the origin of the fracture some modes of crack growth can leave characteristic marks on the surface that identify the mode of crack growth and origin on a macro scale e g beachmarks or striations on fatigue cracks the areas of the product can also be very revealing especially if there are traces of sub critical cracks or cracks which have not grown to completion they can indicate that the material was faulty when loaded or alternatively that the sample was overloaded at the time of failure
a cusp is formed where brittle cracks meet as shown on the picture of a failed catheter cp the cusp was formed by brittle failure of the catheter on a breast implant in silicone rubber the origin of the cracks is at the shoulder at the left hand side identifying such features will allow a fracture surface map to be made of the surface being studied the implant failed because of overload all the imposed loads being concentrated at the connection between the catheter and the bag holding salt solution as a result the patient reported loss of fluid from the implant and it was extracted surgically and replaced
usb microscopy
usb microscopes are especially useful for examining fracture surface features since they are small enough to be hand held a variety of camera sizes and resolution are available commercially at low cost the camera cable plugs into the computer via a usb plug and most such devices come with illumination at the camera supplied by led lights
fracture surface map
a schematic fracture surface map is a valuable result of visual or microscopic examination it seeks to isolate and identify the features on the surface which show how the product failed such a map can be a valuable way of presenting information which shows clearly how a crack was initiated and grew with time in the case of the failed breast implant catheter the crack path was very simple but the cause more subtle further scanning electron microscopy showed numerous microcracks between the bag and the catheter indicating that the adhesive bond between the two components had failed prematurely perhaps through faulty manufacture the material of construction of both bag and catheter silicone rubber is a physically weak elastomer and product design must allow for the low tear or shear strength of the material
scanning electron microscopy
in many cases fractography requires examination at a finer scale which is usually carried out in a scanning electron microscope or sem the resolution is much higher than the optical microscope although samples are examined in a partial vacuum and colour is absent improved sem's now allow examination at near atmospheric pressures so allowing examination of sensitive materials such as those of biological origin
the sem is especially useful when combined with energy dispersive x ray spectroscopy or edx which can be performed in the microscope so very small areas of the sample can be analysed for their elemental composition
applications
fractography is a widely used technique in forensic engineering forensic materials engineering and fracture mechanics to understand the causes of failures and also to verify theoretical failure predictions with real life failures it is of use in forensic science for analysing broken products which have been used as weapons such as broken bottles for example thus a defendant might claim that a bottle was faulty and broke accidentally when it impacted a victim of an assault fractography could show the allegation to be false and that considerable force was needed to smash the bottle before using the broken end as a weapon to deliberately attack the victim bullet holes in glass windscreens or windows can also indicate the direction of impact and the energy of the projectile in these cases the overall pattern of cracking is vital to reconstructing the sequence of events rather than the specific characteristics of a single crack fractography can determine whether a cause of train derailment was a faulty rail or if a wing of a plane had fatigue cracks before a crash
fractography is used also in materials research since fracture properties can correlate with other properties and with structure of materials

electrical system design

electrical system design is the design of electrical systems this can be as simple as a flashlight cell connected through two wires to a light bulb or as involved as the space shuttle electrical systems are groups of electrical components connected to carry out some operation often the systems are combined with other systems they might be subsystems of larger systems and have subsystems of their own for example a subway rapid transit electrical system is composed of the wayside electrical power supply wayside control system and the electrical systems of each transit car each transit car s electrical system is a subsystem of the subway system inside of each transit car there are also subsystems such as the car climate control system
records
simple systems can be created by one person and require minimal records large systems require large teams of people and many paper and electronic records on larger projects a company would have an electrical systems engineering department
with larger systems if proper records are not kept the construction is not neat or care is not taken to follow appropriate standards such as those of the national electrical code the created electrical system is not likely to function as desired to paraphrase a manager s description of a moderate sized electrical system that his company haphazardly created the wiring looked like spaghetti it caught fire when power was applied we settled with the customer and quit the project
design
the following would be appropriate for the design of a moderate to large electrical system

engineering duty officer

an engineering duty officer is a restricted line officer in the united states navy involved with the design acquisition construction repair maintenance conversion overhaul or disposal of ships submarines aircraft carriers and the systems on those platforms weapons command and control communications computers etc as of november there are approximately engineering duty officers on active duty in the united states navy representing approximately percent of its active duty commissioned officers
mission
the engineering duty officer community leadership has stated that the purpose of the engineering duty officer community is to provide experienced naval engineers known for bringing effective technical and business solutions in support of naval power respected for integrity adaptability and agility engineering duty officers ensure that our naval and joint forces operate and fight with the most capable platforms possible we are involved with the design acquisition construction repair maintenance conversion overhaul and disposal of ships submarines aircraft carriers and the systems on those platforms weapons command and control communications computers etc engineering duty officers are unique to the navy because we all start our career as url officers first we learn how to operate ships or submarines next all eds obtain technical engineering master's degrees then we combine that operational experience and technical knowledge to become the technical business leaders for the navy
insignia
as line officers of the navy engineering duty officers wear an inverted gold star above their rank stripes on both their dress blue uniforms and on their shoulder boards in virtually all respects their uniforms are indistinguishable from their url counterparts the two predominant sources of new engineering duty officers are by lateral transfer from another designator or by choosing to exercise an engineering duty option granted upon commissioning a requirement for lateral transfer or for exercising an engineering duty option is the completion of either submarine warfare or surface warfare qualification therefore the vast majority of engineering duty officers wear either the same submarine warfare or surface warfare insignia as their url counterparts
a small number of engineering duty officers not previously qualified as submarine warfare officers volunteer for the engineering duty dolphin program and by successfully completing it earn their submarine engineering duty insignia the submarine engineering duty insignia is the only united states navy insignia which is unique to engineering duty officers
areas of specialization
current engineering duty officers serve in one of several career fields including
history
the importance of engineering duty officers in united states navy history is memorialized in a bronze bas relief by american sculptor antonio tobias toby mendez on the sculpture wall at the united states navy memorial in washington dc entitled engineering duty officers sharpening the point of the spear this is one of such reliefs along southern hemisphere of the granite sea at the navy memorial which commemorate events personnel and communities of the various sea services

enhanced acoustic simulator for engineers

enhanced acoustic simulator for engineers ease is an engineering design and analysis software for optimizing acoustics the full product is licensed and copy protected it can perform complex analysis in three dimensional space there is a free to use web based version available for two dimensional analysis with limited geometry options

reliability centered maintenance

reliability centered maintenance rcm is a process to ensure that systems continue to do what their users require in their present operating context it is generally used to achieve improvements in fields such as the establishment of safe minimum levels of maintenance successful implementation of rcm will lead to increase in cost effectiveness reliability machine uptime and a greater understanding of the level of risk that the organization is managing it is defined by the technical standard sae ja evaluation criteria for rcm processes article
rcm is a process to ensure that assets continue to do what their users require in their present operating context
context
it is generally used to achieve improvements in fields such as the establishment of safe minimum levels of maintenance changes to operating procedures and strategies and the establishment of capital maintenance regimes and plans successful implementation of rcm will lead to increase in cost effectiveness machine uptime and a greater understanding of the level of risk that the organization is managing
the late john moubray in his book rcm characterized reliability centered maintenance as a process to establish the safe minimum levels of maintenance this description echoed statements in the nowlan and heap report from united airlines
it is defined by the technical standard sae ja evaluation criteria for rcm processes which sets out the minimum criteria that any process should meet before it can be called rcm this starts with the seven questions below worked through in the order that they are listed
reliability centered maintenance is an engineering framework that enables the definition of a complete maintenance regime it regards maintenance as the means to maintain the functions a user may require of machinery in a defined operating context as a discipline it enables machinery stakeholders to monitor assess predict and generally understand the working of their physical assets this is embodied in the initial part of the rcm process which is to identify the operating context of the machinery and write a failure mode effects and criticality analysis fmeca the second part of the analysis is to apply the rcm logic which helps determine the appropriate maintenance tasks for the identified failure modes in the fmeca once the logic is complete for all elements in the fmeca the resulting list of maintenance is packaged so that the periodicities of the tasks are rationalised to be called up in work packages it is important not to destroy the applicability of maintenance in this phase lastly rcm is kept live throughout the in service life of machinery where the effectiveness of the maintenance is kept under constant review and adjusted in light of the experience gained
rcm can be used to create a cost effective maintenance strategy to address dominant causes of equipment failure it is a systematic approach to defining a routine maintenance program composed of cost effective tasks that preserve important functions
the important functions of a piece of equipment to preserve with routine maintenance are identified their dominant failure modes and causes determined and the consequences of failure ascertained levels of criticality are assigned to the consequences of failure some functions are not critical and are left to run to failure while other functions must be preserved at all cost maintenance tasks are selected that address the dominant failure causes this process directly addresses maintenance preventable failures failures caused by unlikely events non predictable acts of nature etc will usually receive no action provided their risk combination of severity and frequency is trivial or at least tolerable when the risk of such failures is very high rcm encourages and sometimes mandates the user to consider changing something which will reduce the risk to a tolerable level
the result is a maintenance program that focuses scarce economic resources on those items that would cause the most disruption if they were to fail
rcm emphasizes the use of predictive maintenance pdm techniques in addition to traditional preventive measures
background
the term reliability centered maintenance was first used in public papers authored by tom matteson stanley nowlan howard heap and other senior executives and engineers at united airlines ual to describe a process used to determine the optimum maintenance requirements for aircraft having left united airlines to pursue a consulting career a few months before the publication of the final nowlan heap report matteson received no authorial credit for the work however his contributions were substantial and perhaps indispensable to the document as a whole the us department of defense dod sponsored the authoring of both a textbook by ual and an evaluation report by rand corporation on reliability centered maintenance both published in they brought rcm concepts to the attention of a wider audience the text book described efforts by commercial airlines and the us navy in the s and s to improve the reliability of their new jet the boeing
the first generation of jet aircraft had a crash rate that would be considered highly alarming today and both the federal aviation administration faa and the airlines senior management felt strong pressure to improve matters in the early s with faa approval the airlines began to conduct a series of intensive engineering studies on in service aircraft the studies proved that the fundamental assumption of design engineers and maintenance planners that every airplane and every major component in the airplane such as its engines had a specific lifetime of reliable service after which it had to be replaced or overhauled in order to prevent failures was wrong in nearly every specific example in a complex modern jet airliner
this was one of many astounding discoveries that have revolutionized the managerial discipline of physical asset management and have been at the base of many developments since this seminal work was published among some of the paradigm shifts inspired by rcm were
today rcm is defined in the standard sae ja evaluation criteria for reliability centered maintenance rcm processes this sets out the minimum criteria for what is and for what is not able to be defined as rcm
the standard is a watershed event in the ongoing evolution of the discipline of physical asset management prior to the development of the standard many processes were labeled as rcm even though they were not true to the intentions and the principles in the original report that defined the term publicly
today companies can use this standard to ensure that the processes services and software they purchase and implement conforms with what is defined as rcm ensuring the best possibility of achieving the many benefits attributable to rigorous application of rcm
basic features
the rcm process described in the dod ual report recognized three principal risks from equipment failures threats
modern rcm gives threats to the environment a separate classification though most forms manage them in the same way as threats to safety
rcm offers five principal options among the risk management strategies
rcm also offers specific criteria to use when selecting a risk management strategy for a system that presents a specific risk when it fails some are technical in nature can the proposed task detect the condition it needs to detect does the equipment actually wear out with use others are goal oriented is it reasonably likely that the proposed task and task frequency will reduce the risk to a tolerable level the criteria are often presented in the form of a decision logic diagram though this is not intrinsic to the nature of the process
identification of safety critical elements sce and maintaining associated pre defined performance standards is the foundation of asset integrity management
in use
after being created by the commercial aviation industry rcm was adopted by the u s military beginning in the mid s and by the u s commercial nuclear power industry in the s
starting in the late s an independent initiative led by john moubray corrected some early flaws in the process and adapted it for use in the wider industry john was also responsible for popularizing the method and for introducing it to much of the industrial community outside of the aviation industry rcm
in the two decades since rcm was first released industry has undergone massive change increased economic pressures and competition tied with advances in lean thinking and efficiency methods meant that companies often struggled to find the people required to carry out an rcm initiative
at this point in time many methods sprung up that took an approach of reducing the rigour of the rcm approach the result was the propagation of many methods that called themselves rcm yet had little in common with the original concepts in some cases these were misleading and inefficient while in other cases they were even dangerous
since each initiative is sponsored by one or more consulting firms eager to help clients use it there is still considerable disagreement about their relative dangers or merits also there is a tendency for consulting firms to promote a software package as an alternative methodology in place of the knowledge required to perform analyses
the rcm standard available from http www sae org provides the minimum criteria that processes must comply with if they are to be called rcm
although a voluntary standard it provides a reference for companies looking to implement rcm to ensure they are getting a process software package or service that is in line with the original report
disney introduced rcm to its parks in led by paul pressler and consultants mckinsey company laying off a large number of maintenance workers and saving large amounts of money some people blamed the new cost conscious maintenance culture for some of the incidents at disneyland resort that occurred in the following years

interoperation

in engineering interoperation is the setup of ad hoc components and methods to make two or more systems work together as a combined system with some partial functionality during a certain time possibly requiring human supervision to perform necessary adjustments and corrections
this contrasts to interoperability which theoretically permits any number of systems compliant to a given standard to work together a long time smoothly and unattended as a combined system with the full functionality by the standard
another definition of interoperation services effectively combining multiple resources and domains requires interoperability
usage
interoperation is usually performed when the systems having to be combined were designed before standardization for example legacy systems or when standard compliance is too expensive too difficult or immature
interoperation may use following mechanisms components and methods
in the area of data processing interoperation may also use following components and methods

superficial velocity

superficial velocity or superficial flow velocity in engineering of multiphase flows and flows in porous media is a hypothetical artificial flow velocity calculated as if the given phase or fluid were the only one flowing or present in a given cross sectional area other phases particles the skeleton of the porous medium etc present in the channel are disregarded
superficial velocity is used in many engineering equations because it is the value which is usually readily known and unambiguous whereas real velocity is often variable from place to place its mean not readily available in complex flow systems and subject to assumptions
superficial velocity can be expressed as
where
using the concept of porosity the dependence between the advection velocity and the superficial velocity can be expressed as for one dimensional flow
where
the local physical velocity can still be different than the advection velocity because the vector of the local fluid flow does not have to be parallel to that of average flow also there may be local constriction in the flow channel

remote laboratory

remote laboratory also known as online laboratory remote workbench is the use of telecommunications to remotely conduct real as opposed to virtual experiments at the physical location of the operating technology whilst the scientist is utilizing technology from a separate geographical location remote laboratory comprehends one or more remote experiments
benefits
the benefits of remote laboratories are predominantly in engineering education
researchers from the labshare describe the advantages as being
this allows for economies of scale production
another benefit is that this technology can be integrated into moodle which is probably the most used learning management system around the world
disadvantages
the disadvantages differ depending on the type of remote laboratory and the topic area
the general disadvantages compared to a proximal hands on laboratory are
future direction
current system capabilities include

elegant degradation

elegant degradation is a term used in engineering to describe what occurs to machines which are subject to constant repetitive stress
externally such a machine maintains the same appearance to the user appearing to function properly internally the machine slowly weakens over time eventually unable to withstand the stress it breaks down compared to graceful degradation the operational quality does not decrease at all but the breakdown may be just as sudden
this term's meaning varies depending on context and field and may not be strictly considered exclusive to engineering

statistical interference


when two probability distributions overlap statistical interference exists knowledge of the distributions can be used to determine the likelihood that one parameter exceeds another and by how much
this technique can be used for dimensioning of mechanical parts determining when an applied load exceeds the strength of a structure and in many other situations this type of analysis can also be used to estimate the probability of failure or the frequency of failure
dimensional interference
mechanical parts are usually designed to fit precisely together for example if a shaft is designed to have a sliding fit in a hole the shaft must be a little smaller than the hole traditional tolerances may suggest that all dimensions fall within those intended tolerances a process capability study of actual production however may reveal normal distributions with long tails both the shaft and hole sizes will usually form normal distributions with some average arithmetic mean and standard deviation
with two such normal distributions a distribution of interference can be calculated the derived distribution will also be normal and its average will be equal to the difference between the means of the two base distributions the variance of the derived distribution will be the sum of the variances of the two base distributions
this derived distribution can be used to determine how often the difference in dimensions will be less than zero i e the shaft cannot fit in the hole how often the difference will be less than the required sliding gap the shaft fits but too tightly and how often the difference will be greater than the maximum acceptable gap the shaft fits but not tightly enough
physical property interference
physical properties and the conditions of use are also inherently variable for example the applied load stress on a mechanical part may vary the measured strength of that part tensile strength etc may also be variable the part will break when the stress exceeds the strength
with two normal distributions the statistical interference may be calculated as above this problem is also workable for transformed units such as the log normal distribution with other distributions or combinations of different distributions a monte carlo method or simulation is often the most practical way to quantify the effects of statistical interference

humanitarian engineering

humanitarian engineering is research and design to directly improve the well being of poor marginalized or under served communities which often lack the means to address pressing problems
training for one who participates in humanitarian engineering incorporates history politics economics sociology language as well as rigorous engineering basics several universities in the united states focus efforts on humanitarian engineering penn state university integrates engineering design and research with a strong social entrepreneurship thrust colorado school of mines offers a minor in humanitarian engineering the ohio state university which also offers a minor in the field has many local and international service projects courses and research in humanitarian engineering arizona state university offers a course humanitarian engineering entrepreneurship capstone as a part of the globalresolve program

compensation engineering

in engineering compensation is planning for side effects or other unintended issues in a design in a more simpler term it's a counter procedure plan on expected side effect performed to produce more efficient and useful results the design of an invention can itself also be to compensate for some other existing issue or exception
one example is in a voltage controlled crystal oscillator vcxo which is normally affected not only by voltage but to a lesser extent by temperature a temperature compensated version a tcvcxo is designed so that heat buildup within the enclosure of a transmitter or other such device will not alter the piezoelectric effect thereby causing frequency drift
another example is motion compensation on digital cameras and video cameras which keep a picture steady and not blurry
other examples in electrical engineering include
there are also examples in civil engineering

sqep

sqep is an acronym standing for suitably qualified and experienced person it is usually used to designate
this overall route is usually well documented to satisfy external regulatory authorities that appropriate personnel are in place along the supply chain to undertake critical responsibilities
the term has gained some currency in the uk nuclear power industry see for example from the health and safety executive

design review

a design review is a milestone within a product development process whereby a design is evaluated against its requirements in order to verify the outcomes of previous activities and identify issues before committing to and if need to be re prioritise further work the ultimate design review if successful therefore triggers the product launch or product release
the conduct of design reviews is compulsory as part of design controls when developing products in certain regulated contexts such as medical devices
by definition a review must include persons who are external to the design team
contents of a design review
in order to evaluate a design against its requirements a number of means may be considered such as
timing of design reviews
most formalised systems engineering processes recognise that the cost of correcting a fault increases as it progresses through the development process additional effort spent in the early stages of development to discover and correct errors is therefore likely to be worthwhile design reviews are example of such an effort
therefore a number of design reviews may be carried out for example to evaluate the design against different sets of criteria consistency usability ease of localisation environmental or during various stages of the design process
see also

mechanobiology

mechanobiology is an emerging field of science at the interface of biology and engineering it focuses on the way that physical forces and changes in cell or tissue mechanics contribute to development physiology and disease a major challenge in the field is understanding mechanotransduction the molecular mechanism by which cells sense and respond to mechanical signals
while medicine has typically looked for the genetic basis of disease advances in mechanobiology suggest that changes in cell mechanics extracellular matrix structure or mechanotransduction may contribute to the development of many diseases including atherosclerosis fibrosis asthma osteoporosis heart failure and cancer there is also a strong mechanical basis for many generalized medical disabilities such as lower back pain foot and postural injury deformity and irritable bowel syndrome
the effectiveness of many of the mechanical therapies already in clinical use shows how important physical forces can be in physiological control for example pulmonary surfactant promotes lung development in premature infants modifying the tidal volumes of mechanical ventilators reduces morbidity and death in patients with acute lung injury expandable stents physically prevent coronary artery constriction tissue expanders increase the skin area available for reconstructive surgery and surgical tension application devices are used for bone fracture healing orthodontics cosmetic breast expansion and closure of non healing wounds
insights into the mechanical basis of tissue regulation may also lead to development of improved medical devices biomaterials and engineered tissues for tissue repair and reconstruction
stretch activated ion channels caveolae integrins cadherins growth factor receptors myosin motors cytoskeletal filaments nuclei extracellular matrix and numerous other molecular structures and signaling molecules have been shown to contribute to cellular mechanotransduction in addition endogenous cell generated traction forces contribute significantly to these responses by modulating tensional prestress within cells tissues and organs that govern their mechanical stability as well as mechanical signal transmission from the macroscale to the nanoscale
on a macroscopic level mechanobiology is poorly evidenced and remains mostly theoretical experimental and computational using mankind as an example in closed chain function ground reactive forces a dynamic architecture and a dynamic equilibrium of forces around joint axes impact the posture to produce tissue stress this tissue stress can be both beneficial or harmful since gravity hard unyielding ground surfaces and other factors such as activity level body weight and health state impact each of us differently there is no one plan of care that will work for every individual this results in a lifetime of adaptation of tissues via wolff's and davis laws of bone and soft tissue respectively that can unless compensated and or corrected lead to breakdown injury and reduced quality of life on a case to case basis
foundationally as a further example the foot has an inherited functional foot type which when weighted will remodel and adapt in predictable manners once foot typed practitioners can interject foot centering orthotics muscle engine reactive forces and orthotic reactive forces non operatively to position and engineer the foot in order to treat pain syndromes deformity degeneration and quality of life issues theoretically programs can be establishing for prevention performance enhancement and quality of life upgrading in addition to the treatment of pathology and pain these interventions some day will cause positive remodeling of bone and soft tissue that will extend and possibly improve the mechanobiological timeline of mankind
eventually evidence will surface that will lead to paradigms of mechanobiological diagnosis treatment maintenance and upgrading that will benefit various biological systems until then human mechanobiology remains an intrapersonal medical architectural and engineering field that requires a professional practitioner

nadal formula

the nadal formula also called nadal's formula is an equation in railway design that relates the downward force exerted by a train's wheels upon the rail with the lateral force of the wheel's flange against the face of the rail this relationship is significant in railway design as a wheel climb derailment may occur if the lateral and vertical forces are not properly considered
the nadal formula is represented by
formula
in this equation l and v refer to the lateral and vertical forces acting upon the rail and wheel is the angle made when the wheel flange is in contact with the rail face and is the coefficient of friction between the wheel and the rail
typically the axle load for a railway vehicle should be such that the lateral forces of the wheel against the rail should not exceed of the vertical down force of the vehicle on the rail put another way there should be twice as much downward force holding the wheel to the rail as there is lateral force which will tend to cause the wheel to climb in turns this ratio is accomplished by matching the wheelset with the appropriate rail profile to achieve the l v ratio desired if the l v ratio gets too high the wheel flange will be pressing against the rail face and during a turn this will cause the wheel to climb the face of the rail potentially derailing the railcar
wagner formula
the nadal formula assumes the wheel remains perpendicular to the rail it does not take into account hunting oscillation of the wheelset or the movement of the wheel flange contact point against the rail
a variation of the nadal formula which does take these factors into consideration is the wagner formula as the wheelset yaws relative to the rail the vertical force v is no longer completely vertical but is now acting at an angle to the vertical when this angle is factored into the nadal formula the result is the wagner formula
formula
when the vertical force is truly vertical that is and therefore cos the wagner formula equals the nadal formula

engineering research

engineering research seeks improvements in theory and practice in fields such as for example high speed computation bioengineering earthquake prediction power systems nanotechnology and construction
major contributors to engineering research around the world include governments private business
and academia
the results of engineering research can emerge in journal articles at academic conferences and in the form of new products on the market
much engineering research in the united states of america takes place under the aegis of the department of defense
military related research into science and technology has led to dual use applications with the adaptation of weaponry communications and other defense systems to civilian use programmable digital computers and the internet which connects them the gps satellite network fiber optic cable radar and lasers provide examples

preconsolidation pressure

preconsolidation pressure is the maximum effective vertical overburden stress that a particular soil sample has sustained in the past this quantity is important in geotechnical engineering particularly for finding the expected settlement of foundations and embankments alternative names for the preconsolidation pressure are preconsolidation stress pre compression stress pre compaction stress and preload stress a soil is called overconsolidated if the current effective stress acting on the soil is less than the historical maximum
the preconsolidation pressure can help determine the largest overburden pressure that can be exerted on a soil without irrecoverable volume change this type of volume change is important for understanding shrinkage behavior crack and structure formation and resistance to shearing stresses previous stresses and other changes in a soil's history are preserved within the soil's structure if a soil is loaded beyond this point the soil is unable to sustain the increased load and the structure will break down this breakdown can cause a number of different things depending on the type of soil and its geologic history
preconsolidation pressure cannot be measured directly but can be estimated using a number of different strategies samples taken from the field are subjected to a variety of tests like the constant rate of strain test crs or the incremental loading test il these tests can be costly due to expensive equipment and the long period of time they require each sample must be undisturbed and can only undergo one test with satisfactory results it is important to execute these tests precisely to ensure an accurate resulting plot there are various methods for determining the preconsolidation pressure from lab data the data is usually arranged on a semilog plot of the effective stress frequently represented as versus the void ratio this graph is commonly called the e log p curve or the consolidation curve
methods
the preconsolidation pressure can be estimated in a number of different ways but not measured directly it is useful to know the range of expected values depending on the type of soil being analyzed for example in samples with natural moisture content at the liquid limit liquidity index of preconsolidation ranges between about and tsf depending on soil sensitivity defined as the ratio of undisturbed peak undrained shear strength to totally remolded undrained shear strength for natural moisture at the plastic limit liquidity index equal to zero preconsolidation ranges from about to tsf
see atterberg limits for information about soil properties like liquidity index and liquid limit
arthur casagrande's graphical method
using a consolidation curve casagrande
the point where the lines in part and part intersect is the preconsolidation pressure
estimation of the most probable preconsolidation pressure
using a consolidation curve intersect the horizontal portion of the recompression curve and a line tangent to the compression curve this point is within the range of probable preconsolidation pressures it can be used in calculations that require less accuracy or if a rough estimate is all that is required
see modeling volume change and mechanical properties with hydraulic models from the soil science society of america link in references for a more involved mathematical model based on casagrande's method combining principles from soil mechanics and hydraulics
mechanisms causing preconsolidation
various different factors can cause a soil to approach its preconsolidation pressure
uses
preconsolidation pressure is used in many calculations of soil properties essential for structural analysis and soil mechanics one of the primary uses is to predict settlement of a structure after loading this is required for any construction project such as new buildings bridges large roads and railroad tracks all of these require site evaluation before construction preparing a site for construction requires an initial compression of the soil to prepare for foundation to be added it is important to know the preconsolidation pressure because it will help to determine the amount of loading that is appropriate for the site it will also help to determine whether recompression after excavation if the conditions allow soil can exhibit volumetric expansion recompression due to the removal of load conditions need to be considered

human reliability

human reliability is related to the field of human factors and ergonomics and refers to the reliability of humans in fields such as manufacturing transportation the military or medicine human performance can be affected by many factors such as age state of mind physical health attitude emotions propensity for certain common mistakes errors and cognitive biases etc
human reliability is very important due to the contributions of humans to the resilience of systems and to possible adverse consequences of human errors or oversights especially when the human is a crucial part of the large socio technical systems as is common today user centered design and error tolerant design are just two of many terms used to describe efforts to make technology better suited to operation by humans
analysis techniques
a variety of methods exist for human reliability analysis hra two general classes of methods are those based on probabilistic risk assessment pra and those based on a cognitive theory of control
pra based techniques
one method for analyzing human reliability is a straightforward extension of probabilistic risk assessment pra in the same way that equipment can fail in a power plant so can a human operator commit errors in both cases an analysis functional decomposition for equipment and task analysis for humans would articulate a level of detail for which failure or error probabilities can be assigned this basic idea is behind the technique for human error rate prediction therp therp is intended to generate human error probabilities that would be incorporated into a pra the accident sequence evaluation program asep human reliability procedure is a simplified form of therp an associated computational tool is simplified human error analysis code shean more recently the us nuclear regulatory commission has published the standardized plant analysis risk human reliability analysis spar h method to take account of the potential for human error
cognitive control based techniques
erik hollnagel has developed this line of thought in his work on the contextual control model cocom and the cognitive reliability and error analysis method cream cocom models human performance as a set of control modes strategic based on long term planning tactical based on procedures opportunistic based on present context and scrambled random and proposes a model of how transitions between these control modes occur this model of control mode transition consists of a number of factors including the human operator's estimate of the outcome of the action success or failure the time remaining to accomplish the action adequate or inadequate and the number of simultaneous goals of the human operator at that time cream is a human reliability analysis method that is based on cocom
related techniques
related techniques in safety engineering and reliability engineering include failure mode and effects analysis hazop fault tree and saphire systems analysis programs for hands on integrated reliability evaluations
human factors analysis and classification system hfacs
the human factors analysis and classification system hfacs was developed initially as a framework to understand the role of human error in aviation accidents it is based on james reason's swiss cheese model of human error in complex systems hfacs distinguishes between the active failures of unsafe acts and latent failures of preconditions for unsafe acts unsafe supervision and organizational influences these categories were developed empirically on the basis of many aviation accident reports
unsafe acts are performed by the human operator on the front line e g the pilot the air traffic controller the driver unsafe acts can be either errors in perception decision making or skill based performance or violations routine or exceptional the errors here are similar to the above discussion violations are the deliberate disregard for rules and procedures as the name implies routine violations are those that occur habitually and are usually tolerated by the organization or authority exceptional violations are unusual and often extreme for example driving mph in a mph zone speed limit is a routine violation but driving mph in the same zone is exceptional
there are two types of preconditions for unsafe acts those that relate to the human operator's internal state and those that relate to the human operator's practices or ways of working adverse internal states include those related to physiology e g illness and mental state e g mentally fatigued distracted a third aspect of internal state is really a mismatch between the operator's ability and the task demands for example the operator may be unable to make visual judgments or react quickly enough to support the task at hand poor operator practices are another type of precondition for unsafe acts these include poor crew resource management issues such as leadership and communication and poor personal readiness practices e g violating the crew rest requirements in aviation
four types of unsafe supervision are inadequate supervision planned inappropriate operations failure to correct a known problem and supervisory violations
organizational influences include those related to resources management e g inadequate human or financial resources organizational climate structures policies and culture and organizational processes such as procedures schedules oversight

overhead engineering

in engineering some methods or components make special demands on the system the extra design features necessary to accommodate these demands are called overhead for instance in electrical engineering a particular integrated circuit might draw large current requiring a robust power delivery circuit and a heat dissipation mechanism
example
an example from software engineering is the encoding of information and data the date and time can be expressed as unix time with the bit signed integer codice consuming only byte represented as iso formatted utf encoded string codice the date would consume byte a size overhead of over the binary integer representation as xml this date can be written as follows with an overhead of characters while adding the semantic context that it is a changedate with index
codice
the byte resulting from the utf encoded xml correlates to a size overhead of over the original integer representation

organic engineering systems

organic engineering systems oes are organic hydroponic aeroponic or aquaponic farming technologies that are designed as a building engineering system the main purpose of these technologies is to grow food in buildings oes are agricultural technologies and are considered a sub section of ecological engineering primarily because they mimic natural ecosystems to produce food and are defined as urban horticulture

energy separating agent

in chemical separation processes an energy separating agent esa is the heat or shaft work added to facilitate the separation of two chemical species it is contrasted with a mass separating agent which is any chemical species added to the reaction that facilitates the reaction esas are used in many common separation procedures
some important examples of procedures utilizing esas vaporization heat added distillation heat added crystallization heat evolved and stripping heat added

plasmonic metamaterials

plasmonic metamaterials are metamaterials that exploit surface plasmons to achieve optical properties not seen in nature plasmons are produced from the interaction of light with metal dielectric materials under specific conditions the incident light couples with the surface plasmons to create self sustaining propagating electromagnetic waves known as surface plasmon polaritons spps once launched the spps ripple along the metal dielectric interface compared with the incident light the spps can be much shorter in wavelength
the properties stem from the unique structure of the metal dielectric composites with features smaller than the wavelength of light separated by subwavelength distances light hitting such a metamaterial is transformed into surface plasmon polaritons which are shorter in wavelength than the incident light
plasmonic materials
plasmonic materials are metals or metal like materials that exhibit negative real permittivity most common plasmonic materials are gold and silver however many other materials show metal like optical properties in specific wavelength ranges various research groups are experimenting with different approaches to make plasmonic materials that exhibit lower losses and tunable optical properties
negative index
plasmonic metamaterials are realizations of materials first proposed by victor veselago a russian theoretical physicist in also known as left handed or negative index materials veselago theorized that they would exhibit optical properties opposite to those of glass or air in negative index materials energy is transported in a direction opposite to that of propagating wavefronts rather than paralleling them as is the case in positive index materials
normally light traveling from say air into water bends upon passing through the normal a plane perpendicular to the surface and entering the water in contrast light reaching a negative index material through air would not cross the normal rather it would bend the opposite way
negative refraction was first reported for microwave and infrared frequencies in a collaboration between the california institute of technology and the nist reported narrow band negative refraction of visible light in two dimensions
to create this response incident light couples with the undulating gas like charges plasmons normally on the surface of metals this photon plasmon interaction results in spps that generate intense localized optical fields the waves are confined to the interface between metal
and insulator this narrow channel serves as a transformative guide that in effect traps and compresses the wavelength of incoming light to a fraction of its original value
nanomechanical systems incorporating metamaterials exhibit negative radiation pressure
light falling on conventional materials with a positive index of refraction exerts a positive pressure meaning that it can push an object away from the light source in contrast illuminating negative index metamaterials should generate a negative pressure that pulls an object toward light
three dimensional negative index
computer simulations predict plasmonic metamaterials with a negative index in three dimensions potential fabrication methods include multilayer thin film deposition focused ion beam milling and self assembly
gradient index
pmms can be made with a gradient index a material whose refractive index varies progressively across the length or area of the material one such material involved depositing a thermoplastic known as a pmma on a gold surface via electron beam lithography
hyperbolic
hyperbolic metamaterials behave as a metal when light passes through it in one direction and like a dielectric when light passes in the perpendicular direction called extreme anisotropy the material's dispersion relation forms a hyperboloid the associated wavelength can in principle be infinitely small
isotropy
the first metamaterials created exhibit anisotropy in their effects on plasmons i e they act only in one direction
more recently researchers used a novel self folding technique to create a three dimensional array of split ring resonators that exhibits isotropy when rotated in any direction up to an incident angle of degrees exposing strips of nickel and gold deposited on a polymer silicon substrate to air allowed mechanical stresses to curl the strips into rings forming the resonators by arranging the strips at different angles to each other fold symmetry was achieved which allowed the resonators to produce effects in multiple directions
materials
silicon sandwich
negative refraction for visible light was first produced in a sandwich like construction with thin layers an insulating sheet of silicon nitride was covered by a film of silver and underlain by another of gold the critical dimension is the thickness of the layers which summed to a fraction of the wavelength of blue and green light by incorporating this metamaterial into integrated optics on an ic chip negative refraction was demonstrated over blue and green frequencies the collective result is a relatively significant response to light
graphene
graphene also accommodates surface plasmons observed via near field infrared optical microscopy techniques and infrared spectroscopy potential applications of graphene plasmonics involve terahertz to midinfrared frequencies in devices such as optical modulators photodetectors and biosensors
superlattice
a hyperbolic metamaterial made from titanium nitride metal and aluminum scandium nitride dielectric have compatible crystal structures and can form a superlattice a crystal that combines two or more materials the material is compatible with existing cmos technology unlike traditional gold and silver mechanically strong and thermally stable at higher temperatures the material exhibits higher photonic densities of of states than au or ag the material is an efficient light absorber
the material was created using epitaxy inside a vacuum chamber with a technique known as magnetron sputtering the material featured ultra thin and ultra smooth layers with sharp interfaces
possible applications include a planar hyperlens that could make optical microscopes able to see objects as small as dna advanced sensors more efficient solar collectors nano resonators quantum computing and diffraction free focusing and imaging
the material works across a broad spectrum from near infrared to visible light near infrared is essential for telecommunications and optical communications and visible light is important for sensors microscopes and efficient solid state light sources
applications
microscopy
one potential application is microscopy beyond the diffraction limit gradient index plasmonics were used to produce luneburg and eaton lenses that interact with surface plasmon polaritons rather than photons
a theorized superlens could exceed the diffraction limit that prevents standard positive index lenses from resolving objects smaller than one half of the wavelength of visible light such a superlens would capture spatial information that is beyond the view of conventional optical microscopes several approaches to building such a microscope have been proposed the subwavelength domain could be optical switches modulators photodetectors and directional light emitters
biological and chemical sensing
other proof of concept applications under review involve high sensitivity biological and chemical sensing they may enable the development of optical sensors that exploit the confinement of surface plasmons within a certain type of fabry perot nano resonator this tailored confinement allows efficient detection of specific bindings of target chemical or biological analytes using the spatial overlap between the optical resonator mode and the analyte ligands bound to the resonator cavity sidewalls structures are optimized using finite difference time domain electromagnetic simulations fabricated using a combination of electron beam lithography and electroplating and tested using both near field and far field optical microscopy and spectroscopy
optical computing
optical computing replaces electronic signals with light processing devices
in researchers announced a nanometer teraherz speed optical switch the switch is made of a metamaterial consisting of nanoscale particles of vanadium dioxide a crystal that switches between an opaque metallic phase and a transparent semiconducting phase the nanoparticles are deposited on a glass substrate and overlain by even smaller gold nanoparticles that act as a plasmonic photocathode
femtosecond laser pulses free electrons in the gold particles that jump into the and cause a subpicosecond phase change
the device is compatible with current integrated circuit technology silicon based chips and high k dielectrics materials it operates in the visible and near infrared region of the spectrum it generates only femtojoules bit operation allowing the switches to be packed tightly

surface plasmon polariton

surface plasmon polaritons spps are infrared or visible frequency electromagnetic waves which travel along a metal dielectric or metal air interface the term surface plasmon polariton explains that the wave involves both charge motion in the metal surface plasmon and electromagnetic waves in the air or dielectric polariton
they are a type of surface wave guided along the interface in much the same way that light can be guided by an optical fiber spps are shorter in wavelength than the incident light photons hence spps can have tighter spatial confinement and higher local field intensity perpendicular to the interface they have subwavelength scale confinement an spp will propagate along the interface until its energy is lost either to absorption in the metal or scattering into other directions such as into free space
application of spps enables subwavelength optics in microscopy and lithography beyond the diffraction limit it also enables the first steady state micro mechanical measurement of a fundamental property of light itself the momentum of a photon in a dielectric medium other applications are photonic data storage light generation and bio photonics
excitation
spps can be excited by both electrons and photons excitation by electrons is created by firing electrons into the bulk of a metal as the electrons scatter energy is transferred into the bulk plasma the component of the scattering vector parallel to the surface results in the formation of a surface plasmon polariton
if a free space photon comes from air towards a smooth metal surface it cannot excite an spp at the metal air interface the reason is that if the photon and spp have the same frequency then they necessarily have different in plane wavevectors this incompatibility is analogous to the lack of transmission that occurs during total internal reflection analogously an spp on a smooth metal surface cannot lose energy to radiation into the dielectric if the dielectric is uniform
nevertheless coupling of photons into spps can be achieved using a coupling medium such as a prism or grating to match the photon and surface plasmon wave vectors a prism can be positioned against a thin metal film in the kretschmann configuration or very close to a metal surface in the otto configuration figure a grating coupler matches the wave vectors by increasing the parallel wave vector component by an amount related to the grating period figure this method while less frequently utilized is critical to the theoretical understanding of the effect of surface roughness moreover simple isolated surface defects such as a groove a slit or a corrugation on an otherwise planar surface provides a mechanism by which free space radiation and sps can exchange energy and hence couple
dispersion relation
the electric field of a propagating electromagnetic wave can be expressed
where k is the wave number and is the frequency of the wave by solving maxwell's equations for the electromagnetic wave at an interface between two materials with relative dielectric functions and see figure with the appropriate continuity relation the boundary conditions are
and
where c is the speed of light in a vacuum and kx is same for both media at the interface for a surface wave solving these two equations the dispersion relation for a wave propagating on the surface is
in the free electron model of an electron gas which neglects attenuation the metallic dielectric function is
where the bulk plasma frequency in si units is
where n is the electron density e is the charge of the electron m is the effective mass of the electron and formula is the permittivity of free space the dispersion relation is plotted in figure at low k the spp behaves like a photon but as k increases the dispersion relation bends over and reaches an asymptotic limit called the surface plasma frequency since the dispersion curve lies to the right of the light line k c the spp has a shorter wavelength than free space radiation such that the out of plane component of the spp wavevector is purely imaginary and exhibits evanescent decay the surface plasma frequency is the asymptote of this curve and is given by
in the case of air this result simplifies to
if we assume that is real and then it must be true that a condition which is satisfied in metals electromagnetic waves passing through a metal experience damping due to ohmic losses and electron core interactions these effects show up in as an imaginary component of the dielectric function the dielectric function of a metal is expressed i where and are the real and imaginary parts of the dielectric function respectively generally so the wavenumber can be expressed in terms of its real and imaginary components as
the wave vector gives us insight into physically meaningful properties of the electromagnetic wave such as its spatial extent and coupling requirements for wave vector matching
propagation length and skin depth
as an spp propagates along the surface it loses energy to the metal due to absorption the intensity of the surface plasmon decays with the square of the electric field so at a distance x the intensity has decreased by a factor of exp kx x the propagation length is defined as the distance for the spp intensity to decay by a factor of e this condition is satisfied at a length
likewise the electric field falls off evanescently perpendicular to the metal surface at low frequencies the spp penetration depth into the metal is commonly approximated using the skin depth formula in the dielectric the field will fall off far more slowly the decay lengths in the metal and dielectric medium can be expressed as
where formula is the polarization angle and formula is the angle from the z axis in the xz plane two important consequences come out of these equations the first is that if formula s polarization then formula and the scattered light formula secondly the scattered light has a measurable profile which is readily correlated to the roughness this topic is treated in greater detail in reference

icme cyberinfrastructure

integrated computational materials engineering icme involves the integration of experimental results design models simulations and other computational data related to a variety of materials used in multiscale engineering and design central to the achievement of icme goals has been the creation of a cyberinfrastructure a web based collaborative platform which provides the ability to accumulate organize and disseminate knowledge pertaining to materials science and engineering to facilitate this information being broadly utilized enhanced and expanded
the icme cyberinfrastructure provides storage access and computational capabilities for an extensive network of manufacturing design and life cycle simulation software within this software framework data is archived searchable and interactive offering engineers and scientists a vast database of materials related information for use in research multiscale modeling simulation implementation and an array of other activities in support of more efficient less costly product development furthermore the icme cyberinfrastructure is expected to provide the capability to access and link application codes including the development of protocols necessary to integrate hierarchical modeling approaches with an emphasis on computational efficiency experimental validation of models and protecting intellectual property the cyberinfrastructure assimilates process microstructure property relations development of constitutive materials models that accurately predict multiscale material behaviors admitting microstructure inclusions and history effects access to shared databases of analytical and experimental data and material models as such it is also crucial to identifying gaps in materials knowledge which in turn guides the development of new materials theories models and simulation tools such a community based knowledge foundation ultimately enables materials informatics systems that fuse high fidelity experimental databases with models of physical processes
in addition the vision of the icme cyberinfrastructure is compatible with the national science foundation's nsf cyberinfrastructure vision for st century discovery which advocates development and deployment of human centered information technology it systems that address the needs of science and engineering communities and open new opportunities for enhancing education and workforce development programs according to the nsf directive it systems such as the icme cyberinfrastructure should provide access to tools services and other networked resources including high performance computing facilities data repositories and libraries of computational tools enabling and reliably supporting secure and efficient nationwide or global virtual organizations spanning across administrative boundaries
implementation
the national materials advisory board nmab of the national academy of engineering nae committee proposed the following definition for the term icme cyberinfrastructure
the internet based collaborative materials science and engineering research and development environments that support advanced data acquisition data and model storage data and model management data and model mining data and model visualization and other computing and information processing services required to develop an integrated computational materials engineering capability
according to nmab's vision the building blocks of the icme cyberinfrastructure are the individual web sites web portals which offer access to information data and tools each established for specific purposes by different organizations linked together these constituent web portals will form the icme cyberinfrastructure or icme supply chain i e a series of well established capable and viable organizations these organizations are to provide necessary portions of the icme cyberinfrastructure's value chain

swiss cheese model

the swiss cheese model of accident causation is a model used in risk analysis and risk management including aviation engineering healthcare and as the principle behind layered security as used in computer security and defense in depth it likens human systems to multiple slices of swiss cheese stacked side by side in which the risk of a threat becoming a reality is mitigated by the differing layers and types of defenses which are layered behind each other therefore in theory lapses and weaknesses in one defense do not allow a risk to materialize since other defenses also exist to prevent a single point of weakness the model was originally formally propounded by dante orlandella and james t reason of the university of manchester and has since gained widespread acceptance it is sometimes called the cumulative act effect
although the swiss cheese model is respected and considered to be a useful method of relating concepts it has been subject to criticism that it is used over broadly and without enough other models or support
failure domains
reason hypothesized that most accidents can be traced to one or more of four failure domains organizational influences supervision preconditions and specific acts preconditions for unsafe acts include fatigued air crew or improper communications practices unsafe supervision encompasses for example pairing inexperienced pilots on a night flight into known adverse weather organizational influences encompass such things as reduction in expenditure on pilot training in times of financial austerity
holes and slices
in the swiss cheese model an organisation's defenses against failure are modeled as a series of barriers represented as slices of cheese the holes in the slices represent weaknesses in individual parts of the system and are continually varying in size and position across the slices the system produces failures when a hole in each slice momentarily aligns permitting in reason's words a trajectory of accident opportunity so that a hazard passes through holes in all of the slices leading to a failure
frosch described reason's model in mathematical terms as a model in percolation theory which he analyses as a bethe lattice
active and latent failures
the swiss cheese model includes both active and latent failures active failures encompass the unsafe acts that can be directly linked to an accident such as in the case of aircraft accidents pilot error latent failures include contributory factors that may lie dormant for days weeks or months until they contribute to the accident latent failures span the first three domains of failure in reason's model
applications
the same framework applies in healthcare for example a latent failure could be the similar packaging of two drugs that are then stored close to each other in a pharmacy such a failure would be a contributory factor in the administration of the wrong drug to a patient such research led to the realization that medical error can be the result of system flaws not character flaws and that greed ignorance malice or laziness are not the only causes of error
lubnau lubnau and okray apply the model to the engineering of firefighting systems aiming to reduce human errors by inserting additional layers of cheese into the system namely the techniques of crew resource management
this is one of the many models listed with references in

new manufacturing economy

the new manufacturing economy nme describes the role of advanced manufacturing in the rise of the new economy the term describes manufacturing enabled by digital technologies advanced systems and processes and a highly trained and knowledgeable workforce the new manufacturing economy integrates networks d printers and other proficiencies into business strategies to further develop manufacturing practices
thomas friedman references lawrence f katz that hubs of universities high tech manufacturers software service providers and highly nimble start ups are a needed economic development strategy this is very similar to nme thoughts even though that exact term is not used
the pillars of the new manufacturing economy
technology
focus on geographic expansion information technology and internet commerce are on the rise for industrial manufacturing companies according to the pricewaterhousecooper q manufacturing barometer such conditions compel companies to incorporate new technologies into business plans and to concentrate on the application of open source product development in the creation of physical goods as a form of competitive advantage
new technologies influence various industries to emphasize innovation as a business tool advanced manufacturing is feasible due to continuous improvement investments and modernization of the workforce technologies and supply chains in order to increase global competitiveness environmental sustainability and product customization to meet consumer expectations
workforce
incorporating modern cnc equipment in new manufacturing processes requires better trained employees with more exacting skills than were previously required in heavy industry past manufacturing job consisted largely of physical labor and worker assembly line requirements but in response to technological evolution are becoming tech savvy and information intense with focus on creativity and resourcefulness
strategy
the new manufacturing economy is centered around niche businesses who satisfy the needs of small consumer markets by offering what customers want when they want it the primary foundation of this strategy is selling less of more adopting the efficiencies of digital and web based technologies into current business strategies is an emerging trend in manufacturing practices
industries
advanced technology in the manufacturing marketplace has led to growth in areas such as software development and biotechnology and to emphasis on numerous industries such as

standardization

standardization or standardisation is the process of developing and implementing technical standards standardization can help to maximize compatibility interoperability safety repeatability or quality it can also facilitate commoditization of formerly custom processes in social sciences including economics the idea of standardization is close to the solution for a coordination problem a situation in which all parties can realize mutual gains but only by making mutually consistent decisions this view includes the case of spontaneous standardization processes to produce de facto standards
history
the implementation of standards in industry and commerce became highly important with the onset of the industrial revolution and the need for high precision machine tools and interchangeable parts
first attempts
henry maudslay developed the first industrially practical screw cutting lathe in this allowed for the standardisation of screw thread sizes for the first time and paved the way for the practical application of interchangeability an idea that was already taking hold to nuts and bolts
before this screw threads were usually made by chipping and filing that is with skilled freehand use of chisels and files nuts were rare metal screws when made at all were usually for use in wood metal bolts passing through wood framing to a metal fastening on the other side were usually fastened in non threaded ways such as clinching or upsetting against a washer maudslay standardized the screw threads used in his workshop and produced sets of taps and dies that would make nuts and bolts consistently to those standards so that any bolt of the appropriate size would fit any nut of the same size this was a major advance in workshop technology
national standard
maudslay's work as well as the contributions of other engineers accomplished a modest amount of industry standardization some companies in house standards spread a bit within their industries
joseph whitworth's screw thread measurements were adopted as the first unofficial national standard by companies around the country in it came to be known as the british standard whitworth and was widely adopted in other countries
this new standard specified a thread angle and a thread depth of p and a radius of p where p is the pitch the thread pitch increased with diameter in steps specified on a chart an example of the use of the whitworth thread is the royal navy's crimean war gunboats these were the first instance of mass production techniques being applied to marine engineering
with the adoption of bsw by british railway lines many of which had previously used their own standard both for threads and for bolt head and nut profiles and improving manufacturing techniques it came to dominate british manufacturing
american unified coarse was originally based on almost the same imperial fractions the unified thread angle is and has flattened crests whitworth crests are rounded thread pitch is the same in both systems except that the thread pitch for the in bolt is threads per inch tpi in bsw versus tpi in the unc
national standards body
by the end of the th century differences in standards between companies was making trade increasingly difficult and strained for instance an iron and steel dealer recorded his displeasure in the times architects and engineers generally specify such unnecessarily diverse types of sectional material or given work that anything like economical and continuous manufacture becomes impossible in this country no two professional men are agreed upon the size and weight of a girder to employ for given work
the engineering standards committee was established in london in as the world's first national standards body it subsequently extended its standardization work and became the british engineering standards association in adopting the name british standards institution in after receiving its royal charter in the national standards were adopted universally throughout the country and enabled the markets to act more rationally and efficiently with an increased level of cooperation
after the first world war similar national bodies were established in other countries the deutsches institut f r normung was set up in germany in followed by its counterparts the american national standard institute and the french commission permanente de standardisation both in
international standards
by the mid to late th century efforts were being made to standardize electrical measurement lord kelvin was an important figure in this process introducing accurate methods and apparatus for measuring electricity in he introduced a series of effective instruments including the quadrant electrometer which cover the entire field of electrostatic measurement he invented the current balance also known as the kelvin balance or ampere balance sic for the precise specification of the ampere the standard unit of electric current
another important figure was r e b crompton who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in the early th century many companies had entered the market in the s and all chose their own settings for voltage frequency current and even the symbols used on circuit diagrams adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering
in crompton represented britain at the louisiana purchase exposition in saint louis as part of a delegation by the institute of electrical engineers he presented a paper on standardisation which was so well received that he was asked to look into the formation of a commission to oversee the process by his work was complete and he drew up a permanent constitution for the first international standards organization the international electrotechnical commission the body held its first meeting that year in london with representatives from countries in honour of his contribution to electrical standardisation lord kelvin was elected as the body's first president
the international federation of the national standardizing associations isa was founded in with a broader remit to enhance international cooperation for all technical standards and specifications the body was suspended in during world war ii
after the war isa was approached by the recently formed united nations standards coordinating committee unscc with a proposal to form a new global standards body in october isa and unscc delegates from countries met in london and agreed to join forces to create the new international organization for standardization iso the new organization officially began operations in february
in general each country or economy has a single recognized national standards body nsb examples include abnt aenor afnor ansi bsi dgn din iram jisc kats sabs sac scc sis snz an nsb is likely the sole member from that economy in iso
nsbs may be either public or private sector organizations or combinations of the two for example the three nsbs of canada mexico and the united states are respectively the standards council of canada scc the general bureau of standards dgn and the american national standards institute ansi scc is a canadian crown corporation dgn is a governmental agency within the mexican ministry of economy and ansi and aenor are a c non profit organization with members from both the private and public sectors the determinants of whether an nsb for a particular economy is a public or private sector body may include the historical and traditional roles that the private sector fills in public affairs in that economy or the development stage of that economy
usage
standards can be
the existence of a published standard does not necessarily imply that it is useful or correct just because an item is stamped with a standard number does not by itself indicate that the item is fit for any particular use the people who use the item or service engineers trade unions etc or specify it building codes government industry etc have the responsibility to consider the available standards specify the correct one enforce compliance and use the item correctly validation and verification
standardization is implemented greatly when companies release new products to market compatibility is important for products to be successful this allows consumers to use their new items along with what they already own
social science
in the context of social criticism and social science standardization often means the process of establishing standards of various kinds and improving efficiency to handle people their interactions cases and so forth examples include formalization of judicial procedure in court and establishing uniform criteria for diagnosing mental disease standardization in this sense is often discussed along with or synonymously to such large scale social changes as modernization bureaucratization homogenization and centralization of society
information exchange
in the context of information exchange standardization refers to the process of developing standards for specific business processes using specific formal languages these standards are usually developed in voluntary consensus standards bodies such as the united nations center for trade facilitation and electronic business un cefact the world wide web consortium w c the telecommunications industry association tia and the organization for the advancement of structured information standards oasis
there are many specifications that govern the operation and interaction of devices and software on the internet but they are rarely referred to as standards so as to preserve that word as the domain of relatively disinterested bodies such as iso the w c for example publishes recommendations and the ietf publishes requests for comments rfcs however these publications are sometimes referred to as standards
customer service
in the context of customer service standardization refers to the process of developing an international standard that enables organizations to focus their attention on delivering excellence in customer service whilst at the same time providing recognition of success through a third party organization such as the british standards institution bsi the international customer service standard ticss has been developed by the international customer service institute ticsi with the objective of making it the cornerstone global standard of customer service this standard has the status of an independent standard
supply and materials management
in the context of supply chain management and materials management standardization covers the process of specification and use of any item the company must buy in or make allowable substitutions and build or buy decisions
defense
in the context of defense standardization has been defined by nato as the development and implementation of concepts doctrines procedures and designs to achieve and maintain the required levels of compatibility interchangeability or commonality in the operational procedural material technical and administrative fields to attain interoperability
process
the process of standardization can itself be standardized there are at least four levels of standardization compatibility interchangeability commonality and reference these standardization processes create compatibility similarity measurement and symbol standards
there are typically four different techniques for standardization
types of standardization process
effects
standardization has a variety of benefits and drawbacks for firms and consumers participating in the market and on technology and innovation
effect on firms
the primary effect of standardization on firms is that the basis of competition is shifted from integrated systems to individual components within the system prior to standardization a company's product must span the entire system because individual components from different competitors are incompatible but after standardization each company can focus on providing an individual component of the system when the shift toward competition based on individual components takes place firms selling tightly integrated systems must quickly shift to a modular approach supplying other companies with subsystems or components
effect on consumers
standardization has a variety of benefits for consumers but one of the greatest benefits is enhanced network effects standards increase compatibility and interoperability between products allowing information to be shared within a larger network and attracting more consumers to use the new technology further enhancing network effects other benefits of standardization to consumers are reduced uncertainty because consumers can be more certain that they are not choosing the wrong product and reduced lock in because the standard makes it more likely that there will be competing products in the space consumers may also get the benefit of being able to mix and match components of a system to align with their specific preferences
probably the greatest downside of standardization for consumers is lack of variety there is no guarantee that the chosen standard will meet all consumers needs or even that the standard is the best available option another downside is that if a standard is agreed upon before products are available in the market then consumers are deprived of the penetration pricing that often results when rivals are competing to rapidly increase market share in an attempt to increase the likelihood that their product will become the standard it is also possible that a consumer will choose a product based upon a standard that fails to become dominant in this case the consumer will have spent resources on a product that is ultimately less useful to him or her as the result of the standardization process
effect on technology
much like the effect on consumers the effect of standardization on technology and innovation is mixed increased adoption of a new technology as a result of standardization is important because rival and incompatible approaches competing in the marketplace can slow or even kill the growth of the technology the shift to a modularized architecture as a result of standardization brings increased flexibility rapid introduction of new products and the ability to more closely meet individual customer's needs
the negative effects of standardization on technology have to do with its tendency to restrict new technology and innovation standards shift competition from features to price because the features are defined by the standard the degree to which this is true depends on the specificity of the standard standardization in an area also rules out alternative technologies as options while encouraging others

engineering drawing

an engineering drawing a type of technical drawing is used to fully and clearly define requirements for engineered items
engineering drawing the activity produces engineering drawings the documents more than merely the drawing of pictures it is also a language a graphical language that communicates ideas and information from one mind to another most especially it communicates all needed information from the engineer who designed a part to the workers who will make it
relationship to artistic drawing
engineering drawing and artistic drawing are both types of drawing and either may be called simply drawing when the context is implicit engineering drawing shares some traits with artistic drawing in that both create pictures but whereas the purpose of artistic drawing is to convey emotion or artistic sensitivity in some way subjective impressions the purpose of engineering drawing is to convey information objective facts one of the corollaries that follows from this fact is that whereas anyone can appreciate artistic drawing even if each viewer has his own unique appreciation engineering drawing requires some training to understand like any language but there is also a high degree of objective commonality in the interpretation also like other languages in fact engineering drawing has evolved into a language that is more precise and unambiguous than natural languages in this sense it is closer to a programming language in its communication ability engineering drawing uses an extensive set of conventions to convey information very precisely with very little ambiguity
relationship to other technical drawing types
the process of producing engineering drawings and the skill of producing those is often referred to as technical drawing or drafting also spelled draughting although technical drawings are also required for disciplines that would not ordinarily be thought of as parts of engineering such as architecture landscaping cabinet making road construction and garment making
persons employed in the trade of producing engineering drawings were called draftsmen or draughtsmen in the past although these terms are still in use the non gender specific terms draftsperson and drafter are now more common
cascading of conventions by specialty
the various fields share many common conventions of drawing while also having some field specific conventions for example even within metalworking there are some process specific conventions to be learned casting machining fabricating and assembly all have some special drawing conventions and within fabrication there is further division including welding riveting pipefitting and erecting each of these trades has some details that only specialists will have memorized
legal instruments
an engineering drawing is a legal document that is a legal instrument because it communicates all the needed information about what is wanted to the people who will expend resources turning the idea into a reality it is thus a part of a contract the purchase order and the drawing together as well as any ancillary documents engineering change orders ecos called out specs constitute the contract thus if the resulting product is wrong the worker or manufacturer are protected from liability as long as they have faithfully executed the instructions conveyed by the drawing if those instructions were wrong it is the fault of the engineer because manufacturing and construction are typically very expensive processes involving large amounts of capital and payroll the question of liability for errors has great legal implications as each party tries to blame the other and assign the wasted cost to the other's responsibility this is the biggest reason why the conventions of engineering drawing have evolved over the decades toward a very precise unambiguous state
standardization and disambiguation
engineering drawings specify requirements of a component or assembly which can be complicated standards provide rules for their specification and interpretation in a new revision of iso was published containing the invocation principle this states that once a portion of the iso gps system is invoked in a mechanical engineering product documentation the entire iso gps system is invoked it also goes on to state that marking a drawing tolerancing iso is optional the implication of this is that any drawing using iso symbols can only be interpreted to iso gps rules the only way not to invoke the iso gps system is to invoke a national or other standard now in there is a new standardisation called bs this is now used for all standard and technical drawings
since there are only two widely standardized definitions of size there is only one real alternative to iso gps i e asme y and y m most recently revised in standardization also aids internationalization because people from different countries who speak different languages can read the same engineering drawing and interpret it the same way to that end drawings should be as free of notes and abbreviations as possible so that the meaning is conveyed graphically
media
for centuries until the post world war ii era all engineering drawing was done manually by using pencil and pen on paper or other substrate e g vellum mylar since the advent of computer aided design cad engineering drawing has been done more and more in the electronic medium with each passing decade today most engineering drawing is done with cad but pencil and paper have not disappeared
some of the tools of manual drafting include pencils pens and their ink straightedges t squares french curves triangles rulers protractors dividers compasses scales erasers and tacks or push pins slide rules used to number among the supplies too but nowadays even manual drafting when it occurs benefits from a pocket calculator or its onscreen equivalent and of course the tools also include drawing boards drafting boards or tables the english idiom to go back to the drawing board which is a figurative phrase meaning to rethink something altogether was inspired by the literal act of discovering design errors during production and returning to a drawing board to revise the engineering drawing drafting machines are devices that aid manual drafting by combining drawing boards straightedges pantographs and other tools into one integrated drawing environment cad provides their virtual equivalents
producing drawings usually involves creating an original that is then reproduced generating multiple copies to be distributed to the shop floor vendors company archives and so on the classic reproduction methods involved blue and white appearances whether white on blue or blue on white which is why engineering drawings were long called and even today are still often called blueprints or bluelines even though those terms are anachronistic from a literal perspective since most copies of engineering drawings today are made by more modern methods often inkjet or laser printing that yield black or multicolour lines on white paper the more generic term print is now in common usage in the u s to mean any paper copy of an engineering drawing in the case of cad drawings the original is the cad file and the printouts of that file are the prints
relationship to model based definition mbd dpd
for centuries engineering drawing was the sole method of transferring information from design into manufacture in recent decades another method has arisen called model based definition mbd or digital product definition dpd in mbd the information captured by the cad software app is fed automatically into a cam app computer aided manufacturing and is translated via postprocessor into other languages such as g code which is executed by a cnc machine tool computer numerical control thus today it is often the case that the information travels from the mind of the designer into the manufactured component without having ever been codified by an engineering drawing in mbd the dataset not a drawing is the legal instrument the term technical data package tdp is now used to refer to the complete package of information in one medium or another that communicates information from design to production such as d model datasets engineering drawings engineering change orders ecos spec revisions and addenda and so on however even in the mbd era where theoretically production could happen without any drawings or humans at all it is still the case that drawings and humans are involved it still takes cad cam programmers cnc setup workers and cnc operators to do manufacturing as well as other people such as quality assurance staff inspectors and logistics staff for materials handling shipping and receiving and front office functions these workers often use drawings in the course of their work that have been produced by rendering and plotting printing from the mbd dataset
when proper procedures are being followed a clear chain of precedence is always documented such that when a person looks at a drawing s he is told by a note thereon that this drawing is not the governing instrument because the mbd dataset is in these cases the drawing is still a useful document although legally it is classified as for reference only meaning that if any controversies or discrepancies arise it is the mbd dataset not the drawing that governs
systems of dimensioning and tolerancing
almost all engineering drawings except perhaps reference only views or initial sketches communicate not only geometry shape and location but also dimensions and tolerances for those characteristics several systems of dimensioning and tolerancing have evolved the simplest dimensioning system just specifies distances between points such as an object's length or width or hole center locations since the advent of well developed interchangeable manufacture these distances have been accompanied by tolerances of the plus or minus or min and max limit types coordinate dimensioning involves defining all points lines planes and profiles in terms of cartesian coordinates with a common origin coordinate dimensioning was the sole best option until the post world war ii era saw the development of geometric dimensioning and tolerancing gd t which departs from the limitations of coordinate dimensioning e g rectangular only tolerance zones tolerance stacking to allow the most logical tolerancing of both geometry and dimensions that is both form shapes locations and sizes
engineering drawings common features
drawings convey the following critical information
line styles and types
a variety of line styles graphically represent physical objects types of lines include the following
lines can also be classified by a letter classification in which each line is given a letter
multiple views and projections
in most cases a single view is not sufficient to show all necessary features and several views are used types of views include the following
orthographic projection
the orthographic projection shows the object as it looks from the front right left top bottom or back and are typically positioned relative to each other according to the rules of either first angle or third angle projection the origin and vector direction of the projectors also called projection lines differs as explained below
until the late th century first angle projection was the norm in north america as well as europe but circa the s the meme of third angle projection spread throughout the north american engineering and manufacturing communities to the point of becoming a widely followed convention and it was an asa standard by the s circa world war i british practice was frequently mixing the use of both projection methods
as shown above the determination of what surface constitutes the front back top and bottom varies depending on the projection method used
not all views are necessarily used generally only as many views are used as are necessary to convey all needed information clearly and economically the front top and right side views are commonly considered the core group of views included by default but any combination of views may be used depending on the needs of the particular design in addition to the principal views front back top bottom right side left side any auxiliary views or sections may be included as serve the purposes of part definition and its communication view lines or section lines lines with arrows marked a a b b etc define the direction and location of viewing or sectioning sometimes a note tells the reader in which zone s of the drawing to find the view or section
auxiliary projection
an auxiliary view is an orthographic view that is projected into any plane other than one of the six principal views these views are typically used when an object contains some sort of inclined plane using the auxiliary view allows for that inclined plane and any other significant features to be projected in their true size and shape the true size and shape of any feature in an engineering drawing can only be known when the line of sight los is perpendicular to the plane being referenced
it is shown like a three dimensional object
isometric projection
the isometric projection shows the object from angles in which the scales along each axis of the object are equal isometric projection corresponds to rotation of the object by about the vertical axis followed by rotation of approximately arcsin tan about the horizontal axis starting from an orthographic projection view isometric comes from the greek for same measure one of the things that makes isometric drawings so attractive is the ease with which degree angles can be constructed with only a compass and straightedge
isometric projection is a type of axonometric projection the other two types of axonometric projection are
oblique projection
an oblique projection is a simple type of graphical projection used for producing pictorial two dimensional images of three dimensional objects
in both oblique projection and orthographic projection parallel lines of the source object produce parallel lines in the projected image
perspective
perspective is an approximate representation on a flat surface of an image as it is perceived by the eye the two most characteristic features of perspective are that objects are drawn
section views
projected views either auxiliary or orthographic which show a cross section of the source object along the specified cut plane these views are commonly used to show internal features with more clarity than may be available using regular projections or hidden lines in assembly drawings hardware components e g nuts screws washers are typically not sectioned
scale
plans are usually scale drawings meaning that the plans are drawn at specific ratio relative to the actual size of the place or object various scales may be used for different drawings in a set for example a floor plan may be drawn at or whereas a detailed view may be drawn at or site plans are often drawn at or
scale is a nuanced subject in the use of engineering drawings on one hand it is a general principle of engineering drawings that they are projected using standardized mathematically certain projection methods and rules thus great effort is put into having an engineering drawing accurately depict size shape form aspect ratios between features and so on and yet on the other hand there is another general principle of engineering drawing that nearly diametrically opposes all this effort and intent that is the principle that users are not to scale the drawing to infer a dimension not labeled this stern admonition is often repeated on drawings via a boilerplate note in the title block telling the user do not scale drawing
the explanation for why these two nearly opposite principles can coexist is as follows the first principle that drawings will be made so carefully and accurately serves the prime goal of why engineering drawing even exists which is successfully communicating part definition and acceptance criteria including what the part should look like if you've made it correctly the service of this goal is what creates a drawing that one even could scale and get an accurate dimension thereby and thus the great temptation to do so when a dimension is wanted but was not labeled the second principle that even though scaling the drawing will usually work one should nevertheless never do it serves several goals such as enforcing total clarity regarding who has authority to discern design intent and preventing erroneous scaling of a drawing that was never drawn to scale to begin with which is typically labeled drawing not to scale or scale nts when a user is forbidden from scaling the drawing s he must turn instead to the engineer for the answers that the scaling would seek and s he will never erroneously scale something that is inherently unable to be accurately scaled
but in some ways the advent of the cad and mbd era challenges these assumptions that were formed many decades ago when part definition is defined mathematically via a solid model the assertion that one cannot interrogate the model the direct analog of scaling the drawing becomes ridiculous because when part definition is defined this way it is not possible for a drawing or model to be not to scale a d pencil drawing can be inaccurately foreshortened and skewed and thus not to scale yet still be a completely valid part definition as long as the labeled dimensions are the only dimensions used and no scaling of the drawing by the user occurs this is because what the drawing and labels convey is in reality a symbol of what is wanted rather than a true replica of it for example a sketch of a hole that is clearly not round still accurately defines the part as having a true round hole as long as the label says mm dia because the dia implicitly but objectively tells the user that the skewed drawn circle is a symbol representing a perfect circle but if a mathematical model essentially a vector graphic is declared to be the official definition of the part then any amount of scaling the drawing can make sense there may still be an error in the model in the sense that what was intended is not depicted modeled but there can be no error of the not to scale type because the mathematical vectors and curves are replicas not symbols of the part features
even in dealing with d drawings the manufacturing world has changed since the days when people paid attention to the scale ratio claimed on the print or counted on its accuracy in the past prints were plotted on a plotter to exact scale ratios and the user could know that a line on the drawing mm long corresponded to a mm part dimension because the drawing said in the scale box of the title block today in the era of ubiquitous desktop printing where original drawings or scaled prints are often scanned on a scanner and saved as a pdf file which is then printed at any percent magnification that the user deems handy such as fit to paper size users have pretty much given up caring what scale ratio is claimed in the scale box of the title block which under the rule of do not scale drawing never really did that much for them anyway
showing dimensions
the required sizes of features are conveyed through use of dimensions distances may be indicated with either of two standardized forms of dimension linear and ordinate
sizes of circular features are indicated using either diametral or radial dimensions radial dimensions use an r followed by the value for the radius diametral dimensions use a circle with forward leaning diagonal line through it called the diameter symbol followed by the value for the diameter a radially aligned line with arrowhead pointing to the circular feature called a leader is used in conjunction with both diametral and radial dimensions
all types of dimensions are typically composed of two parts the nominal value which is the ideal size of the feature and the tolerance which specifies the amount that the value may vary above and below the nominal
sizes of drawings
sizes of drawings typically comply with either of two different standards iso world standard or ansi asme y american according to the following tables
the metric drawing sizes correspond to international paper sizes these developed further refinements in the second half of the twentieth century when photocopying became cheap engineering drawings could be readily doubled or halved in size and put on the next larger or respectively smaller size of paper with no waste of space and the metric technical pens were chosen in sizes so that one could add detail or drafting changes with a pen width changing by approximately a factor of the square root of a full set of pens would have the following nib sizes and mm however the international organization for standardization iso called for four pen widths and set a colour code for each white yellow brown blue these nibs produced lines that related to various text character heights and the iso paper sizes
all iso paper sizes have the same aspect ratio one to the square root of meaning that a document designed for any given size can be enlarged or reduced to any other size and will fit perfectly given this ease of changing sizes it is of course common to copy or print a given document on different sizes of paper especially within a series e g a drawing on a may be enlarged to a or reduced to a
the u s customary a size corresponds to letter size and b size corresponds to ledger or tabloid size there were also once british paper sizes which went by names rather than alphanumeric designations
american society of mechanical engineers asme y y and y are commonly referenced standards in the u s
technical lettering
technical lettering is the process of forming letters numerals and other characters in technical drawing it is used to describe or provide detailed specifications for an object with the goals of legibility and uniformity styles are standardized and lettering ability has little relationship to normal writing ability engineering drawings use a gothic sans serif script formed by a series of short strokes lower case letters are rare in most drawings of machines iso lettering templates designed for use with technical pens and pencils and to suit iso paper sizes produce lettering characters to an international standard the stroke thickness is related to the character height for example mm high characters would have a stroke thickness pen nib size of mm would use a mm pen and so forth the iso character set font has a seriffed one a barred seven an open four six and nine and a round topped three that improves legibility when for example an a drawing has been reduced to a or even a and perhaps enlarged back or reproduced faxed microfilmed c when cad drawings became more popular especially using us american software such as autocad the nearest font to this iso standard font was romantic simplex romans a proprietary shx font with a manually adjusted width factor over ride to make it look as near to the iso lettering for the drawing board however with the closed four and arced six and nine romans shx typeface could be difficult to read in reductions in more recent revisions of software packages the truetype font isocpeur reliably reproduces the original drawing board lettering stencil style however many drawings have switched to the ubiquitous arial ttf
conventional parts areas of an engineering drawing
title block
the title block t b tb is an area of the drawing that conveys header type information about the drawing such as
traditional locations for the title block are the bottom right most commonly or the top right or center
revisions block
the revisions block rev block is a tabulated list of the revisions versions of the drawing documenting the revision control
traditional locations for the revisions block are the top right most commonly or adjoining the title block in some way
next assembly
the next assembly block often also referred to as where used or sometimes effectivity block is a list of higher assemblies where the product on the current drawing is used this block is commonly found adjacent to the title block
notes list
the notes list provides notes to the user of the drawing conveying any information that the callouts within the field of the drawing did not it may include general notes flagnotes or a mixture of both
traditional locations for the notes list are anywhere along the edges of the field of the drawing
general notes
general notes g n gn apply generally to the contents of the drawing as opposed to applying only to certain part numbers or certain surfaces or features
flagnotes
flagnotes or flag notes fl f n are notes that apply only where a flagged callout points such as to particular surfaces features or part numbers typically the callout includes a flag icon some companies call such notes delta notes and the note number is enclosed inside a triangular symbol similar to capital letter delta fl flagnote and d delta note are typical ways to abbreviate in ascii only contexts
field of the drawing
the field of the drawing f d fd is the main body or main area of the drawing excluding the title block rev block and so on
list of materials bill of materials parts list
the list of materials l m lm lom bill of materials b m bm bom or parts list p l pl is a usually tabular list of the materials used to make a part and or the parts used to make an assembly it may contain instructions for heat treatment finishing and other processes for each part number sometimes such loms or pls are separate documents from the drawing itself
traditional locations for the lom bom are above the title block or in a separate document
parameter tabulations
some drawings call out dimensions with parameter names that is variables such a a b c then tabulate rows of parameter values for each part number
traditional locations for parameter tables when such tables are used are floating near the edges of the field of the drawing either near the title block or elsewhere along the edges of the field
views and sections
each view or section is a separate set of projections occupying a contiguous portion of the field of the drawing usually views and sections are called out with cross references to specific zones of the field
zones
often a drawing is divided into zones by a grid with zone labels along the margins such as a b c d up the sides and along the top and bottom names of zones are thus for example a d or b this feature greatly eases discussion of and reference to particular areas of the drawing
abbreviations and symbols
as in many technical fields a wide array of abbreviations and symbols have been developed in engineering drawing during the th and st centuries for example cold rolled steel is often abbreviated as crs and diameter is often abbreviated as dia d or
example of an engineering drawing
here is an example of an engineering drawing an isometric view of the same object is shown above the different line types are colored for clarity
sectional views are indicated by the direction of arrows as in the example right side
references
engineering drawing practices asme y

reference dimension

a reference dimension is a dimension on an engineering drawing provided for information only reference dimensions are provided for a variety of reasons and are often an accumulation of other dimensions that are defined elsewhere e g on the drawing or other related documentation these dimensions may also be used for convenience to identify a single dimension that is specified elsewhere e g on a different drawing sheet
reference dimensions are not intended to be used directly to define the geometry of an object reference dimensions do not normally govern manufacturing operations such as machining in any way and therefore do not typically include a dimensional tolerance though a tolerance may be provided if such information is deemed helpful consequently reference dimensions are also not subject to dimensional inspection under normal circumstances
prior to use of modern computer aided design cad software reference dimensions were traditionally indicated on a drawing by the abbreviation ref written adjacent to the dimension typically to the right or underneath the dimension however with the assistance of modern cad software the abbreviation ref has often been replaced with the use of parentheses around the dimension as an example a distance of millimeters might be denoted by mm instead of mm ref although both of these methods of denoting a reference dimension are accepted and still in use today it is increasingly common to see the use of parentheses instead of the abbreviation ref primarily due to widespread use of modern cad software that makes use of parentheses as the default denotation method whenever reference dimensions are automatically created by the software

spin engineering

spin engineering describes the control and manipulation of quantum spin systems to develop devices and materials this includes the use of the spin degrees of freedom as a probe for spin based phenomena
because of the basic importance of quantum spin for physical and chemical processes spin engineering is relevant for a wide range of scientific and technological applications current examples range from bose einstein condensation to spin based data storage and reading in state of the art hard disk drives as well as from powerful analytical tools like nuclear magnetic resonance spectroscopy and electron paramagnetic resonance spectroscopy to the development of magnetic molecules as qubits and magnetic nanoparticles in addition spin engineering exploits the functionality of spin to design materials with novel properties as well as to provide a better understanding and advanced applications of conventional material systems many chemical reactions are devised to create bulk materials or single molecules with well defined spin properties such as a single molecule magnet
the aim of this article is to provide an outline of fields of research and development where the focus is on the properties and applications of quantum spin
introduction
as spin is one of the fundamental quantum properties of elementary particles it is relevant for a large range of physical and chemical phenomena for instance the spin of the electron plays a key role in the electron configuration of atoms which is the basis of the periodic table of elements the origin of ferromagnetism is also closely related to the magnetic moment associated with the spin and the spin dependent pauli exclusion principle thus the engineering of ferromagnetic materials like mu metals or alnico at the beginning of the last century can be considered as early examples of spin engineering although the concept of spin was not yet known at that time spin engineering in its generic sense became possible only after the first experimental characterization of spin in the stern gerlach experiment in followed by the development of relativistic quantum mechanics by paul dirac this theory was the first to accommodate the spin of the electron and its magnetic moment
whereas the physics of spin engineering dates back to the groundbreaking findings of quantum chemistry and physics within the first decades of the th century the chemical aspects of spin engineering have received attention especially within the last twenty years today researchers focus on specialized topics such as the design and synthesis of molecular magnets or other model systems in order to understand and harness the fundamental principles behind phenomena such as the relation between magnetism and chemical reactivity as well as microstructure related mechanical properties of metals and the biochemical impact of spin e g photoreceptor proteins and spin transport
research fields of spin engineering
spintronics
spintronics is the exploitation of both the intrinsic spin of the electron and its fundamental electronic charge in solid state devices and is thus a part of spin engineering spintronics is probably one of the most advanced fields of spin engineering with many important inventions which can be found in end user devices like the reading heads for magnetic hard disk drives this section is divided in basic spintronic phenomena and their applications
applications of spintronics
this section is devoted to current and possible future applications of spintronics which make use of one or the combination of several basic spintronic phenomena
spin materials
materials which properties are determined or strongly influenced by quantum spin
spin based detection
methods to characterize materials and physical or chemical processes via spin based phenomena

applied physics

applied physics is physics which is intended for a particular technological or practical use
it is usually considered as a bridge or a connection between physics and engineering
applied is distinguished from pure by a subtle combination of factors such as the motivation and attitude of researchers and the nature of the relationship to the technology or science that may be affected by the work
it usually differs from engineering in that an applied physicist may not be designing something in particular but rather is using physics or conducting physics research with the aim of developing new technologies or solving an engineering problem this approach is similar to that of applied mathematics in other words applied physics is rooted in the fundamental truths and basic concepts of the physical sciences but is concerned with the utilization of these scientific principles in practical devices and systems
applied physicists can also be interested in the use of physics for scientific research for instance the field of accelerator physics can contribute to research in theoretical physics by enabling design and construction of high energy colliders

transient response

in electrical engineering and mechanical engineering a transient response or natural response is the response of a system to a change from equilibrium the transient response is not necessarily tied to on off events but to any event that affects the equilibrium of the system the impulse response and step response are transient responses to a specific input an impulse and a step respectively
damping
the response can be classified as one of three types of damping that describes the output in relation to the steady state response
an underdamped response is one that oscillates within a decaying envelope the more underdamped the system the more oscillations and longer it takes to reach steady state here damping ratio is always
a critically damped response is the response that reaches the steady state value the fastest without being underdamped it is related to critical points in the sense that it straddles the boundary of underdamped and overdamped responses here damping ratio is always equal to one there should be no oscillation about the steady state value in the ideal case
an overdamped response is the response that does not oscillate about the steady state value but takes longer to reach than the critically damped case
here damping ratio is
it is the response of a system with respect to the input as a function of time

imagineering

imagineering is a portmanteau combining the words imagination and engineering
the word is well known for its use within the name of walt disney imagineering however contrary to popular belief the term was neither coined by disney nor did it originate there the word was invented by alcoa around and appeared widely in numerous publications of several disciplines such as urban design geography and politics evolutionary economics corporate culture and futures studies
earliest usage
following world war ii alcoa created an internal imagineering program to encourage innovative usage of aluminum in order to keep up demand
a time magazine ad from february titled the place they do imagineering relates the origin
for a long time we've sought a word to describe what we all work at hard here at alcoa imagineering is the word imagineering is letting your imagination soar and then engineering it down to earth
other notable pre disney usages include an october mention in the new york times in an article titled christian imagineering a oxford english dictionary entry which cites an advertisement from the wall street journal and the use by artist arthur c radebaugh to describe his work which was mentioned in the article in the portsmouth times portsmouth ohio
other early usage includes richard f sailer's article brainstorming is imagination engineering written for the national carbon company management magazine and reprinted by the union carbide company
wed enterprises applied for a trademark for the term in claiming first use in
other uses
imagineering has also been used by

grubbing

grubbing or clearing refers to the removal of trees shrubs stumps and rubbish from the future right of way of a transportation corridor i e a highway cut lines or the footprint of a structure grubbing is performed following surveying and preceding construction

precision mechanics

precision mechanics also fine mechanics is an engineering discipline that deals with the design and construction of smaller precision machines often including measuring and control mechanisms of different kinds
the study may be further defined as the practices of rigid body kinematics to the positioning and holding of objects on the micrometre scale and smaller

operating deflection shape

operating deflection shape ods is a term often used in the structural vibration analysis known as ods analysis ods analysis is a method used for visualisation of the vibration pattern of a machine or structure as influenced by its own operating forces this is as opposed to the study of the vibration pattern of a machine under an known external force analysis which is called modal analysis

boundary friction

boundary friction occurs when a surface is at least partially wet but not so lubricated that there is no direct friction between two surfaces
the effect
when two consistent unlubricated surfaces slide against each other there is a specific predictable amount of friction that occurs this amount increases as velocity does but only up to a certain point that increase generally follows what is known as a stribeck curve after richard stribeck on the other hand if the two surfaces are completely lubricated there is no direct friction or rubbing at all in real life though there is often a situation where the surfaces are not completely dry but also not so lubricated that they don't touch
this boundary friction produces various effects like an increase in lubrication through the generation of shearing forces or an oscillation effect during motion as the friction increases and decreases
for example one can experience vibration when trying to break on a partially damp road or a cold glass that is slowly condensing moisture can be lifted until it spontaneously slides across the surface it's resting on

corrosion engineering

corrosion engineering is the specialist discipline of applying scientific knowledge natural laws and physical resources in order to design and implement materials structures devices systems and procedures to manage the natural phenomenon known as corrosion generally related to metallurgy corrosion engineering also relates to non metallics including ceramics corrosion engineers often manage other not strictly corrosion processes including but not restricted to cracking brittle fracture crazing fretting erosion and more
corrosion engineering groups have formed around the world in order to prevent slow and manage the effects of corrosion examples of such groups are the national association of corrosion engineers nace and the european federation of corrosion efc see corrosion societies the corrosion engineers main task is to economically and safely manage the effects of corrosion on materials corrosion engineering masters degree courses are available worldwide and are concerned with the control and understanding of corrosion
zaki ahmad in his book principles of corrosion engineering and corrosion control states that corrosion engineering is the application of the principles evolved from corrosion science to minimize or prevent corrosion corrosion engineering involves designing of corrosion prevention schemes and implementation of speci c codes and practices corrosion prevention measures like cathodic protection designing to prevent corrosion and coating of structures fall within the regime of corrosion engineering however corrosion science and engineering go hand in hand and they cannot be separated it is a permanent marriage to produce new and better methods of protection from time to time in the handbook of corrosion engineering the author pierre r roberge states corrosion is the destructive attack of a material by reaction with its environment the serious consequences of the corrosion process have become a problem of worldwide significance
curtin university offer a postgraduate certificate in corrosion engineering in corrosion engineering and state corrosion engineering offers exciting career opportunities for engineers scientists and researchers corrosion to metals as an example of corrosion had cost the u s economy nearly billion per year in it is an issue particularly pertinent to the energy and resource industries where optimal corrosion management practices can have a positive impact on maximising efficient safe production
references
corrosion for students of science and engineering tretheway k r chamberlain j
corrosion vols and metal environment reactions edited by l l shrier pub butterworth heinemann ltd
corrosion engineering mars guy fontana mcgraw hill
handbook of corrosion engineering by pierre r roberge
corrosion engineering principles and practice pierre r roberge mcgraw hill prof med tech
curtin university postgraduate certificate in corrosion engineering http courses curtin edu au course overview postgraduate corrosion eng
corrosion engineering by volkan cicek author publisher wiley scrivener edition april inc asin b jjul lc
corrosion engineering principles and practice kindle edition pierre roberge author publisher mcgraw hill professional edition march asin b g hek

steel moment resisting frame

moment resisting frames are rectilinear assemblages of beams and columns with the beams rigidly connected to the columns resistance to lateral forces is provided primarily by rigid frame action that is by the development of bending moment and shear force in the frame members and joints by virtue of the rigid beam column connections a moment frame cannot displace laterally without bending the beams or columns depending on the geometry of the connection the bending rigidity and strength of the frame members is therefore the primary source of lateral stiffness and strength for the entire frame
the northridge earthquake revealed a common flaw in the construction and building codes were revised to strengthen them
early history
steel moment frames have been in use for more than one hundred years dating to the earliest use of structural steel in building construction steel building construction with the frame carrying the vertical loads initiated with the home insurance building in chicago a story structure constructed in with a height of ft often credited with being the first skyscraper this and other tall buildings in chicago spawned an entire generation of tall buildings constructed with load bearing steel frames supporting concrete floors and non load bearing unreinforced masonry infill walls at their perimeters framing in these early structures typically utilized h shapes built up from plates and l and z sections

contextualization computer science

in computer science contextualization is an initialization phase permitting at instantiation time to set or override properties having unknown or default values at the time of template creation
usage
templates permit to define generic capacities and behavior of objects and default values for some object properties without imposing strict limits on these properties
contextualization is an initialization phase permitting on instantiation of such a template to obtain the desired object with precisely customized properties

talascend

talascend is a global engineering resource company founded in headquartered in troy michigan the company has offices worldwide and employs people
the company operates predominantly in the oil gas power manufacturing automotive it healthcare it telecoms and rail engineering industries with a growing presence in nuclear energy
talascend was formed from the merger of three engineering recruitment companies in september epcglobal founded and owned by bechtel quality technical services the mobile based epc specialist and modern professional services a well known name in the detroit automotive market in business since
the talascend global training academy trains engineers and designers for jobs in other industries when jobs in their current area of expertise are in low demand the academy has successfully created jobs for engineering designers within the distressed detroit market
the oil gas industry is the key beneficiary of the cross trained engineers
talascend has retained its headquarters in detroit and has been actively involved in pushing the engineering and construction industry to make michigan a part of future planning beyond the auto industry
the company's australasia operation has offices in abu dhabi new delhi and brisbane in addition to four european offices in the united kingdom norway and switzerland the company's north american operations comprises offices seven in the united states and one in calgary alberta canada which began operations in late
the company has been a champion of renewable energy pressing for greater investment and development in the long term future of the global energy industry

glossary of engineering

this glossary of engineering terms is a list of definitions about the fundamentals of engineering its sub disciplines and related fields

improvisation

improvisation is the process of devising a solution to a requirement by making do despite absence of resources that might be expected to produce a solution in a technical context this can mean adapting a device for some use other than that which it was designed for or building a device from unusual components in an ad hoc fashion improvisation in the context of performing arts is spontaneous performance without specific preparation the skills of improvisation can apply to many different faculties across all artistic scientific physical cognitive academic and non academic disciplines
musical improvisation is usually defined as the composition of music while simultaneously singing or playing an instrument improvisational comedy is a theater art performed throughout the world and has had on again off again status throughout history dance improvisation as a choreographic tool improvisation was originally rarely used on dramatic television a major exception was the situation comedy mork and mindy where star robin williams famed for this kind of performing was allotted specific sections in each episode where he was allowed to perform freely
improvisation also exists outside the arts improvisation in engineering is to solve a problem with the tools and materials immediately at hand improvised weapons are often used by guerrillas insurgents and criminals
engineering
improvisation in engineering is to solve a problem with the tools and materials immediately at hand examples of such improvisation was the re engineering of carbon dioxide scrubbers with the materials on hand during the apollo space mission or the use of a knife in place of a screwdriver to turn a screw
engineering improvisations may be needed because of emergencies embargo obsolescence of a product and the loss of manufacturer support or just a lack of funding appropriate for a better solution users of motor vehicles in parts of africa develop improvised solutions where it is not feasible to obtain manufacturer approved spare parts
the popular television program macgyver used as its gimmick a hero who could solve almost any problem with jury rigged devices from everyday materials a swiss army knife and some duct tape
performing arts
improvisation can be thought of as an on the spot or off the cuff spontaneous moment of sudden inventiveness that can just come to mind body and spirit as an inspiration no preparation or training is needed however improvisation in any life or art form can occur more often if it is practiced as a way of encouraging creative behavior that practice includes learning to use one's intuition as well as learning a technical understanding of the necessary skills and concerns within the domain in which one is improvising this can be when an individual or group is acting dancing singing playing musical instruments talking creating artworks problem solving or reacting in the moment and in response to the stimulus of one's immediate environment and inner feelings this can result in the invention of new thought patterns new practices new structures or symbols and or new ways to act
skills and techniques
the skills of improvisation can apply to many different abilities or forms of communication and expression across all artistic scientific physical cognitive academic and non academic disciplines for example improvisation can make a significant contribution in music dance cooking presenting a speech sales personal or romantic relationships sports flower arranging martial arts psychotherapy and much more
techniques of improvisation are widely used in training for performing arts or entertainment for example music theatre and dance to extemporize or ad lib is basically the same as improvising colloquial terms such as let's play it by the ear take it as it comes and make it up as we go along are all used to describe improvisation
the simple act of speaking requires a good deal of improvisation because the mind is addressing its own thought and creating its unrehearsed delivery in words sounds and gestures forming unpredictable statements that feed back into the thought process the performer as listener creating an enriched process that is not unlike instantaneous composition with a given set or repertoire of elements
where the improvisation is intended to solve a problem on a temporary basis the proper solution being unavailable at the time it may be known as a stop gap this applies to the field of engineering another improvisational group problem solving technique being used in organizations of all kinds is brainstorming in which any and all ideas that a group member may have are permitted and encouraged to be expressed regardless of actual practicality as in all improvisation the process of brainstorming opens up the minds of the people involved to new unexpected and possibly useful ideas the colloquial term for this is thinking outside the box
music
improvisation is usually defined as the composition of music while simultaneously singing or playing an instrument in other words the art of improvisation can be understood as composing music on the fly improvisation can take place as a solo performance or interdependently in ensemble with other players when done well it often elicits gratifying emotional responses from the audience one notable improvisational pianist is franz liszt the origins of liszt's improvisation in an earlier tradition of playing variations on a theme were mastered and epitomized by johann sebastian bach wolfgang amadeus mozart and ludwig van beethoven
notable improvisational musicians from the modern era include keith jarrett an improvisational jazz pianist and multi instrumentalist who has performed many completely improvised concerts all over the world w a mathieu aka william allaudin mathieu was the musical director for the second city in chicago the first on going improvisational theater troupe in the united states and later was musical director for another improv theater the committee improv group an off shoot of the second city in san francisco derek bailey an improvisational guitarist stephen nachmanovitch an improvisational violinist and eugene friesen an improvisational cellist
improvised freestyle rap is commonly practiced as a part of rappers creative processes as a finished product for release on recordings when the improvisation is judged good enough as a spiritual event as a means of verbal combat in battle rap and simply for fun it often incorporates insults similar to those in the african american game the dozens and complex rhythmic and sometimes melodic forms comparable to those heard in jazz improvisation
a few pianists have given modern recitals of improvisation in the baroque style there have also been a few other exceptional improvised solo piano concerts in stuttgart southern germany in the s
in the realm of silent film music there are also a small number of musicians whose work has been recognized as exceptional by critics scholars and audiences alike these include neil brand and john sweeney among others who are all performers at le giornate del cinema muto the annual conference on silent film in pordenone italy their performances must match the style and pacing of those films which they accompany and the knowledge of a wide range of musical styles is required as well as the stamina to play for films which occasionally run more than three hours in length without a pause
theatre
comedy
improvisational comedy is a theater art performed throughout the world and has had on again off again status throughout history
some of the more famous improv theaters and training centers in the world include i o formerly improvolympic in chicago and los angeles the second city in chicago and toronto the players workshop in chicago national comedy theatre in san diego new york and phoenix upright citizens brigade the peoples improv theater the groundlings bats improv bay area theatre sports in san francisco wing it productions in seattle philly improv theater in philadelphia brave new workshop in minneapolis comedysportz in milwaukee and theatresports in calgary canada
there are also many well known university improv teams including theatre strike force at the university of florida gigglepants at the university of texas at austin and erasable inc at the university of maryland improvisation found a home at universities the origins of the second city was the compass players an off shoot of theater programs at the university of chicago in the s later once improv had been established as an art form improv groups sprung up on college campuses starting in the s where crowds were easy to find and teams could perform frequently now an improv group is a common staple of college extra curricular activities
notable pioneers in the field of improvisation comedic or otherwise include mike myers neil mullarkey paul merton stephen fry john sessions josie lawrence viola spolin paul sills david shepherd del close josephine forsberg gary austin martin de maat and keith johnstone notable performers include paul merton stephen colbert steve carell bill murray harold ramis robert townsend colin mochrie ryan stiles ross noble eddie izzard tony slattery mike mcshane sandi toksvig wayne brady jonathan winters tj jagodowski and david pasquesi
dance
dance improvisation as a choreographic tool improvisation is used as a choreographic tool in dance composition experimenting with the concepts of shape space time and energy while moving without inhibition or cognitive thinking can create unique and innovative movement designs spatial configuration dynamics and unpredictable rhythms improvisation without inhibition allows the choreographer to connect to their deepest creative self which in turn clears the way for pure invention
contact improvisation a form developed in that is now practiced around the world contact improvisation originated from the movement studies of steve paxton in the s and developed through the continued exploration of the judson dance theater it is a dance form based on weight sharing partnering playing with weight exploring negative space and unpredictable outcomes
poetry
traditional epic poetry included improvisation moments where the reciter flattered the audience especially the authorities or to substitute a forgotten passage there are also societies that value improvised poetry as a genre often as a debate or poetic joust where improvisators compete for public approval some of these impromptu poems are later recorded in paper or transmitted orally
some of these forms also include humour but michel ducom established himself within bordeaux poetical improvisation movement in the s but has since composed and performed with a wide range of poets working in diverse poetical areas bernat manciet serge pey m ryl marchetti the emergence of poetical improvisation like previous developments in french poetry was largely tied to the free jazz experience
sculpture
sculpture often relies on the enlargement of a small model or maquette to create the final work in a chosen material where the material is plastic such as clay a working structure or armature often needs to be built to allow the pre determined design to be realized alan thornhill's method for working with clay abandons the maquette seeing it as ultimately deadening to creativity without the restrictions of the armature a clay matrix of elements allows that when recognisable forms start to emerge they can be essentially disregarded by turning the work allowing for infinite possibility and the chance for the unforeseen to emerge more powerfully at a later stage
moving from adding and taking away to purely reductive working the architectural considerations of turning the work are eased considerably but continued removal of material through the rejection of forms deemed too obvious can mean one ends up with nothing former pupil jon edgar uses thornhill's method as a creative extension to direct carving in stone and wood
film
the director mike leigh uses lengthy improvisations developed over a period of weeks to build characters and story lines for his films he starts with some sketch ideas of how he thinks things might develop but does not reveal all his intentions with the cast who discover their fate and act out their responses as their destinies are gradually revealed including significant aspects of their lives which will not subsequently be shown onscreen the final filming draws on dialogue and actions that have been recorded during the improvisation period
the film company act cam uses improvisation to create the characters contexts and plot for their films improvisation also forms a large part of the final filmed product
television
improvisation was originally rarely used on dramatic television a major exception was the situation comedy mork and mindy where star robin williams famed for this kind of performing was allotted specific sections in each episode where he was allowed to perform freely
in the s a tv show called whose line is it anyway popularized shortform comedic improvisation the original version aired on british television but it was later revived and popularized in the united states with drew carey as its host with improvisation becoming a more common aspect of television there have been television shows which have garnered great success by utilizing partial improvisation to create longer form programs with more dramatic flavor while some shows are completely improvised in terms of lines including the office parks and recreation curb your enthusiasm significant others the loop sons daughters items or less dog bites man halfway home reno the league free ride campus ladies lovespring international players and after lately
in canada the global television soap opera train based on the australian series going home uses a form of structured improvisation in which actors improvise dialog from written plot outlines australia's thank god you're here is a game show where celebrities are put into scenes they know nothing about and have to improvise
writing
improvisational writing is an exercise that imposes limitations on a writer such as a time limit word limit a specific topic or rules on what can be written this forces the writer to work within stream of consciousness and write without judgment of the work they produce this technique is used for a variety of reasons such as to bypass writer's block improve creativity strengthen one's writing instinct and enhance one's flexibility in writing
some improvisational writing is collaborative focusing on an almost dadaist form of collaborative fiction this can take a variety of forms from as basic as passing a notebook around a circle of writers with each writing a sentence to coded environments that focus on collaborative novel writing like otherspace
improvised weapons
improvised weapons are often used by guerrillas insurgents and criminals as conventional weapons may be unavailable such weapons vary in sophistication from simple sharpened sticks to petrol bombs and homemade napalm to ieds and makeshift bomber aircraft weapons are also improvised by regular military organizations and formations as stop gap measures when purpose built equipment is either not on hand or is simply not yet available

applied engineering field

applied engineering is the field concerned with the application of management design and technical skills for the design and integration of systems the execution of new product designs the improvement of manufacturing processes and the management and direction of physical and or technical functions of a firm or organization applied engineering degreed programs typically include instruction in basic engineering principles project management industrial processes production and operations management systems integration and control quality control and statistics
on completion of an applied engineering program students will demonstrate the following management competencies that clearly distinguish them from traditional engineering graduates
use appropriate statistical techniques in variable and attribute control charts and in sampling tables for continuous improvement
evaluate and or implement total quality systems in industry
perform production scheduling develop and monitor an inventory control system utilize appropriate production planning techniques and identify and exhibit key factors in project management
exhibit knowledge of federal and state safety legislation and identify the role of management in an industrial safety program
recognize evaluate and control varied industrial health and safety hazards
demonstrate knowledge of traditional management functions and practices including applications and limitations of various management schemes
solve problems in typical industrial organizations work effectively in teams and demonstrate knowledge of the managed area of an industrial enterprise
apply business marketing and economic principles to solve problems
identify responsibility of supervision and management within various industries
demonstrate communication skills safe and efficient individual and group work habits leadership within groups and an attitude of cooperation and tolerance
applied engineering students specialize with a technical area of study and blend hands on process experience with theory examples of these technical specialties include automation robotics aviation computer aided drafting design electro mechanical electronics construction graphic communications manufacturing nanofabrication
applied engineers are employed in a large and wide array of industries including manufacturing construction transportation healthcare printing publishing and distribution they are responsible for implementing a design or process improvement although a degree in applied engineering is not considered a traditional design engineering degree those eligible to sit for the professional engineering examination it is quite common for employers to hire applied engineering and technology graduates with the term engineer in their job titles examples of the use of applied engineering titles include applications engineers control engineers manufacturing engineers process engineers product engineers safety engineers and sales engineers
graduates of applied engineering programs are frequently found in management positions due to their coursework training and experience in economics statistics financial accounting operations management quality management industrial safety and supervision common management related titles may also include engineering managers team leaders plant managers project managers supervisors technical managers
applied engineers are prepared to take an engineering design and see it through implementation and execution they wear many hats in industry commanding the necessary resources and personnel to contribute to an organization's bottom line
accreditation and certification
the association of technology management and applied engineering atmae accredits selected collegiate programs in applied engineering an instructor or graduate of an applied engineering program may choose to become a certified technology manager ctm by sitting for a rigorous exam administered by atmae covering production planning and control safety quality and management supervision
atmae program accreditation is recognized by the council for higher education accreditation chea for accrediting applied engineering programs chea recognizes atmae in the us for accrediting associate baccalaureate and master s degree programs in technology applied technology engineering technology and technology related disciplines delivered by national or regional accredited institutions in the united states

engineering notation

engineering notation is a version of scientific notation in which the powers of ten must be multiples of three i e they are powers of a thousand but written as for example instead of as an alternative to writing powers of si prefixes can be used which also usually provide steps of a factor of a thousand
compared to normalized scientific notation one disadvantage of using si prefixes and engineering notation is that significant figures are not always readily apparent for example m and m cannot express the uncertainty distinctions between and m this can be solved by changing the range of the coefficient in front of the power from the common to in some cases this may be suitable in others it may be impractical in the previous example or mm would have been used to show uncertainty and significant figures it is also common to state the precision explicitly such as k
another example when the speed of light exactly m s by the definition of the meter and second is expressed as m s or km s then it is clear that it is between and km s but when using m s or km s km s or the unusual but short mm s this is not clear a possibility is using gm s convenient to write but somewhat impractical in understanding writing something large as a fraction of something even larger in a context of larger numbers expressed in the same unit this could be convenient but that is not applicable here
engineering notation like scientific notation generally can use the e notation such that

can be written as
e or e
the e or e should not be confused with the exponential e which holds a completely different significance in the latter case it would be shown that e

human error

human error has been cited as a primary cause or contributing factor in disasters and accidents in industries as diverse as nuclear power e g the three mile island accident aviation see pilot error space exploration e g the space shuttle challenger disaster and space shuttle columbia disaster and medicine see medical error prevention of human error is generally seen as a major contributor to reliability and safety of complex systems
definition
human error means that something has been done that was not intended by the actor not desired by a set of rules or an external observer or that led the task or system outside its acceptable limits in short it is a deviation from intention expectation or desirability logically human actions can fail to achieve their goal in two different ways the actions can go as planned but the plan can be inadequate leading to mistakes or the plan can be satisfactory but the performance can be deficient leading to slips and lapses however a mere failure is not an error if there had been no plan to accomplish something in particular
performance
human error and performance are two sides of the same coin human error mechanisms are the same as human performance mechanisms performance later categorized as error is done so in hindsight therefore actions later termed human error are actually part of the ordinary spectrum of human behaviour the study of absent mindedness in everyday life provides ample documentation and categorization of such aspects of behavior while human error is firmly entrenched in the classical approaches to accident investigation and risk assessment it has no role in newer approaches such as resilience engineering
categories
there are many ways to categorize human error
sources
the cognitive study of human error is a very active research field including work related to limits of memory and attention and also to decision making strategies such as the availability heuristic and other cognitive biases such heuristics and biases are strategies that are useful and often correct but can lead to systematic patterns of error
misunderstandings as a topic in human communication have been studied in conversation analysis such as the examination of violations of the cooperative principle and gricean maxims
organizational studies of error or dysfunction have included studies of safety culture one technique for analyzing complex systems failure that incorporates organizational analysis is management oversight risk tree analysis mort
controversies
some researchers have argued that the dichotomy of human actions as correct or incorrect is a harmful oversimplification of a complex phenomena a focus on the variability of human performance and how human operators and organizations can manage that variability may be a more fruitful approach newer approaches such as resilience engineering mentioned above highlight the positive roles that humans can play in complex systems in resilience engineering successes things that go right and failures things that go wrong are seen as having the same basis namely human performance variability a specific account of that is the efficiency thoroughness trade off principle etto principle which can be found on all levels of human activity in individual as well as collective

tecnun

the school of engineering at san sebasti n also known as tecnun is the technological campus of the university of navarra the school of engineering offers numerous degrees in engineering at its two campuses in the city of donostia san sebasti n the school is currently led by professor i igo puente
location
tecnun has two campuses in the basque city of san sebasti n one in the ibaeta neighbourhood and the other campus which houses the telecommunications department is located in at the san sebasti n technology park in miram n
tecnun along with iese business school in barcelona and madrid and issa in san sebasti n are the only university of navarra schools located outside of pamplona
ibaeta campus
tecnun's ibaeta campus has three buildings the main building built in which houses lecture halls and the administration the cit building built in which houses laboratories and the multipurpose building completed in additionally ceit centre of studies and technical research of gipuzkoa has a building on the campus and shares certain installations and resources with tecnun such as the library
the cit building contains lecture halls multiple electronics and mechanics laboratories a cafeteria meeting rooms and faculty offices
the main building contains administrative offices faculty offices two study halls four computer labs meeting rooms a chapel an audio visual room two conference halls and a photocopy centre among other facilities
the ibaeta campus has parking a txoko gastronomic society and a cafeteria
miram n campus
located on paseo mikeletegi in the san sebasti n technology park in miram n this campus opened its doors on march with the prince and princess of asturias and viana presiding over the opening ceremony
this campus is dedicated to the study and technological development of microelectronics and telecommunications the facilities and resources are shared with ceit
founding
the school of engineering at san sebasti n also called tecnun the technological campus of the university of navarra began its academic activity in the spring of with the first specialisation course in metallurgy for postgraduates in october of the same year the regular courses for the industrial engineering degree began and in october the regular courses for the degree programme in telecommunications engineering began
initially the campus was located in the building that currently houses the koldo mitxelena library in the plaza del buen pastor currently tecnun tecnun has two campuses the university campus located in the ibaeta neighbourhood of san sebasti n has a laboratory building which opened in a main teaching and administrative building which was built in and the multipurpose building which was built in the other campus is located in the san sebasti n technology park in miram n the building which houses the telecommunications and microsystems laboratories was completed in march
in the academic year the school of engineering at san sebasti n began offering three official degrees automation and industrial electronics engineering industrial management engineering and materials engineering the degree plan was changed in s starting in that academic year tecnun tecnun offered several specialisations in industrial engineering unofficial degrees from the university of navarra which made double degrees possible
the school of engineering at san sebasti n began offering a degree in telecommunications engineering in this degree programme focuses on the design construction and operation of telecommunications equipment systems and services or other equipment systems and services that involve similar technologies such as radio communication and telematics in bioengineering was added to the list of specialisations in telecommunications engineering starting in tecnun tecnun expanded its degree offerings to include athe graduate master s in biomedical engineering this official master s degree is jointly taught with other centres at the university of navarra the schools of medicine sciences and pharmacy the cl nica universitaria deof navarra and the center for applied medical research cima this degree provides highly qualified training to professionals in engineering physical sciences mathematics etc so they are able to apply the principles and fundamentals of engineering and physics to solving existing and emerging issues in medicine biology and bio sanitary technology
the research carried out at the school of engineering is undertaken in collaboration with the centre of studies and technical research of gipuzkoa ceit with whom the school shares laboratories and a library this allows scientific speculation a necessary feature of a university to be complemented by the practical interests of a centre like ceit which focuses on meeting the needs of industry
degrees
tecnun tecnun offers five official degrees that are recognised by the spanish ministry of education at both undergraduate and doctorate levels
these degree programmes are currently being phased out and will be gradually replaced by the new undergraduate and master s degrees
beginning with the academic year tecnun tecnun began offering its new four year undergraduate and master s degree programmes which conform to the european higher education area ehea
the undergraduate degrees are
the postgraduate degrees are
other teaching and training activities seminars courses and master s programmes are offered to postgraduates

advanced composite materials engineering

advanced composite materials acms are also known as advanced polymer matrix composites these are generally characterized or determined by unusually high strength fibres with unusually high stiffness or modulus of elasticity characteristics compared to other materials while bound together by weaker matrices these are termed advanced composite materials acm in comparison to the composite materials commonly in use such as reinforced concrete or even concrete itself the high strength fibers are also low density while occupying a large fraction of the volume
advanced composites exhibit desirable physical and chemical properties that include light weight coupled with high stiffness elasticity and strength along the direction of the reinforcing fiber dimensional stability temperature and chemical resistance flex performance and relatively easy processing advanced composites are replacing metal components in many uses particularly in the aerospace industry
composites are classified according to their matrix phase these classifications are polymer matrix composites pmcs ceramic matrix composites cmcs and metal matrix composites mmcs also materials within these categories are often called advanced if they combine the properties of high axial longitudinal strength values and high axial longitudinal stiffness values with low weight corrosion resistance and in some cases special electrical properties
advanced composite materials have broad proven applications in the aircraft aerospace and sports equipment sectors even more specifically acms are very attractive for aircraft and aerospace structural parts acms have been developing for nasa's advanced space transportation program armor protection for army aviation and the federal aviation administration of the usa and high temperature shafting for the comanche helicopter additionally acms have a decades long history in military and government aerospace industries however much of the technology is new and not presented formally in secondary or undergraduate education and the technology of advanced composites manufacture is continually evolving
overview and historical perspective
manufacturing acms is a multibillion dollar industry worldwide composite products range from skateboards to components of the space shuttle the industry can be generally divided into two basic segments industrial composites and advanced composites several of the composites manufacturing processes are common to both segments the two basic segments are described below
industrial composites
the industrial composites industry has been in place for over years in the u s this large industry utilizes various resin systems including polyester epoxy and other specialty resins these materials along with a catalyst or curing agent and some type of fiber reinforcement typically glass fibers are used in the production of a wide spectrum of industrial components and consumer goods boats piping auto bodies and a variety of other parts and components
advanced composites
the advanced polymer matrix composites industry or advanced composite materials industry is characterized by the use of expensive high performance resin systems and high strength high stiffness fiber reinforcement the aerospace industry including military and commercial aircraft of all types is the major customer for advanced composites these materials have also been adopted for use by the sporting goods suppliers who sell high performance equipment to the golf tennis fishing and archery markets as well as in the swimming pool industry with composite wall structures
while aerospace is the predominant market for advanced composites today the industrial and automotive markets will increasingly see the use of advanced composites toward the year at present both manual and automated processes are employed in making advanced composite parts as automated processes become more predominant the costs of advanced composites are expected to decline to the point at which these materials will be used widely in electronic machinery and surface transportation equipment
suppliers of advanced composite materials tend to be larger companies capable of doing the research and development necessary to provide the high performance resin systems used in this segment of the industry end users also tend to be large and many are in the aircraft and aerospace businesses
thermosets and thermoplastics
advanced composite systems are divided into two basic types thermosets and thermoplastics thermosets are by far the predominant type in use today thermosets are subdivided into several resin systems including epoxies phenolics polyurethanes and polyimides of these epoxy systems currently dominate the advanced composite industry
thermosets
thermoset resins require addition of a curing agent or hardener and impregnation onto a reinforcing material followed by a curing step to produce a cured or finished part once cured the part cannot be changed or reformed except for finishing some of the more common thermosets include epoxy polyurethanes phenolic and amino resins bismaleimides bmi polyimides polyamides
of these epoxies are the most commonly used in the industry epoxy resins have been in use in u s industry for over years epoxy compounds are also referred to as glycidyl compounds the epoxy molecule can also be expanded or cross linked with other molecules to form a wide variety of resin products each with distinct performance characteristics these resins range from low viscosity liquids to high molecular weight solids typically they are high viscosity liquids
the second of the essential ingredients of an advanced composite system is the curing agent or hardener these compounds are very important because they control the reaction rate and determine the performance characteristics of the finished part since these compounds act as catalysts for the reaction they must contain active sites on their molecules some of the most commonly used curing agents in the advanced composite industry are the aromatic amines two of the most common are methylene dianiline mda and sulfonyldianiline dds
several other types of curing agents are also used in the advanced composite industry these include aliphatic and cycloaliphatic amines polyaminoamides amides and anhydrides again the choice of curing agent depends on the cure and performance characteristics desired for the finished part polyurethanes are another group of resins used in advanced composite processes these compounds are formed by reacting the polyol component with an isocyanate compound typically toluene diisocyanate tdi methylene diisocyanate mdi and hexamethylene diisocyanate hdi are also widely used phenolic and amino resins are another group of pmc resins the bismaleimides and polyamides are relative newcomers to the advanced composite industry and have not been studied to the extent of the other resins
thermoplastics
thermoplastics currently represent a relatively small part of the acm industry they are typically supplied as nonreactive solids no chemical reaction occurs during processing and require only heat and pressure to form the finished part unlike the thermosets the thermoplastics can usually be reheated and reformed into another shape if desired
fiber reinforcements
fiber reinforcement materials are added to the resin system increase the tensile strength and stiffness of the finished part the selection of reinforcement material is based on the properties desired in the finished product these materials do not react with the resin but are a distinct and integral part of the advanced composite system
the three basic types of fiber reinforcement materials in use in the advanced composite industry are
fibers used in advanced composite manufacture come in various forms including tows yarns rovings chopped strands and woven fabric
mats each of these has its own special application when prepreg materials are used in parts manufacture woven fabric or mats are required in processes such as filament wet winding or pultrusion yarns and rovings are used
prepreg
prepregs are resin impregnated cloth mat or filaments in flat form that can be stored for later use the resin is often partially cured to a tack free state called b staging catalysts inhibitors flame retardants and other additives may be included to obtain specific end use properties and improve processing storage and handling characteristics
limitations
despite their strength and low weight composites have not been a miracle solution for aircraft structures composites are typically difficult to inspect for flaws some of them absorb moisture most importantly they can be prohibitively expensive primarily because they are labor intensive and often require complex and expensive fabrication machines aluminum by contrast is easy and inexpensive to manufacture and repair for example in a minor collision an aluminium component can often be hammered back into its original shape whereas a crunched fiberglass component will likely have to be completely replaced
aluminum has a relatively high fracture toughness allowing it to undergo large amounts of plastic deformation before failure composites on the other hand are less damage tolerant and undergo much less plastic deformation before failure an airplane made entirely from aluminum can be repaired almost anywhere this is not the case for composite materials particularly as they use different and more exotic materials because of this composites will probably always be used more in military aircraft which are constantly being maintained than in commercial aircraft which have to require less maintenance aluminum still remains a remarkably useful material for aircraft structures and metallurgists have worked hard to develop better aluminum alloys for example aluminium lithium alloys

manifold fluid mechanics

a manifold is a wide and or bigger pipe or channel into which smaller pipes or channels lead
types of manifolds in engineering include
in biology manifolds are found in
manifolds are used in

placebo button

a placebo button is a push button with apparent functionality that actually has no effect when pressed such buttons can be psychologically rewarding to pressers by giving an illusion of control they are commonly placed in situations where it would have once been useful to have such a button but the system now proceeds automatically
in some cases the button may have been functional but may have failed or been disabled during installation or maintenance or was in a relatively small number of cases installed to keep people contented much in the same way as placebos
in many cases a button may appear to do nothing but in fact cause behavior that is not immediately apparent this can give the appearance of it being a placebo button
walk buttons
many walk buttons at pedestrian crossings were once functional in new york city but now serve as placebo buttons
in the united kingdom and hong kong pedestrian push buttons on crossings using the split cycle offset optimisation technique may or may not have any real effect on crossing timings depending on their location and the time of day and some junctions may be completely automated with push buttons which do not have any effect at all in other areas the buttons only have an effect during the night
london underground train door buttons
london underground stock stock and stock include door control buttons the doors are normally driver operated but a switch in the driving cab can hand control to passengers once the driver activates the buttons much like mainline railway stock in addition london underground d stock used on the district line were built with door open buttons which worked much like those of the and stock these buttons were subsequently removed when the stock was refurbished trains on the central line dlr and overground all have buttons with current central line stock retaining both open and closed buttons that are no longer active
office thermostats
it has been reported that the temperature set point adjustment on thermostats in many office buildings in the united states is non functional installed to give tenants employees a similar illusion of control in some cases they act as input devices to a central control computer but in others they serve no purpose other than to keep employees contented
a common implementation in buildings with an hvac central control computer is to allow the thermostats to provide a graded level of control temperatures in such a system are governed by the central controller's settings which are typically set by the building maintenance staff or hvac engineers the individual thermostats in various offices provide the controller with a temperature reading of the zone provided the thermocouples are not installed as inline duct sensors but also serve as modifiers for the central controller's set point while the thermostat may include settings from for example the actual effect of the thermostat is to apply pressure to the central controller's set point thus if the controller's setting is setting the thermostat to its maximum warm or cool settings will deflect the output temperature generally by only a few degrees fahrenheit about two degrees celsius at most so although the thermostat can be set to its lowest marking of in reality it may only change the hvac system's output temperature to in this case the thermostat has a swing of f c it can alter the produced temperature from the main controller's set point by a maximum of f c in either direction consequently while not purely a placebo the thermostat in this setup does not provide the level of control that is expected but the combination of the lower setting number and the feeling of a slight change in temperature can induce the office occupants to believe that the temperature was significantly decreased
placebo thermostats work on two psychological principles which are classical conditioning and the placebo effect first placebo thermostats work in accordance with classical conditioning classical conditioning was first discovered by ivan pavlov and is a type of learning which pairs a stimulus with a physiological response applied to placebo thermostats this is when the employee adjusts the thermostat and they hear the noise of hissing or a fan running and they will physically feel more content this is due to the countless trials involving the thermostat in their own home which actually works the employee has paired the sound of hissing or a fan running to being more physically content due to the actual temperature change and therefore when they experience the noise at work they feel the same way even though there is no change in temperature as long as individuals get the result they are looking for noise associated with temperature change they will continue with the practice changing the placebo thermostat additionally placebo thermostats work due to the placebo effect the placebo effect works on the basis that individuals will experience what they believe they will experience this is attributed to expectancy theory which states that the placebo effect is mediated by overt expectancies the most common example is in medical testing sugar pills are given to patients and told they are actual medicine and some patients will experience relief from symptoms regardless this is applicable to placebo thermostats because if people believe they are going to experience a temperature change after changing the thermostat they may psychologically experience one without an actual change happening to apply the expectancy theory because individuals consciously expect a temperature change to occur after changing the thermostat they will experience one both psychological concepts of classical conditioning and the placebo effect may play a role in the effectiveness of placebo thermostats

engineering design management

engineering design management is a knowledge area within engineering management it represents the adaptation and application of customary management practices with the intention of achieving a productive engineering design process engineering design management is primarily applied in the context of engineering design teams whereby the activities outputs and influences of design teams are planned guided monitored and controlled

transition engineering

transition engineering is an engineering discipline that enables change from unsustainable existing systems to more sustainable ones just as safety considerations are incorporated into design parameters in all engineering fields sustainability thinking is also built in transition engineering is emerging as a field to give engineers the tools necessary to address sustainability in design transition engineering is a cross disciplinary field that addresses the issues of future resource availability and identifies then realizes opportunities in resilience and adaptation
overview
engineering professions emerge when new technologies new problems or new opportunities arise this was the case when safety engineering grew in the early s to combat the high workplace injury and fatality rates in the s environmental engineering emerged as a discipline to reduce industrial pollution and mitigate impacts on environmental health and water quality quality engineering came about with the increase in mass production techniques during wwii and the need to confirm the quality of the products
there are two serious problems driving the emergence of transition engineering the exponential growth in the concentration of carbon dioxide in earth s atmosphere and the lack of growth and imminent decline of conventional oil production known as peak oil the concentration of carbon dioxide in the atmosphere recently exceeded ppm a level that earth has not known for years transition engineering aims to take advantage of the current access to the remaining lower cost and higher eroi energy resources to re develop all aspects of urban and industrial engineered systems to adapt as fossil fuel use is dramatically reduced
origins
the idea behind transition engineering has sprouted from many different roots both technical and non technical the concept of sustainable development has been around since and the problem of sustainability was a driving force in the development of transition engineering the transition town movement provided further inspiration as it showed that there were many groups of people around the world motivated to prepare for peak oil and climate change transition towns and ecovillages demonstrate the need for engineers to build systems that manage un sustainable risks and provide people with sustainable options engineers are ethically required to hold paramount the safety health and welfare of the public and answer society's need for sustainable development
the origins of safety engineering provided much of the inspiration for transition engineering at the beginning of the s business owners viewed workplace safety as a wasted investment and politicians were slow to change after the triangle shirtwaist factory fire in new york city killed trapped workers engineers came together to investigate how to make the workplace a safer place to be this eventually lead to the formation of the american society of safety engineers
as safety engineering manages the risks of unsafe conditions transition engineering manages the risks of unsustainable conditions to give engineers a better grasp of sustainability transition engineering defines the problem as un sustainability this is similar to the problem of un safe conditions that is the purpose of safety engineering we do not necessarily know what a perfectly safe system looks like but we do know what unsafe systems look like and how to improve them the same applies to unsustainability of systems by reducing unsustainability issues we take steps in the right direction
the seven step method
the transition engineering method involves steps to help engineers develop projects to deal with changing unsustainable activities as a discipline transition engineering recognizes that business as usual projections of future scenarios from past trends are not valid because the underlying conditions have changed sufficiently from the conditions of the past for example the projection of future oil supply in from data prior to would give an expectation of a increase in demand over that time frame however the actual production rate of conventional oil has not increased since and is projected to decline by more than by
global association for transition engineering gate
the gate opened the first group in the uk in feb the gate is an emerging network of engineers and non engineers that share the idea that engineers are responsible for changing engineered systems in order to adapt to reducing fossil fuel and other un sustainable resources transition engineering is a change management discipline like safety engineering transition engineering uses and audit and stock take of current system design and operation to quantify the risks to essential activities and resources over a time frame of study the time frame of study should be commensurate with the lifetime of the assets involved in the activity an activity is anything that the engineered system supports for example manufacturing sewage treatment mobility or food preservation transition engineering recognizes that the analytical methods of strategic analysis over a life cycle time frame are at odds with most economic analyses that discount values with time the strategic analysis carried out by transition engineers seeks to avoid stranded investment by recognizing resource risks a classic example of stranded investments is the north atlantic cod fishery where the largest number of bottom trawling ships e g those ships responsible for destroying the cod spawning beds were manufactured in the year that the fish stocks collapsed
see also
systems engineering
safety engineering
engineering design process
sustainable transport

standard dimension ratio

standard dimension ratio sdr is a method of rating a pipe's durability against pressure the standard dimension ratio describes the correlation between the pipe dimension and the thickness of the pipe wall
common nominations are sdr sdr and sdr pipes with a lower sdr can withstand higher pressures
formula
formula pipe outside diameter
formula pipe wall thickness

oxygen compatibility

oxygen compatibility is the issue of compatibility of materials for service in high concentrations of oxygen it is a critical issue in space aircraft medical underwater diving and industrial applications
aspects include effects of increased oxygen concentration on the ignition and burning of materials and components exposed to these concentrations in service
understanding of fire hazards is necessary when designing operating and maintaining oxygen systems so that fires can be prevented ignition risks can be minimized by controlling heat sources and using materials that will not ignite or will not support burning in the applicable environment some materials are more susceptible to ignition in oxygen rich environments and compatibility should be assessed before a component is introduced into an oxygen system
the issues of cleaning and design are closely related to the compatibility of materials for safety and durability in oxygen service
prevention of fire
fires occur when oxygen fuel and heat energy combine in a self sustaining chemical reaction in an oxygen system the presence of oxygen is implied and in a sufficiently high partial pressure of oxygen most materials can be considered fuel potential ignition sources are present in almost all oxygen systems but fire hazards can be mitigated by controlling the risk factors associated with the oxygen fuel or heat which can limit the tendency for a chemical reaction to occur
materials are easier to ignite and burn more readily as oxygen pressure or concentration increase so operating oxygen systems at the lowest practicable pressure and concentration may be enough to avoid ignition and burning
use of materials which are inherently more difficult to ignite or are resistant to sustained burning or which release less energy when they burn can in some cases eliminate the possibility of fire or minimize the damage caused by a fire
although heat sources may be inherent in the operation of an oxygen system initiation of the chemical reaction between the system materials and oxygen can be limited by controlling the ability of those heat sources to cause ignition design features which can limit or dissipate the heat generated to keep temperatures below the ignition temperatures of the system materials will prevent ignition
an oxygen system should also be protected from external heat sources
assessment of oxygen compatibility
the process of assessment of oxygen compatibility would generally include the following stages
compatibility analysis would also consider the history of use of the component or material in similar conditions or of a similar component
research
hazards analyses are performed on materials components and systems and failure analyses determine the cause of fires results are used in design and operation of safe oxygen systems

darcy number

in fluid dynamics through porous media the darcy number da represents the relative effect of the permeability of the medium versus its cross sectional area commonly the diameter squared the number is named after henry darcy and is found from nondimensionalizing the differential form of darcy's law this number should not be confused with the darcy friction factor which applies to pressure drop in a pipe it is defined as
where
alternate forms of this number do exist depending on the approach that darcy's law is made dimensionless and the geometry of the system the darcy number is commonly used in heat transfer through porous media

plasmonic lens

in nano optics a plasmonic lens generally refers to a lens for surface plasmon polaritons spps i e a device that redirects spps to converge towards a single focal point since spps can have very small wavelength they can converge into a very small and very intense spot much smaller than the free space wavelength and the diffraction limit
a simple example of a plasmonic lens is a series of concentric rings on a metal film any light that hits the film from free space at normal incidence will get coupled into a spp this part works like a grating coupler and that spp will be heading towards the center of the circles which is the focal point another example is a tapered dimple
in a novel plasmonic lens is proposed by modulating light phase through a metallic film with arrayed nano slits which have constant depth but variant widths the slits transport electromagnetic energy in the form of spps in nanometric waveguides and provide desired phase retardations of beam manipulating with variant phase propagation constant the scientists claim that it is an improvement over other subwavelength imaging techniques such as superlenses where the object and image are confined to the near field
these devices have been suggested for various applications that take advantage of the small size and high intensity of the spps at the focal point these include photolithography heat assisted magnetic recording microscopy biophotonics biological molecule sensors and solar cells as well as other applications
the term plasmonic lens is also sometimes used to describe something different any free space lens i e a lens that focuses free space light rather than spps that has something to do with plasmonics these often come up in discussions of superlenses

resilience engineering and construction

resilience is a design objective for buildings and infrastructure it is the ability to absorb or avoid damage without suffering complete failure

structural integrity and failure

structural integrity and failure is an aspect of engineering which deals with the ability of a structure to support a designed load weight force etc without breaking tearing apart or collapsing and includes the study of breakage that has previously occurred in order to prevent failures in future designs
structural integrity is the term used for the performance characteristic applied to a component a single structure or a structure consisting of different components structural integrity is the ability of an item to hold together under a load including its own weight resisting breakage or bending it assures that the construction will perform its designed function during reasonable use for as long as the designed life of the structure items are constructed with structural integrity to ensure that catastrophic failure does not occur which can result in injuries severe damage death or monetary losses
structural failure refers to the loss of structural integrity which is the loss of the load carrying capacity of a component or member within a structure or of the structure itself structural failure is initiated when the material is stressed beyond its strength limit thus causing fracture or excessive deformations in a well designed system a localized failure should not cause immediate or even progressive collapse of the entire structure ultimate failure strength is one of the limit states that must be accounted for in structural engineering and structural design
introduction
structural integrity is the ability of a structure or a component to withstand a designed service load resisting structural failure due to fracture deformation or fatigue structural integrity is a concept often used in engineering to produce items that will not only function adequately for their designed purposes but also to function for a desired service life
to construct an item with structural integrity an engineer must first consider the mechanical properties of a material such as toughness strength weight hardness and elasticity and then determine a suitable size thickness or shape that will withstand the desired load for a long life a material with high strength may resist bending but without adequate toughness it may have to be very large to support a load and prevent breaking however a material with low strength will likely bend under a load even though its high toughness prevents fracture a material with low elasticity may be able to support a load with minimum deflection flexing but can be prone to fracture from fatigue while a material with high elasitcity may be more resistant to fatigue but may produce too much deflection unless the object is drastically oversized
structural integrity must always be considered in engineering when designing buildings gears or transmissions support structures mechanical components or any other item that may bear a load the engineer must carefully balance the properties of a material with its size and the load it is intended to support bridge supports for instance need good yield strength whereas the bolts that hold them need good shear and tensile strength springs need good elasticity but lathe tooling needs high rigidity and minimal deflection when applied to a structure the integrity of each component must be carefully matched to its individual application so that the entire structure can support its load without failure due to weak links when a weak link breaks it can put more stress on other parts of the structure leading to cascading failures
history
the need to build structure with integrity goes back as far as recorded history houses needed to be able to support their own weight plus the weight of the inhabitants castles needed to be fortified to withstand assaults from invaders tools needed to be strong and tough enough to do their jobs however it was not until the s that the science of fracture mechanics namely the brittleness of glass was described by alan arnold griffith even so a real need for the science did not present itself until world war ii when over welded steel ships broke in half due to brittle fracture caused by a combination of the stresses created from the welding process temperature changes and the stress points created at the square corners of the bulkheads the squared windows in the de havilland comet aircraft of the s caused stress points which allowed cracks to form causing the pressurized cabins to explode in mid flight failures in pressurized boiler tanks were a common problem during this era causing severe damage the growing sizes of bridges and buildings began to lead to even greater catastrophes and loss of life the need to build constructions with structural integrity led to great advances in the fields of material sciences and fracture mechanics
types of failure
failure of a structure can occur from many types of problems most of these problems are unique to the type of structure or to the various industries however most can be traced to one of five main causes
notable failures
bridges
dee bridge
on may the new railway bridge over the river dee collapsed as a train passed over it with the loss of lives it was designed by robert stephenson using cast iron girders reinforced with wrought iron struts the bridge collapse was the subject of one of the first formal inquiries into a structural failure the result of the inquiry was that the design of the structure was fundamentally flawed as the wrought iron did not reinforce the cast iron at all and that owing to repeated flexing the casting had suffered a brittle failure due to fatigue
first tay rail bridge
the dee bridge disaster was followed by a number of cast iron bridge collapses including the collapse of the first tay rail bridge on december like the dee bridge the tay collapsed when a train passed over it causing people to lose their lives the bridge failed because of poorly made cast iron and the failure of the designer thomas bouch to consider wind loading on the bridge the collapse resulted in cast iron largely being replaced by steel construction and a complete redesign in of the forth railway bridge as a result the forth bridge was the first entirely steel bridge in the world
first tacoma narrows bridge
the collapse of the original tacoma narrows bridge is sometimes characterized in physics textbooks as a classic example of resonance although this description is misleading the catastrophic vibrations that destroyed the bridge were not due to simple mechanical resonance but to a more complicated oscillation between the bridge and winds passing through it known as aeroelastic flutter robert h scanlan father of the field of bridge aerodynamics wrote an article about this misunderstanding this collapse and the research that followed led to an increased understanding of wind structure interactions several bridges were altered following the collapse to prevent a similar event occurring again the only fatality was tubby the dog
i w bridge
the i w mississippi river bridge officially known simply as bridge was an eight lane steel truss arch bridge that carried interstate w across the mississippi river in minneapolis minnesota united states the bridge was completed in and its maintenance was performed by the minnesota department of transportation the bridge was minnesota's fifth busiest carrying vehicles daily the bridge catastrophically failed during the evening rush hour on august collapsing to the river and riverbanks beneath thirteen people were killed and were injured following the collapse the federal highway administration advised states to inspect the u s bridges of similar construction after a possible design flaw in the bridge was discovered related to large steel sheets called gusset plates which were used to connect girders together in the truss structure officials expressed concern about many other bridges in the united states sharing the same design and raised questions as to why such a flaw would not have been discovered in over years of inspections
buildings
thane building collapse
on april a building collapsed on tribal land in mumbra a suburb of thane in maharashtra india it has been called the worst building collapse in the area people died including children women and men while more than people survived the search for additional survivors ended on april
the building was under construction and did not have an occupancy certificate for its to low to middle income residents
living in the building were the site construction workers and families it was reported that the building was illegally constructed because standard practices were not followed for safe lawful construction land acquisition and resident occupancy
by april a total of suspects were arrested including builders engineers municipal officials and other responsible parties governmental records indicate that there were two orders to manage the number of illegal buildings in the area a maharashtra state order to use remote sensing and a bombay high court order there were also complaints made to state and municipal officials
on april a campaign began by the thane municipal corporation to demolish area illegal buildings focusing first on dangerous buildings the forest department said that it will address encroachment of forest land in the thane district a call centre was established by the thane municipal corporation to accept and track resolution of caller complaints about illegal buildings
savar building collapse
on april rana plaza an eight story commercial building collapsed in savar a sub district in the greater dhaka area the capital of bangladesh the search for the dead ended on may with the death toll of approximately injured people were rescued from the building alive
it is considered to be the deadliest garment factory accident in history as well as the deadliest accidental structural failure in modern human history
the building contained clothing factories a bank apartments and several other shops the shops and the bank on the lower floors immediately closed after cracks were discovered in the building warnings to avoid using the building after cracks appeared the day before had been ignored garment workers were ordered to return the following day and the building collapsed during the morning rush hour
sampoong department store collapse
on june the story sampoong department store in the seocho district of seoul south korea collapsed resulting in the deaths of people in april cracks began to appear in the ceiling of the fifth floor of the store's south wing due to the presence of an air conditioning unit on the weakened roof of the poorly built structure on the morning of june as the number of cracks in the ceiling increased dramatically the top floor was closed and managers shut the air conditioning off the store management failed to shut the building down or issue formal evacuation orders however the executives themselves left the premises as a precaution five hours before the collapse the first of several loud bangs was heard emanating from the top floors as the vibration of the air conditioning caused the cracks in the slabs to widen further amid customer reports of vibration the air conditioning was turned off but the cracks in the floors had already grown to cm at about p m local time the fifth floor ceiling began to sink by p m the roof gave way and the air conditioning unit crashed through into the already overloaded fifth floor trapping more than people and killing
ronan point
on may the story residential tower ronan point in the london borough of newham collapsed when a relatively small gas explosion on the th floor caused a structural wall panel to be blown away from the building the tower was constructed of precast concrete and the failure of the single panel caused one entire corner of the building to collapse the panel was able to be blown out because there was insufficient reinforcement steel passing between the panels this also meant that the loads carried by the panel could not be redistributed to other adjacent panels because there was no route for the forces to follow as a result of the collapse building regulations were overhauled to prevent disproportionate collapse and the understanding of precast concrete detailing was greatly advanced many similar buildings were altered or demolished as a result of the collapse
oklahoma city bombing
on april the nine story concrete framed alfred p murrah federal building in oklahoma was struck by a huge car bomb causing partial collapse resulting in the deaths of people the bomb though large caused a significantly disproportionate collapse of the structure the bomb blew all the glass off the front of the building and completely shattered a ground floor reinforced concrete column see brisance at second story level a wider column spacing existed and loads from upper story columns were transferred into fewer columns below by girders at second floor level the removal of one of the lower story columns caused neighbouring columns to fail due to the extra load eventually leading to the complete collapse of the central portion of the building the bombing was one of the first to highlight the extreme forces that blast loading from terrorism can exert on buildings and led to increased consideration of terrorism in structural design of buildings
versailles wedding hall
the versailles wedding hall located in talpiot jerusalem is the site of the worst civil disaster in israel's history at on thursday night may during the wedding of keren and asaf dror a large portion of the third floor of the four story building collapsed
world trade center towers and
in the september attacks two commercial airliners were deliberately crashed into the twin towers of the world trade center in new york city the impact and resulting fires caused both towers to collapse within two hours after the impacts had severed exterior columns and damaged core columns the loads on these columns were redistributed the hat trusses at the top of each building played a significant role in this redistribution of the loads in the structure the impacts dislodged some of the fireproofing from the steel increasing its exposure to the heat of the fires temperatures became high enough to weaken the core columns to the point of creep and plastic deformation under the weight of higher floors perimeter columns and floors were also weakened by the heat of the fires causing the floors to sag and exerting an inward force on exterior walls of the building wtc building also collapsed later that day according to the official report the story skyscraper collapsed within seconds due to a combination of a large fire inside the building and heavy structural damage from the collapse of the north tower
aircraft
repeated structural failures of aircraft types occurred in when two de havilland comet c jet airliners crashed due to decompression caused by metal fatigue and in when the vertical stabilizer on four boeing b bombers broke off in mid air
other
warsaw radio mast
on august at utc warsaw radio mast the tallest man made object ever built before the erection of burj khalifa collapsed as consequence of an error in exchanging the guy wires on the highest stock the mast first bent and then snapped at roughly half its height it destroyed at its collapse a small mobile crane of mostostal zabrze as all workers left the mast before the exchange procedures there were no fatalities in contrast to the similar collapse of wlbt tower in
hyatt regency walkway
on july two suspended walkways through the lobby of the hyatt regency in kansas city missouri collapsed killing and injuring more than people at a tea dance the collapse was due to a late change in design altering the method in which the rods supporting the walkways were connected to them and inadvertently doubling the forces on the connection the failure highlighted the need for good communication between design engineers and contractors and rigorous checks on designs and especially on contractor proposed design changes the failure is a standard case study on engineering courses around the world and is used to teach the importance of ethics in engineering

self powered dynamic systems

a self powered dynamic system
is defined as a dynamic system powered by its own excessive kinetic energy renewable energy or a combination of both the particular area of work is the concept of fully or partially self powered dynamic systems requiring zero or reduced external energy inputs the exploited technologies are particularly associated with self powered sensors regenerative actuators human powered devices and dynamic systems powered by renewable resources e g solar powered airships as self sustained systems various strategies can be employed to improve the design of a self powered system and among them adopting a bio inspired design is investigated to demonstrate the advantage of biomimetics in improving power density the concept of self powered dynamic systems is illustrated in the figure
the concept of self powered dynamic systems in the figure is described as follows
i input power e g fuel energy powering a vehicle engine or propulsion system or input excitation e g vibration excitation to a structure to the system the source of this input energy can be of renewable energy source e g solar power to a dynamic system
ii the kinetic energy in the direction of motion of a dynamic system is only recovered if the system is stationery e g a bridge structure or the recoverable energy is negligible in comparison with the power required for motion e g a low powered sensor
iii the movement of the dynamic system perpendicular to the desired direction of the motion is usually the wasted kinetic energy in the system e g the vertical motion of an automobile suspension is wasted to heat energy in the shock absorbers or vibration of an aircraft wing is converted into heat energy through structural damping
iv the vertical movement of the dynamic system is a source of recoverable kinetic energy
v the recoverable kinetic energy can be converted to electrical energy through an energy conversion mechanism such as an electromagnetic scheme e g replacing the viscous damper of a car shock absorber with regenerative actuator piezoelectric e g embedding piezoelectric material in aircraft wings or electrostatic e g vibration of a micro cantilever in a mems sensor
vi the recovered electrical power can be stored or used as a power source
vii the recovered electrical energy can power subsystems of the dynamic system such as sensors and actuators
viii the recovered electrical power can be realized as an input to the dynamic system itself
such self powered scheme is particularly beneficial in development of self powered sensors and self powered actuators by employing energy harvesting techniques
where kinetic energy is converted to electrical energy through piezoelectric electromagnetic or electrostatic electromechanical mechanisms developing a self powered sensor eliminates the use of an external source of power such as a battery and therefore can be considered as a self sustained system a self sustained system does not required maintenance e g replacing the battery of the sensor at the end of the battery life this is particularly beneficial in remote sensing and applications in hostile or inaccessible environments

general rating of city appeal

the general rating of city appeal is a method for calculating and comparing the city appeal and urban environment based on determining their number values and the threshold of appeal the method is based on collection description and assessment of qualitative and quantitative indices
quantitative indices are represented by annual statistical data on the cities of the russian federation whereas qualitative characteristics are represented by own properties of a city the purpose of the rating is to carry out an integrated collective and public analysis of the urban environment compare urban environments analyze strengthens and weaknesses and to prepare information for managerial analysis and to make personnel decisions
description
the general rating of city appeal for russian cities was designed by the russian union of engineers in the rating was designed due to the objective need to conduct a complex analysis and assess disproportions in distribution of population labour resources and production capacity in the russian federation
the purpose of the rating is to carry out an unbiased and complex appraisal of russian cities in terms of the criteria which determine the quality level of all spheres of activity of the population city economy and urban environment
tasks
rating methodology
the methodology of appraisal of city appeal is based on quality assessment of elements of the city economy and comfort of living of citizens the main method is a procedure borrowed from qualimetry in which several approaches to quantitative appraisal of quality were developed
to compare various properties measured in scales different in range and dimension formula a relative dimensionless ratio is used to show the degree of approximation of formula absolute property indicator to the maximum formula and minimum formula the relative ratio is described as the following relationship formula which may be represented as a normalized function as follows formula
to compare the relativities of all properties included in the tree of properties formula dimensionless importance ratios are used for the purposes of convenience the following is usually assumed formula and formula
the values of importance ratios are determined with the help of expert and non expert analytical methods this study combines both techniques as described below the relative importance was determined using a method of expert survey involving experts of various industries and areas of business different social and professional status
thus the qualitative appraisal of quality formula is expressed using the following formula
formula where
formula for all indices forming a part of the final general index of city appeal
the general city appeal index is directly calculated as an average compound of all characteristics
formula
the key problem of selection of the minimum combination of properties indicators forming a part of the item quality may be solved through the functional and typological analysis considering the quality as a system of objective properties and based on the volume of the original information statistical and open access information
therefore to conduct the appraisal quasisimple properties were selected forming overall indices which are in their turn interpreted as indices
ranking results
the top ten cities are characterized by high indicators in nearly all the aspects the only exception is the housing affordability factor in terms of which the leading cities have low indicators because of high prices for square meter of residential space the cities leading in terms of most indicators are million plus cities moscow saint petersburg novosibirsk yekaterinburg kazan krasnoyarsk rostov on don and others cities having development advantages vladivostok krasnodar sochi and kaliningrad cities that are attractive from the point of view of investments tomsk omsk surgut tyumen irkutsk yaroslavl saratov and moscow region cities with high construction rate podolsk khimki balashikha and mytishchi
the medium ranked cities are the cities characterized by dynamic development kaluga krasnodar kislovodsk industrial cities pervouralsk chelyabinsk ulyanovsk kamensk uralsky shakhty the single industry city of naberezhnye chelny as well as bryansk ryazan vologda and yoshkar ola
the following cities are noted for satisfactory development levels orsk ulan ude orenburg sterlitamak syzran ussuriysk oktyabrsky votkinsk single industry cities magnitogorsk nizhni tagil and the single industry city having the highest investment inflow nakhodka
the bottom ranked cities in most subratings are the north caucasus cities kaspiysk and yessentuki cities of the altai territory rubtsovsk barnaul biysk single industry cities leninsk kuznetsky and severodvinsk as well as artyom miass novocheboksarsk and kopeisk yamalo nenets autonomous district cities novy urengoy and noyabrsk in spite of high economic indicators generally lose on of the indicators
overall ranking indicates considerable disproportions in city potential which becomes clear if we delete population dynamics indices from the rating thus if we exclude this parameter the potential of the st city will be more than twice as high as of the th city and times higher than the potential of the th city evidently such a high difference is determined by objective difference of potentials of the cities it is also important to notice that in accordance with the pareto principle it is not obligatory to improve all the components of qualitative appraisal of cities here the key aspect is economic potential
it is also necessary to compare some social factors first of all the development of healthcare education social services because these are the key indicators
the overall ranking of cities in the rating shows that even absolute leaders are not so far from the cities in the middle of the rating this is caused by leveling of low indicators of parameters of some leaders in particular the value of the general index of omsk which ranks th is just times by higher than that of the mid city mezhdurechensk the only exception is moscow the value of the general indicator is times higher than that of mezhdurechensk times higher than that of the th city omsk and twice as high as of the rd city novosibirsk
besides if we analyze the total of indicators of all the cities as their total potential then the weight of moscow the leader of the rating is and the weight of grozny the last in the rating is
for most cities it can be said that the level of economic and industrial development would differ very much depending on living conditions this is true for nearly all cities for example syberia and ural are centers for hydrocarbon production and metal mining with the highest income rates per capita these cities are characterized by the worst natural climatic conditions and low developed transport system
the group of mid cities comprises rather different cities ranging from the largest metallurgic mono cities chelyabinsk magnitogorsk kamensk uralsky t cities with quite versatile economy vladimir yuzhno sakhalinsk volzhsky on the whole this group consists of either industrially developed centers with negative natural and climatic conditions or mid cities of old cultivated regions characterized by primarily average values of all indicators
the lowest indices are found among underdeveloped cities of the north caucasus and south siberia this is connected primarily with agricultural specialization of the economy of these cities and adverse effect of conflicts which took place on the territory of the north caucasus the group of depressed cities also includes former large and middle sized industrial centers which are primarily mono cities zlatoust metallurgy leninsk kuznetsky metallurgy norilsk metallurgy severodvinsk defense industry complex in money terms the income difference may be more than by times and as a rule not in favour of underdeveloped cities where income growth rate may be observed primarily in budget sectors and general income growth rate is low in addition real income of population grows faster which is determined by constant production growth in innovation industries
theoretical implications
the study and appraisal of urban environment namely the parameter of appeal for internal and external factors and consumers is of value
practical implications of the rating
on november in the polytechnical museum of moscow the russian union of engineers presented the general rating of appeal of russian cities for which attracted much interest among the general public mass media and local regional and federal authorities
on may the ministry of regional development of the russian federation the federal agency of construction and housing of the russian federation russian union of engineers and the experts of lomonosov moscow state university developed a rating and methodology of evaluation of urban environment and analyzed largest russian cities for this work the union designed the methods of urban environment quality assessment and a city appeal threshold
the rating has been developed

pdf e

iso is an iso standard published in
this standard defines a format pdf e for the creation of documents used in geospatial construction and manufacturing workflows ref name pdf e iso standard ref and is based on the pdf reference version from adobe systems the specification also supports interactive media including animation and d
pdf e is a subset of pdf designed to be an open and neutral exchange format for engineering and technical documentation ref name pdf e ready guide ref
description
the pdf e standard specifies how the portable document format pdf should be used for the creation of documents in engineering workflows
key benefits of pdf e include
the standard does not define a method for the creation or conversion from paper or electronic documents to the pdf e format
the committee managing iso pdf e needs subject matter experts to assist in the development of part of the standard
iso pdf e was created to meet the needs of organizations who need to reliably create exchange and review engineering documentation however the first part of the standard does not address d video or other dynamic content nor does it address integrated source data

solid state chemistry

solid state chemistry also sometimes referred to as materials chemistry is the study of the synthesis structure and properties of solid phase materials particularly but not necessarily exclusively of non molecular solids it therefore has a strong overlap with solid state physics mineralogy crystallography ceramics metallurgy thermodynamics materials science and electronics with a focus on the synthesis of novel materials and their characterization
history
because of its direct relevance to products of commerce solid state inorganic chemistry has been strongly driven by technology progress in the field has often been fueled by the demands of industry well ahead of purely academic curiosity applications discovered in the th century include zeolite and platinum based catalysts for petroleum processing in the s high purity silicon as a core component of microelectronic devices in the s and high temperature superconductivity in the s the invention of x ray crystallography in the early s by william lawrence bragg enabled further innovation our understanding of how reactions proceed at the atomic level in the solid state was advanced considerably by carl wagner's work on oxidation rate theory counter diffusion of ions and defect chemistry because of this he has sometimes been referred to as the father of solid state chemistry
synthetic methods
given the diversity of solid state compounds an equally diverse array of methods are used for their preparation for organic materials such as charge transfer salts the methods operate near room temperature and are often similar to the techniques of organic synthesis redox reactions are sometimes conducted by electrocrystallisation as illustrated by the preparation of the bechgaard salts from tetrathiafulvalene
oven techniques
for thermally robust materials high temperature methods are often employed for example bulk solids are prepared using tube furnaces which allow reactions to be conducted up to ca c special equipment e g ovens consisting of a tantalum tube through which an electric current is passed can be used for even higher temperatures up to c such high temperatures are at times required to induce diffusion of the reactants but this depends strongly on the system studied some solid state reactions already proceed at temperatures as low as c
melt methods
one method often employed is to melt the reactants together and then later anneal the solidified melt if volatile reactants are involved the reactants are often put in an ampoule that is evacuated often while keeping the reactant mixture cold e g by keeping the bottom of the ampoule in liquid nitrogen and then sealed the sealed ampoule is then put in an oven and given a certain heat treatment
solution methods
it is possible to use solvents to prepare solids by precipitation or by evaporation at times the solvent is used hydrothermally i e under pressure at temperatures higher than the normal boiling point a variation on this theme is the use of flux methods where a salt of relatively low melting point is added to the mixture to act as a high temperature solvent in which the desired reaction can take place
gas reactions
many solids naman react vigorously with reactive gas species like chlorine iodine oxygen etc others form adducts with other gases e g co or ethylene such reactions are often carried out in a tube that is open ended on both sides and through which the gas is passed a variation of this is to let the reaction take place inside a measuring device such as a tga in that case stoichiometric information can be obtained during the reaction which helps identify the products
a special case of a gas reaction is a chemical transport reaction these are often carried out in a sealed ampoule to which a small amount of a transport agent e g iodine is added the ampoule is then placed in a zone oven this is essentially two tube ovens attached to each other which allows a temperature gradient to be imposed such a method can be used to obtain the product in the form of single crystals suitable for structure determination by x ray diffraction
chemical vapor deposition is a high temperature method that is widely employed for the preparation of coatings and semiconductors from molecular precursors
air and moisture sensitive materials
many solids are hygroscopic and or oxygen sensitive many halides e g are very thirsty and can only be studied in their anhydrous form if they are handled in a glove box filled with dry and or oxygen free gas usually nitrogen
characterization
new phases phase diagrams structures
the synthetic methodology and the characterization of the product often go hand in hand in the sense that not one but a series of reaction mixtures are prepared and subjected to heat treatment the stoichiometry is typically varied in a systematic way to find which stoichiometries will lead to new solid compounds or to solid solutions between known ones a prime method to characterize the reaction products is powder diffraction because many solid state reactions will produce polycristalline ingots or powders powder diffraction will facilitate the identification of known phases in the mixture if a pattern is found that is not known in the diffraction data libraries an attempt can be made to index the pattern i e to identify the symmetry and the size of the unit cell if the product is not crystalline the characterization is typically much more difficult
once the unit cell of a new phase is known the next step is to establish the stoichiometry of the phase this can be done in a number of ways sometimes the composition of the original mixture will give a clue if one finds only one product a single powder pattern or if one was trying to make a phase of a certain composition by analogy to known materials but this is rare
often considerable effort in refining the synthetic methodology is required to obtain a pure sample of the new material
if it is possible to separate the product from the rest of the reaction mixture elemental analysis can be used another way involves sem and the generation of characteristic x rays in the electron beam the easiest way to solve the structure is by using single crystal x ray diffraction
the latter often requires revisiting and refining the preparative procedures and that is linked to the question which phases are stable at what composition and what stoichiometry in other words what does the phase diagram looks like an important tool in establishing this is thermal analysis techniques like dsc or dta and increasingly also thanks to the advent of synchrotrons temperature dependent powder diffraction increased knowledge of the phase relations often leads to further refinement in synthetic procedures in an iterative way new phases are thus characterized by their melting points and their stoichiometric domains the latter is important for the many solids that are non stoichiometric compounds the cell parameters obtained from xrd are particularly helpful to characterize the homogeneity ranges of the latter
further characterization
in many but certainly not all cases new solid compounds are further characterized by a variety of techniques that straddle the fine line that hardly separates solid state chemistry from solid state physics
optical properties
for non metallic materials it is often possible to obtain uv vis spectra in the case of semiconductors that will give an idea of the band gap

front end engineering

front end engineering
front end engineering fee or front end engineering design feed is an engineering design approach used to control project expenses and thoroughly plan a project before a fix bid quote is submitted it may also be referred to as pre project planning ppp front end loading fel feasibility analysis or early project planning
the fee is basic engineering which comes after the conceptual design or feasibility study the fee design focuses the technical requirements as well as rough investment cost for the project the fee can be divided into separate packages covering different portions of the project the fee package is used as the basis for bidding the execution phase contracts epc epci etc and is used as the design basis
a good fee will reflect all of the client's project specific requirements and avoid significant changes during the execution phase fee contracts usually take around year to complete for larger sized projects during the fee phase there is close communication between project owners and operators and the engineering contractor to work up the project specific requirements
front end engineering focuses on technical requirements and identifying main costs for a proposed project it is used to establish a price for the execution phase of the project and evaluate potential risks it is typically followed by detailed design or detailed engineering the amount of time invested in front end engineering is higher than a traditional quote because project specifications are thoroughly extracted and the following typically developed in detail
traditionally all of these documents would be developed in detail during a design review after a quote has been agreed to a company using fee will develop these materials before submitting a quote
front end engineering is typically used by design build engineering firms these firms may operate in various industries including
fee methodology
fee methodology
fee is a way of looking at a project before completing detailed design there is not set way to conduct a front end engineering study generally fee requires an engineer or a group of engineers to thoroughly and logically consider a proposed project example considerations may include

subsurface utility engineering

subsurface utility engineering sue is a branch of engineering practice that involves managing certain risks associated with utility mapping at appropriate quality levels utility coordination utility relocation design and coordination utility condition assessment communication of utility data to concerned parties utility relocation cost estimates implementation of utility accommodation policies and utility design
overview
the sue process begins with a work plan that outlines the scope of work project schedule levels of service vs risk allocation and desired delivery method non destructive surface geophysical methods are then leveraged to determine the presence of subsurface utilities and to mark their horizontal position on the ground surface vacuum excavation techniques are employed to expose and record the precise horizontal and vertical position of the assets this information is then typically presented in cad format or a gis compatible map a conflict matrix is also created to evaluate and compare collected utility information with project plans identify conflicts and propose solutions the concept of sue is gaining popularity worldwide as a framework to mitigate costs associated with project redesign and construction delays and to avoid risk and liability that can result from damaged underground utilities
history
the practice of collecting recording and managing subsurface data has historically been widely unregulated in response to this challenge in the american society of civil engineers asce developed standard guideline for the collection and depiction of existing subsurface utility data which defined the practice of sue many countries followed the u s lead by creating similar standards including malaysia canada australia and most recently great britain developed and refined over the last years sue classifies information according to quality levels with an objective to vastly improve data reliability this provides project owners and engineers with a benchmark to determine the integrity of utility data at the outset of an infrastructure project
standards that govern sue
a number of standards of care have been developed to govern the use of sue
asce standard
in the american society of civil engineers asce published standard titled the standard defined sue and set guidance for the collection and depiction of subsurface utility information the asce standard presents a system to classify the quality of existing subsurface utility data in accordance with four quality levels
malaysia standard guideline for underground utility mapping
the standard guideline for underground utility mapping in malaysia was launched in to create populate and maintain the national underground utility database
this standard addresses issues such as roles of stakeholders and how utility information can be obtained and was a call to action from the malaysian government due to increasing demands for improvements on basic infrastructure facilities including utilities
the standard is similar to asce using quality levels d a as its basis although it does not classify utility definition colours or symbols the malaysian standard does specify an accuracy cm for both horizontal and vertical readings the standard is supported by the malaysian government but is not backed by an association or governing body
csa standard s
in the canadian standards association csa released the standard is described as a collective framework for best practices to map depict and manage records across canada
csa s compliments and extends asce standard by setting out requirements for generating storing distributing and using mapping records to ensure that underground utilities are readily identifiable and locatable accuracy levels expand upon asce quality level a prescribing a finer level of detail to define the positional location of the infrastructure
standards australia committee as
in june the standards australia committee it on subsurface utility engineering information launched standard classification of subsurface utility information to provide utility owners operators and locators with a framework for the consistent classification of information concerning subsurface utilities the standard also provides guidance on how subsurface utility information can be obtained and conveyed to users
british standards institute pas
an industry consultation event in january kicked off the development of a british sue standard the first technical draft was reviewed by the committee in december and it was released for public general industry review in march pas applies to the detection verification and location of active abandoned redundant or unknown underground utilities and associated surface features that facilitate the location and identification of underground utility infrastructure it sets out the accuracy to which the data is captured for specific purposes the quality expected of that data and a means by which to assess and indicate the confidence that can be placed in the data
applications of sue
sue is mainly used at the design stage of a capital works project and when information is being collected for asset management purposes in both situations a similar process is followed but the scope of the work and presentation of the information may vary
when a sue investigation is carried out for a capital works project prior to construction the objective is generally to collect accurate utility information within the project area to avoid conflict at later stages of the project
for initiatives involving asset management project owners may be missing information about their underground utilities or have inaccurate data in this situation a sue provider would collect the required information and add it to the asset management database according to the four quality levels prescribed by asce standard

universality diversity paradigm

the universality diversity paradigm is the analysis of biological materials based on the universality and diversity of its fundamental structural elements and functional mechanisms the analysis of biological systems based on this classification has been a cornerstone of modern biology
for example proteins constitute the elementary building blocks of a vast variety of biological materials such as cells spider silk or bone where they create extremely robust multi functional materials by self organization of structures over many length and time scales from nano to macro some of the structural features are commonly found in a many different tissues that is they are highly conserved examples of such universal building blocks include alpha helices beta sheets or tropocollagen molecules in contrast other features are highly specific to tissue types such as particular
filament assemblies beta sheet nanocrystals in spider silk or tendon fascicles this coexistence of universality and diversity referred to as the universality diversity paradigm udp is an overarching feature in biological materials and a crucial component of materiomics it might provide guidelines for bioinspired and biomimetic material development where this concept is translated into the use of inorganic or hybrid organic inorganic building blocks

engineering procurement and construction

the epc contractor epcc agrees to deliver epc which is an acronym that stands for engineering procurement and construction it is a common form of contracting arrangement within the construction industry
background
under an epc contract the contractor designs the installation procures the necessary materials and builds the project either directly or by of the work in some cases the contractor carries the project risk for schedule as well as budget in return for a fixed price called lump sum turnkey lstk depending on the agreed scope of work
when the scope is restricted to engineering and procurement this is referred to as an ep e and p or e p contract this is often done in situations where the construction risk is too great for the contractor or when the owner does the construction
the keys to a commissioned plant are handed to the owner for an agreed amount just as a builder hands the keys of a flat to the purchaser one should recognise that some epc contracts terminate at mechanical completion but before commissioning while lstk contracts always include commissioning epc is gaining importance worldwide it requires good understanding by the epcc to return a profit an owner decides for an epc contract for reasons that include
besides the plant siting in an epc contract the owner defines
the cost the price to be paid to the epcc is negotiated and finalised and paid in mutually agreed installments
functions
engineering functions
procurement functions
construction functions
owner and contractor liabilities
once an epc contract is signed the epc contractor becomes liable for completing the project according to the tender conditions the epc contractor in turn may hire sub contractors or sub vendors to complete different portions payment commensurate with the work completed in addition to an advance is normally preferred by a contractor
projects are more likely to succeed when the owner
the contractor also has ways to improve project success
global arena
an epc contract is a complex agreement in a global context epc management is more complex the epcc must have data and expertise in all the required fields some important areas are
cost certainty
one main reason an owner may prefer an epc arrangement is certainty of cost an epc contract binds the coned however changes to specifications initiated by the owner better finishes for example may be incorporated thro changes and these changes prices are recorded in the change order document
owner responsibility
to ensure quality the owner must select an experienced epc contractor
changes in scope of work can affect project schedule cost and risk such changes are the responsibility of the owner

engineering procurement and construction management

epcm engineering procurement and construction management is a common form of contracting arrangement for very large projects within the infrastructure mining resources and energy industries in an epcm arrangement the client selects a head contractor who manages the whole project on behalf of the client the epcm contractor coordinates all design procurement and construction work and ensures that the whole project is completed as required and in time the epcm contractor may or may not undertake actual site work
an epcm contract is a natural progression for an epc contractor as if one is able to do an epc of a project then getting a bigger epcm job is advantageous it helps to tap the already present competencies while ensuring better control over the project also the value of the project managed through an epcm contract is far greater than the individual epc contracts
normally an epcm contractor completes the basic work such as site surveys getting clearances from authorities doing the basic engineering and preparing the site for the subcontractors subcontractors are chosen by the epcm company but they have an agreement directly with the final customer

instrumentation and control engineering

instrumentation is defined as the art and science of measurement and control of process variables within a production or manufacturing area the process variables used in industries are level pressure temperature humidity flow ph force speed etc
control engineering or control systems engineering is the engineering discipline that applies control theory to design systems with desired behaviors control engineers are responsible for the research design development and control devices systems typically in manufacturing facilities and plants the practice uses sensors to measure the output performance of the device being controlled and those measurements can be used to give feedback to the input actuators that can make corrections toward desired performance when a device is designed to perform without the need of human inputs for correction it is called automatic control such as cruise control for regulating a car's speed multi disciplinary in nature control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of systems of a diverse range
instrumentation and control engineering
this term refers to the graduate discipline which many universities provide at graduate and postgraduate level instrumentation and control plays a significant role in both gathering information from the field and changing the field parameters and as such are a key part of control loops the instrumentation technology being an inter disciplinary branch of engineering is heading towards development of new intelligent sensors smart transducers mems technology blue tooth technology this discipline finds its origin in both electrical and electronics engineering and it covers subjects related to electronics electrical mechanical chemical and computing streams in short it deals with measurement automation and control processes in today's scenario there are many people who are willing to make a career in this stream almost all process and manufacturing industry such as steel oil petrochemical power and defiance production will have a separate instrumentation and control department which is manned and managed by instrumentation and control engineers automation is the buzz word in process industry and automation is the core job of instrumentation and control engineers hence the demand for instrumentation will always be there
nature of job
an instrumentation and control engineer is required to
requirement
instrumentation and control engineers have a role to play in all the fields where there is automation the instruments created by control engineers to automate the processes thus reducing the involvement of manpower
an instrumentation and control engineer is expected to learn subjects like
knowledge of plc programming panel view hmi screens scada application programming preferred
goal of instrumentation and control engineers
instrumentation and control engineers works with the goal of improving
these engineers design develop and maintain and manage the instruments and the instrumentation systems

ausforming

ausforming also known as low and high temperature thermomechanical treatments is a method used to increase the hardness and stubbornness of an alloy by simultaneously tempering rapid cooling deforming and quenching to change its shape and refine the microstructure this treatment is an important part in the processing of steel

applied acoustics

applied acoustics is a scientific journal in the field of acoustics it is published by elsevier

repetition method

in surveying the repetition method is used to improve precision and accuracy of measurements of horizontal angles the same angle is measured multiple times with the survey instrument rotated so that systematic errors tend to cancel the arithmetic mean of these observations gives true value of an angle the precision of the measurement can exceed the least count of the instrument used
the repetition method is used when high accuracy is required for rough or approximate survey work the ordinary method of measuring horizontal angles is used as it is less time consuming

adjustments of theodolite

adjustments of theodolite may refer to

sand containers

geotextile containers are usually made of polypropylene needle punched staple fibres nonwovens and are used for flood emergency protection in dams and dikes and also as construction elements for erosion control bottom scour protection and scour fill artificial reefs groynes seawalls breakwaters and dune reinforcement
history
the use geotextile sand containers gscs as shoreline protection systems has grown moderately since the first applications in the s this slow growth can be attributed to two factors firstly the lack of understanding of coastal processes and design fundamentals by the larger geosynthetic community in
order to provide coastal engineers with suitable solutions and secondly there has been very little rigorous scientific wave flume testing with which to analyse the wave stability of geotextile sand containers
the application of geotextile containers in coastal protection works can be traced back to early works carried out in s the application of these types of structures was somewhat haphazard as very little was understood about the wave stability and durability of the structures early wave stability work was
carried out ray and jacobs with small containers however the testing programs were limited and did not provide sufficient confidence in the product to carry out exhaustive engineering design as a result the technology until recently has relied on manufacturers design suggestions based
on monitoring of actual structures
sand filled geotextile containers are a more recent development designed primarily as a user friendly alternative to traditional rock rubble structures or in applications where rock is not readily available the use of geosynthetic sand containers in coastal structures is increasing worldwide in a wide range of applications such as revetments coastal protection reefs surfing reefs breakwaters etc
today
over the past ten years coastal population pressure extreme events and concerns over climate change and sea level rise have resulted in more emphasis being placed on shoreline protection systems geotextile manufacturers have responded to the challenges put forward by design engineers and intensive research has been carried out in the field new shore protection structures especially at sandy coasts are increasingly needed

ranging rod

ranging rod is an surveying instrument used for marking the position of stations and for sightings of those stations as well as for ranging the straight lines initially these were made of light thin and straight bamboo or of well seasoned wood such as teak pine or deodar and were shod with iron at the bottom and surmounted with a flag about cm square in size but nowadays these are made of metallic materials only these are usually cm in diameter and m or m long painted alternatively either red and white or black and white in lengths of cm i e one link length of metric chain these colours are used so that the rod is properly sited in case of long distances and bad weather ranging rods of more length i e m to m are called as ranging poles and are used in in case of very long survey lines another type of ranging rod is known as offset rod in which there is no flag at the top it is used for measuring small offsets from the survey line when the work is of ordinary nature
notes
when the ranging rods are limited thin sticks cm to m length with white papers in the cuts at tops can serve their purpose such sticks are pointed at the bottom and are cut from wood these are called as whites

survey camp

survey camp is an army tradition that was discontinued in the later part of twentieth century but was reinstated in across the universities of the world with a whole new structure it is the civil engineering training course for two weeks usually after completion four semesters of bachelor of technology that consists of days working in the field and days of map preparation in the computer lab experts say that survey camp provides necessary foundation for civil engineer each day in the course there are at least hours of working in the field students are divided into groups and they get out with practising surveyors and use their equipment out in the field camp incharge teacher appoints group leaders in each group the leaders are responsible for all the works of his particular group and the equipments in the computer lab students learn applications such as autocad and carlson survey the students use these programs to take data collected from the field to develop topographic maps of the particular area the basic aim of the survey camp is to know various works carried out in the industrial field by surveying which includes determining the topography of particular area with the help of survey work map study and reconnaissance work the methods used for surveying are traversing levelling and contouring
instruments
the instruments used include
practicals
the survey practicals generally performed in survey camp are listed below
procedure
the working rule used in survey camp is quite simple all the groups of students are allotted different stations for survey work for first couple of days the students carry out traversing work to determine length of different sides of traverse and inclued angles between them coordinates of each side are also determined by formulas latitude l formula and departure l formula where l is the length of the side and formula is the angle which the side makes with north or south directions the instruments used in traversing include total station or theodolite with stand compass with stand ranging rods and measuring tape then in next step rl reduced level of each station is determined with reference to permanent benchmark the instruments used to determine reduced level include auto level with stand and levelling staff then rl of different points keeping specified spacing between them with reference to each station is determined the points having same rl are joined to form contour lines and thus topographic map of an area is prepared using autocad or with the help of plane table survey
results
in survey camp students obtain extensive hands on experience in the use of land surveying instruments and in the essentials of survey practice measurements of distances and angles calculation and correction of errors are introduced concepts of higher order surveys and global positioning systems are reviewed and illustrated thus students learn surveying practically this gives them confidence to work with good precision and accuracy in industrial fields in future

engineering

engineering from latin ingenium meaning cleverness and ingeniare meaning to contrive devise is the application of scientific economic social and practical knowledge in order to invent design build maintain research and improve structures machines devices systems materials and processes
the discipline of engineering is extremely broad and encompasses a range of more specialized fields of engineering each with a more specific emphasis on particular areas of applied science technology and types of application
meaning
the american engineers council for professional development ecpd the predecessor of abet has defined engineering as
the creative application of scientific principles to design or develop structures machines apparatus or manufacturing processes or works utilizing them singly or in combination or to construct or operate the same with full cognizance of their design or to forecast their behavior under specific operating conditions all as respects an intended function economics of operation or safety to life and property
one who practices engineering is called an engineer and those licensed to do so may have more formal designations such as professional engineer designated engineering representative chartered engineer incorporated engineer ingenieur or european engineer
history
engineering has existed since ancient times as humans devised fundamental inventions such as the wedge lever wheel and pulley each of these inventions is essentially consistent with the modern definition of engineering
the term engineering itself has a much more recent etymology deriving from the word engineer which itself dates back to when an engine'er literally one who operates an engine originally referred to a constructor of military engines in this context now obsolete an engine referred to a military machine i e a mechanical contraption used in war for example a catapult notable examples of the obsolete usage which have survived to the present day are military engineering corps e g the u s army corps of engineers
the word engine itself is of even older origin ultimately deriving from the latin ingenium c meaning innate quality especially mental power hence a clever invention
later as the design of civilian structures such as bridges and buildings matured as a technical discipline the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non military projects and those involved in the older discipline of military engineering
ancient era
the pharos of alexandria the pyramids in egypt the hanging gardens of babylon the acropolis and the parthenon in greece the roman aqueducts via appia and the colosseum teotihuac n and the cities and pyramids of the mayan inca and aztec empires the great wall of china the brihadeeswarar temple of thanjavur and tombs of india among many others stand as a testament to the ingenuity and skill of the ancient civil and military engineers
the earliest civil engineer known by name is imhotep as one of the officials of the pharaoh djos r he probably designed and supervised the construction of the pyramid of djoser the step pyramid at saqqara in egypt around bc
ancient greece developed machines in both civilian and military domains the antikythera mechanism the first known mechanical computer and the mechanical inventions of archimedes are examples of early mechanical engineering some of archimedes inventions as well as the antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing two key principles in machine theory that helped design the gear trains of the industrial revolution and are still widely used today in diverse fields such as robotics and automotive engineering
chinese greek and roman armies employed complex military machines and inventions such as artillery which was developed by the greeks around the th century b c the trireme the ballista and the catapult in the middle ages the trebuchet was developed
renaissance era
william gilbert is considered to be the first electrical engineer with his publication of de magnete he coined the term electricity
the first steam engine was built in by thomas savery the development of this device gave rise to the industrial revolution in the coming decades allowing for the beginnings of mass production
with the rise of engineering as a profession in the th century the term became more narrowly applied to fields in which mathematics and science were applied to these ends similarly in addition to military and civil engineering the fields then known as the mechanic arts became incorporated into engineering
modern era
the early stages of electrical engineering included the experiments of alessandro volta in the s the experiments of michael faraday georg ohm and others and the invention of the electric motor in the work of james maxwell and heinrich hertz in the late th century gave rise to the field of electronics the later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty
the inventions of thomas savery and the scottish engineer james watt gave rise to modern mechanical engineering the development of specialized machines and their maintenance tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace britain and abroad
john smeaton was the first self proclaimed civil engineer and is often regarded as the father of civil engineering he was an english civil engineer responsible for the design of bridges canals harbours and lighthouses he was also a capable mechanical engineer and an eminent physicist smeaton designed the third eddystone lighthouse where he pioneered the use of hydraulic lime a form of mortar which will set under water and developed a technique involving dovetailed blocks of granite in the building of the lighthouse his lighthouse remained in use until and was dismantled and partially rebuilt at plymouth hoe where it is known as smeaton's tower he is important in the history rediscovery of and development of modern cement because he identified the compositional requirements needed to obtain hydraulicity in lime work which led ultimately to the invention of portland cement
chemical engineering like its counterpart mechanical engineering developed in the nineteenth century during the industrial revolution industrial scale manufacturing demanded new materials and new processes and by the need for large scale production of chemicals was such that a new industry was created dedicated to the development and large scale manufacturing of chemicals in new industrial plants the role of the chemical engineer was the design of these chemical plants and processes
aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design its origins can be traced back to the aviation pioneers around the start of the th century although the work of sir george cayley has recently been dated as being from the last decade of the th century early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering
the first phd in engineering technically applied science and engineering awarded in the united states went to josiah willard gibbs at yale university in it was also the second phd awarded in science in the u s
only a decade after the successful flights by the wright brothers there was extensive development of aeronautical engineering through development of military aircraft that were used in world war i meanwhile research to provide fundamental background science continued by combining theoretical physics with experiments
in with the rise of computer technology the first search engine was built by computer engineer alan emtage
main branches of engineering
engineering is a broad discipline which is often broken down into several sub disciplines these disciplines concern themselves with differing areas of engineering work although initially an engineer will usually be trained in a specific discipline throughout an engineer's career the engineer may become multi disciplined having worked in several of the outlined areas engineering is often characterized as having four main branches
beyond these four a number of other branches are recognized historically naval engineering and mining engineering were major branches modern fields sometimes included as major branches are manufacturing engineering acoustical engineering corrosion engineering instrumentation and control aerospace automotive computer electronic petroleum systems audio software architectural agricultural biosystems biomedical geological textile industrial materials and nuclear engineering these and other branches of engineering are represented in the institutions forming the membership of the uk engineering council
new specialties sometimes combine with the traditional fields and form new branches for example earth systems engineering and management involves a wide range of subject areas including anthropology engineering studies environmental science ethics and philosophy a new or emerging area of application will commonly be defined temporarily as a permutation or subset of existing disciplines there is often gray area as to when a given sub field warrants classification as a new branch one key indicator of such emergence is when major universities start establishing departments and programs in the new field
for each of these fields there exists considerable overlap especially in the areas of the application of fundamental sciences to their disciplines such as physics chemistry and mathematics
methodology
engineers apply mathematics and sciences such as physics to find suitable solutions to problems or to make improvements to the status quo more than ever engineers are now required to have knowledge of relevant sciences for their design projects as a result they may keep on learning new material throughout their career
if multiple options exist engineers weigh different design choices on their merits and choose the solution that best matches the requirements the crucial and unique task of the engineer is to identify understand and interpret the constraints on a design in order to produce a successful result it is usually not enough to build a technically successful product it must also meet further requirements
constraints may include available resources physical imaginative or technical limitations flexibility for future modifications and additions and other factors such as requirements for cost safety marketability productibility and serviceability by understanding the constraints engineers derive specifications for the limits within which a viable object or system may be produced and operated
problem solving
engineers use their knowledge of science mathematics logic economics and appropriate experience or tacit knowledge to find suitable solutions to a problem creating an appropriate mathematical model of a problem allows them to analyze it sometimes definitively and to test potential solutions
usually multiple reasonable solutions exist so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements genrich altshuller after gathering statistics on a large number of patents suggested that compromises are at the heart of low level engineering designs while at a higher level the best design is one which eliminates the core contradiction causing the problem
engineers typically attempt to predict how well their designs will perform to their specifications prior to full scale production they use among other things prototypes scale models simulations destructive tests nondestructive tests and stress tests testing ensures that products will perform as expected
engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure however the greater the safety factor the less efficient the design may be
the study of failed products is known as forensic engineering and can help the product designer in evaluating his or her design in the light of real conditions the discipline is of greatest value after disasters such as bridge collapses when careful analysis is needed to establish the cause or causes of the failure
computer use
as with all modern scientific and technological endeavors computers and software play an increasingly important role as well as the typical business application software there are a number of computer aided applications computer aided technologies specifically for engineering computers can be used to generate models of fundamental physical processes which can be solved using numerical methods
one of the most widely used design tools in the profession is computer aided design cad software like catia autodesk inventor dss solidworks or pro engineer which enables engineers to create d models d drawings and schematics of their designs cad together with digital mockup dmu and cae software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time consuming physical prototypes
these allow products and components to be checked for flaws assess fit and assembly study ergonomics and to analyze static and dynamic characteristics of systems such as stresses temperatures electromagnetic emissions electrical currents and voltages digital logic levels fluid flows and kinematics access and distribution of all this information is generally organized with the use of product data management software
there are also many tools to support specific engineering tasks such as computer aided manufacturing cam software to generate cnc machining instructions manufacturing process management software for production engineering eda for printed circuit board pcb and circuit schematics for electronic engineers mro applications for maintenance management and aec software for civil engineering
in recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management plm
social context
engineering as a subject ranges from large collaborations to small individual projects almost all engineering projects are beholden to some sort of financing agency a company a set of investors or a government the few types of engineering that are minimally constrained by such issues are pro bono engineering and open design engineering
by its very nature engineering has interconnections with society and human behavior every product or construction used by modern society will have been influenced by engineering engineering is a very powerful tool to make changes to environment society and economies and its application brings with it a great responsibility many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large
engineering projects can be subject to controversy examples from different engineering disciplines include the development of nuclear weapons the three gorges dam the design and use of sport utility vehicles and the extraction of oil in response some western engineering companies have enacted serious corporate and social responsibility policies
engineering is a key driver of human development sub saharan africa in particular has a very small engineering capacity which results in many african nations being unable to develop crucial infrastructure without outside aid the attainment of many of the millennium development goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development
all overseas development and relief ngos make considerable use of engineers to apply solutions in disaster and development scenarios a number of charitable organizations aim to use engineering directly for the good of mankind
engineering companies in many established economies are facing significant challenges ahead with regard to the number of skilled engineers being trained compared with the number retiring this problem is very prominent in the uk there are many economic and political issues that this can cause as well as ethical issues it is widely agreed that engineering faces an image crisis rather than it being fundamentally an unattractive career much work is needed to avoid huge problems in the uk and well as the usa and other western economies
relationships with other disciplines
science
there exists an overlap between the sciences and engineering practice in engineering one applies science both areas of endeavor rely on accurate observation of materials and phenomena both use mathematics and classification criteria to analyze and communicate observations
scientists may also have to complete engineering tasks such as designing experimental apparatus or building prototypes conversely in the process of developing technology engineers sometimes find themselves exploring new phenomena thus becoming for the moment scientists
in the book what engineers know and how they know it walter vincenti asserts that engineering research has a character different from that of scientific research first it often deals with areas in which the basic physics or chemistry are well understood but the problems themselves are too complex to solve in an exact manner
examples are the use of numerical approximations to the navier stokes equations to describe aerodynamic flow over an aircraft or the use of miner's rule to calculate fatigue damage second engineering research employs many semi empirical methods that are foreign to pure scientific research one example being the method of parameter variation
as stated by fung et al in the revision to the classic engineering text foundations of solid mechanics
engineering is quite different from science scientists try to understand nature engineers try to make things that do not exist in nature engineers stress invention to embody an invention the engineer must put his idea in concrete terms and design something that people can use that something can be a device a gadget a material a method a computing program an innovative experiment a new solution to a problem or an improvement on what is existing since a design has to be concrete it must have its geometry dimensions and characteristic numbers almost all engineers working on new designs find that they do not have all the needed information most often they are limited by insufficient scientific knowledge thus they study mathematics physics chemistry biology and mechanics often they have to add to the sciences relevant to their profession thus engineering sciences are born
although engineering solutions make use of scientific principles engineers must also take into account safety efficiency economy reliability and constructability or ease of fabrication as well as legal considerations such as patent infringement or liability in the case of failure of the solution
medicine and biology
the study of the human body albeit from different directions and for different purposes is an important common link between medicine and some engineering disciplines medicine aims to sustain repair enhance and even replace functions of the human body if necessary through the use of technology
modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as for example brain implants and pacemakers the fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems
conversely some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology this has led to fields such as artificial intelligence neural networks fuzzy logic and robotics there are also substantial interdisciplinary interactions between engineering and medicine
both fields provide solutions to real world problems this often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both
medicine in part studies the function of the human body the human body as a biological machine has many functions that can be modeled using engineering methods
the heart for example functions much like a pump the skeleton is like a linked structure with levers the brain produces electrical signals etc these similarities as well as the increasing importance and application of engineering principles in medicine led to the development of the field of biomedical engineering that uses concepts developed in both disciplines
newly emerging branches of science such as systems biology are adapting analytical tools traditionally used for engineering such as systems modeling and computational analysis to the description of biological systems
art
there are connections between engineering and art
they are direct in some fields for example architecture landscape architecture and industrial design even to the extent that these disciplines may sometimes be included in a university's faculty of engineering and indirect in others
the art institute of chicago for instance held an exhibition about the art of nasa's aerospace design robert maillart's bridge design is perceived by some to have been deliberately artistic at the university of south florida an engineering professor through a grant with the national science foundation has developed a course that connects art and engineering
among famous historical figures leonardo da vinci is a well known renaissance artist and engineer and a prime example of the nexus between art and engineering
other fields
in political science the term engineering has been borrowed for the study of the subjects of social engineering and political engineering which deal with forming political and social structures using engineering methodology coupled with political science principles financial engineering has similarly borrowed the term

permanent adjustments of theodolites

the permanent adjustments of theodolites are made to establish fixed relationship between the instrument's fundamental lines the fundamental lines or axis of a transit theodolite include the following
these adjustments once made last for a long time these are important for accuracy of observations taken from the instrument the permanent adjustments in case of transit theodolite are
the horizontal axis must be perpendicular to the vertical axis
the vertical circle must read zero when the line of collimation is horizontal
the axis of altitude level must be parallel to the line of collimation
the line of collimation or line of sight should coincide with axis of the telescope the line of sight should also be perpendicular to the horizontal axis at its intersection with the vertical axis also the optical axis the axis of the objective slide and the line of sight should coincide
the axis of plate levels must be perpendicular to the vertical axis

reduced level

reduced level in surveying refers to equating elevations of survey points with reference to a common assumed datum it is a vertical distance between survey point and adopted datum plane thus it is considered as the base elevation which is used as reference to reckon heights or depths of other important places reduced here means equating and level means elevations datum may be real or imaginary location with a nominated elevation of straight zero
datum used
the most common and convenient datum which is internationally accepted is mean sea level countries take their nearby sea levels as datum planes for calculations of reduced levels for example pakistan takes sea near karachi as its datum while india takes sea near mumbai as its datum for calculation of reduced levels of different places in their respective countries the term reduced level is denoted shortly by rl national survey departments of each country determines rl s of significantly important locations or points these points are called as permanent benchmarks and this survey process is known as great trigonometrical surveying gts the permanent bench marks act as reference points for determining rl s of other locations in a particular country
instruments
the instruments used to determine reduced level include
and coke s reversible level anyone of these can be used at a time for the purpose
rl calculation
rl of a survey point can be determined by two methods

construction contract

a construction contract is a mutual or legally binding agreement between two parties based on certain policies and conditions generally recorded in documentation form the two parties involved are owner and contractor the owner has full authority to decide what type of contract should be used for a specific facility to be constructed and to set forth the terms and conditions in a contractual agreement
types
for the construction work the owner awards certain type of contract to the contractor the types of contracts which are usually awarded to the contractor are of six types as
lump sum contract
a lump sum is a contract in which owner agrees to pay a contractor specified amount of lumpsum money after the completion of work without requiring cost breakdown
item rate contract
in this contract a contractor is required to quote rate of individual item of works in tender this system of contract is mainly done in railway departments and public work departments
an advantage of this contract is that thorough analysis of rates can be done
lump sum and scheduled contract
this type of contract is same as lump sum contract the only difference is that cost breakdown is essentially required here
cost plus fixed fee contract
in this type of contract the contractor is paid by the owner an agreed amount over and above the actual cost of work
cost plus percentage of cost contract
in this construction contract a method of payment to a contractor in which an additional amount of money expressed as a percentage is paid by the owner over and above actual cost of work is used when paid as a predetermined profit the owner will usually require a strict accounting of expenses
special contracts
special contracts are further classified into five types

engineering design process

the engineering design process is a methodical series of steps that engineers use in creating functional products and processes the process is highly iterative parts of the process often need to be repeated many times before production phase can be entered though the part s that get iterated and the number of such cycles in any given project can be highly variable
one framing of the engineering design process delineates the following stages research conceptualization feasibility assessment establishing design requirements preliminary design detailed design production planning and tool design and production the steps tend to get articulated subdivided and or illustrated in a variety of different ways but they generally reflect certain core principles regarding the underlying concepts and their respective sequence and interrelationship
common stages of the engineering design process
research
a significant amount of time is spent on locating information and research consideration should be given to the existing applicable literature problems and successes associated with existing solutions costs and marketplace needs
the source of information should be relevant including existing solutions reverse engineering can be an effective technique if other solutions are available on the market other sources of information include the internet local libraries available government documents personal organizations trade journals vendor catalogs and individual experts available
feasibility
at first a feasibility study is carried out after which schedules resource plans and estimates for the next phase are developed the feasibility study is an evaluation and analysis of the potential of a proposed project to support the process of decision making it outlines and analyses alternatives or methods of achieving the desired outcome the feasibility study helps to narrow the scope of the project to identify the best scenario
a feasibility report is generated following which post feasibility review is performed
the purpose of a feasibility assessment is to determine whether the engineer's project can proceed into the design phase this is based on two criteria the project needs to be based on an achievable idea and it needs to be within cost constraints it is important to have engineers with experience and good judgment to be involved in this portion of the feasibility study
conceptualization
following feasibility a concept study conceptualization conceptual engineering is performed a concept study is the phase of project planning that includes producing ideas and taking into account the pros and cons of implementing those ideas this stage of a project is done to minimize the likelihood of error manage costs assess risks and evaluate the potential success of the intended project
once an engineering issue is defined solutions must be identified these solutions can be found by using ideation the mental process by which ideas are generated the following are the most widely used techniques
design requirements
establishing design requirements is one of the most important elements in the design process and this task is normally performed at the same time as the feasibility analysis the design requirements control the design of the project throughout the engineering design process some design requirements include hardware and software parameters maintainability availability and testability
preliminary design
the preliminary design or high level design also called feed bridges the gap between the design concept and the detailed design phase in this task the overall system configuration is defined and schematics diagrams and layouts of the project will provide early project configuration during detailed design and optimization the parameters of the part being created will change but the preliminary design focuses on creating the general framework to build the project on
detailed design
following feed is the detailed design detailed engineering phase which may consist of procurement as well
this phase builds on the already developed feed aiming to further elaborate each aspect of the project by complete description through solid modeling drawings as well as specifications
some of the said specifications include
computer aided design cad programs have made the detailed design phase more efficient this is because a cad program can provide optimization where it can reduce volume without hindering the part's quality it can also calculate stress and displacement using the finite element method to determine stresses throughout the part it is the engineer's responsibility to determine whether these stresses and displacements are allowable so the part is safe
production planning and tool design
the production planning and tool design consists in planning how to mass produce the project and which tools should be used in the manufacturing of the part tasks to complete in this step include selecting the material selection of the production processes determination of the sequence of operations and selection of tools such as jigs fixtures and tooling this task also involves testing a working prototype to ensure the created part meets qualification standards
production
with the completion of qualification testing and prototype testing the engineering design process is finalized the part must now be manufactured and the machines must be inspected regularly to make sure that they do not break down and slow production
comparison with the scientific method
the engineering design process bears some similarity to the scientific method both processes begin with existing knowledge and gradually become more specific in the search for knowledge or a solution

human sensing

human sensing also called human detection or human presence detection encompasses a range of technologies for detecting the presence of a human body in an area of space typically without the intentional participation of the detected person common applications include search and rescue surveillance and customer analytics for example people counters
modern technologies proposed or deployed for human sensing include
examples
various commercial heartbeat detection systems employ a set of vibration or seismic sensors to detect the presence of a person inside a vehicle or container by sensing vibrations caused by the human heartbeat
another commercial product uses infrared light to detect the level of carbon dioxide in an enclosed space from which it infers the presence of humans or other living creatures
in september the united states department of homeland security's science and technology directorate demonstrated a prototype of the finder radar technology device which it developed in conjunction with nasa's jet propulsion laboratory finder uses microwave radar to detect the unique signature of a human's breathing pattern and heartbeat through feet of solid concrete feet of a crushed mixture of concrete and rebar and feet of open space in september the dhs promoted the technology to swat teams at the urban shield trade show

levelling terms

the terms commonly used in levelling are as follows
back sight
back sight b s or back sight reading is the first staff reading taken by the surveyor after the levelling instrument is set up and levelled b s is generally taken on the point of known reduced level as on the benchmark or a change point
fore sight
fore sight f s or fore sight reading is the last staff reading taken before changing the instrument to the other position it is the staff reading taken on point whose r l is to determined this sight is considered as negative and deduced from height of instrument to determine r l of the point
intermediate sight
all readings taken between back sight and fore sight are called as inter sight readings or intermediate sights these are the points whose r l is determined by the method already mentioned above in f s
reduced level
reduced level refers to equating elevations of survey points with reference to a common assumed datum the elevation is positive or negative according as point lies above or below datum
datum surface
datum surface is the reference plane with respect to which r l of the other survey points is determined the datum surface may be real or imaginary location with a nominated elevation of zero the commonly used datum is mean sea level
benchmark
benchmark is the fixed reference point of known elevation with respect to which r l of other points is determined benchmarks can be arbitrary or permanent the former is used for calculation of reduced levels for small survey works and the latter is used to calculate the elevations of significantly important locations and points arbitrary benchmarks are assumed to be equal to meters generally and then the elevations with respect to assumed benchmark is determined it is commonly practiced by engineering students for gts surveys of the country surveyors use permanent benchmarks to calculate the elevations of different points
see also
power levelling

bending moment

a bending moment is the reaction induced in a structural element when an external force or moment is applied to the element causing the element to bend the most common or simplest structural element subjected to bending moments is the beam the example shows a beam which is simply supported at both ends simply supported means that each end of the beam can rotate therefore each end support has no bending moment the ends can only react the shear load other beams can have both ends fixed therefore each end support has both bending moment and shear reaction loads beams can also have one end fixed and one end simply supported the simplest type of beam is the cantilever which is fixed at one end and is free at the other end neither simple or fixed in reality beam supports are usually neither absolutely fixed nor absolutely rotating freely
the internal reaction loads in a cross section of the structural element can be resolved into a resultant force and a resultant couple for equilibrium the moment created by external forces and external moments must be balanced by the couple induced by the internal loads the resultant internal couple is called the bending moment while the resultant internal force is called the shear force if it is transverse to the plane of element or the normal force if it is along the plane of the element
the bending moment at a section through a structural element may be defined as the sum of the moments about that section of all external forces acting to one side of that section the forces and moments on either side of the section must be equal in order to counteract each other and maintain a state of equilibrium so the same bending moment will result from summing the moments regardless of which side of the section is selected if clockwise bending moments are taken as negative then a negative bending moment within an element will cause sagging and a positive moment will cause hogging it is therefore clear that a point of zero bending moment within a beam is a point of contraflexure that is the point of transition from hogging to sagging or vice versa
moments and torques are measured as a force multiplied by a distance so they have as unit newton metres n m or pound foot or foot pound ft lb the concept of bending moment is very important in engineering particularly in civil and mechanical engineering and physics
background
tensile and compressive stresses increase proportionally with bending moment but are also dependent on the second moment of area of the cross section of a beam that is the shape of the cross section such as a circle square or i beam being common structural shapes failure in bending will occur when the bending moment is sufficient to induce tensile stresses greater than the yield stress of the material throughout the entire cross section in structural analysis this bending failure is called a plastic hinge since the full load carrying ability of the structural element is not reached until the full cross section is past the yield stress it is possible that failure of a structural element in shear may occur before failure in bending however the mechanics of failure in shear and in bending are different
moments are calculated by multiplying the external vector forces loads or reactions by the vector distance at which they are applied when analysing an entire element it is sensible to calculate moments at both ends of the element at the beginning centre and end of any uniformly distributed loads and directly underneath any point loads of course any pin joints within a structure allow free rotation and so zero moment occurs at these points as there is no way of transmitting turning forces from one side to the other
it is more common to use the convention that a clockwise bending moment to the left of the point under consideration is taken as positive this then corresponds to the second derivative of a function which when positive indicates a curvature that is lower at the centre i e sagging when defining moments and curvatures in this way calculus can be more readily used to find slopes and deflections
critical values within the beam are most commonly annotated using a bending moment diagram where negative moments are plotted to scale above a horizontal line and positive below bending moment varies linearly over unloaded sections and parabolically over uniformly loaded sections
engineering descriptions of the computation of bending moments can be confusing because of unexplained sign conventions and implicit assumptions the descriptions below use vector mechanics to compute moments of force and bending moments in an attempt to explain from first principles why particular sign conventions are chosen
computing the moment of force
an important part of determining bending moments in practical problems is the computation of moments of force
let formula be a force vector acting at a point a in a body the moment of this force about a reference point o is defined as
where formula is the moment vector and formula is the position vector from the reference point o to the point of application of the force a the formula symbol indicates the vector cross product for many problems it is more convenient to compute the moment of force about an axis that passes through the reference point o if the unit vector along the axis is formula the moment of force about the axis is defined as
where formula indicates the vector dot product
example
the adjacent figure shows a beam that is acted upon by a force formula if the coordinate system is defined by the three unit vectors formula we have the following
therefore
the moment about the axis formula is then
sign conventions
the negative value suggests that a moment that tends to rotate a body clockwise around an axis should have a negative sign however the actual sign depends on the choice of the three axes formula for instance if we choose another right handed coordinate system with formula we have
then
for this new choice of axes a positive moment tends to rotate body clockwise around an axis
computing the bending moment
in a rigid body or in an unconstrained deformable body the application of a moment of force causes a pure rotation but if a deformable body is constrained it develops internal forces in response to the external force so that equilibrium is maintained an example is shown in the figure below these internal forces will cause local deformations in the body
for equilibrium the sum of the internal force vectors is equal to the applied external force and the sum of the moment vectors created by the internal forces is equal to the moment of the external force the internal force and moment vectors are oriented in such a way that the total force internal external and moment external internal of the system is zero the internal moment vector is called the bending moment
though bending moments have been used to determine the stress states in arbitrary shaped structures the physical interpretation of the computed stresses is problematic however physical interpretations of bending moments in beams and plates have a straightforward interpretation as the stress resultants in a cross section of the structural element for example in a beam in the figure the bending moment vector due to stresses in the cross section a perpendicular to the x axis is given by
expanding this expression we have
we define the bending moment components as
the internal moments are computed about an origin that is at the neutral axis of the beam or plate and the integration is through the thickness formula
example
in the beam shown in the adjacent figure the external forces are the applied force at point a formula and the reactions at the two support points o and b formula and formula the reactions can be computed using balances of forces and moments about point a i e
if formula is the length of the beam we have
if we solve for the reactions we have
looking at the free body diagram of the part of the beam to the left of point x the total moment of the external forces about the point x is
if we compute the cross products we have
for this situation the only non zero component of the bending moment is
for the sum of the moments at x about the axis formula to be zero we require
at formula we have formula
sign convention
in the above discussion it is implicitly assumed that the bending moment is positive when the top of the beam is compressed that can be seen if we consider a linear distribution of stress in the beam and find the resulting bending moment let the top of the beam be in compression with a stress formula and let the bottom of the beam have a stress formula then the stress distribution in the beam is formula the bending moment due to these stresses is
where formula is the area moment of inertia of the cross section of the beam therefore the bending moment is positive when the top of the beam is in compression
many authors follow a different convention in which the stress resultant formula is defined as
in that case positive bending moments imply that the top of the beam is in tension of course the definition of top depends on the coordinate system being used in the examples above the top is the location with the largest formula coordinate

automation engineering

automation engineering is a field of science combining engineering and the management of
information so that different systems can work effectively in every aspect such as cost of production
quality of products safety of work environment and ability to adjust production rates to respond to the
demands of the market as well as the administration and management in the industry
automation engineer
automation engineers are experts who have the knowledge and ability to design create develop and
manage systems for example factory automation process automation as well as building and home
automation to make the systems work by themselves or require a minimum number of personnel
learning about
automation engineering is the integration between standard engineering fields and management
information technology
engine and management information technology means information technology communication
networking management and information management
work and career after graduation
graduates can work for both government and private sector entities such as industrial production
companies that create and use automation systems for example paper industry automotive industry or
food and agricultural industry graduates can work on other systems that are not industries such as
building and home automation which use computer controlled energy and security through a
communication system called building and home automation
job description
information systems relate to the creation and application of technology to monitor or control the
production and delivery of products and services by the international society of automation and the
automation federation automation engineers can design program simulate and test automated
machinery and processes that will complete the work automation engineers usually are employed
in industries such as car manufacturing or food processing plants robots or machines used for
implementation automation engineers are responsible for detailed design specifications and other
documents in their creations
necessary skills
automation engineers must have these qualities as defined by u s bureau of labor statistics bls
understand about computer programming and software development
ability to solve the problems of equipment and perform complex system tests
creative thinking
detail oriented personality
excellent proficiency
ability to communicate well with other members of the development team

goldieblox

goldieblox is an award winning interactive toy company on a mission to inspire the next generation of female innovators goldieblox launched in and went from a prototype on kickstarter to more than m of pre orders placed in under a month in just under two years goldieblox has made its way to toys r us amazon and more than retailers worldwide including canada australia and the uk the company was founded by debbie sterling a stanford engineering graduate and entrepreneur and is based in oakland ca
goldieblox was named people s choice and most educational toy at the toy of the year awards now offering six unique toy sets and expansion packs a mobile app and digital playground the brand aims to highlight goldie and her friends as role models for girls around the world
goldieblox pairs a construction kit with a storybook engaging girls verbal skills and encouraging them to build alongside the narrative of goldie a curious and confident inventor and her friends goldieblox s mission is to close the gender gap in stem science technology engineering and math and change the way we think about toys for girls
history
while debbie was a student at stanford she noticed her classes were predominantly male this ratio was indicative of a larger gender gap while she was at stanford the percentage of women in engineering in the united states was only after research debbie found that girls begin to lose interest in math and science as young as age she set out to create a solution but knew that creating a pink construction toy wasn t enough she spent two years studying early child development specifically in girls and the gender marketing of toys and learned that girls excel in verbal skills reading and writing the breakthrough of goldieblox marries the story of goldie a girl inventor who loves to build with a construction kit
to fund her first round of production sterling created a kickstarter campaign in the project reached its funding goal of in days and went on to raise a total of with backers by the end of the campaign in just under two years goldieblox has made its way to toys r us amazon and more than retailers nationwide and in canada the u k and australia
products
geared toward ages toys in the goldieblox series introduce engineering concepts to girls through storytelling and building girls follow goldie a quirky female inventor and her friends on adventures as they solve problems by building simple machines girls build alongside the story motivated to create by prompts within the narrative goldieblox toys help girls develop spatial skills learn basic engineering concepts and build confidence in problem solving the toys are each compatible with each other so they can be used in a series as well as separate sets each toy introduces new characters and concepts and there are currently six sets in the series
in goldieblox also began introducing digital content the company s first mobile app goldieblox and the movie machine was introduced in october the app features the company s first ever animated cartoon and was named by apple as one of the best apps of bloxtown com goldieblox s digital playground houses original content and videos of new design ideas for kids to watch and build at home
videos
goldieblox rube goldberg princess machine
goldieblox s princess machine video launched on youtube in november garnering over million views in days the video features three young girls building a rube goldberg machine and was originally set to an original parody of the beastie boys song girls shortly after the release the beastie boys accused the company of copyright infringement and goldieblox responded by suing for declaratory judgment in u s district court of san francisco seeking declaration of fair use due to parody the beastie boys responded with a counter lawsuit and a settlement was reached in march the music in the video was changed
this is your brain on engineering goldieblox easter psa
this is your brain on engineering goldieblox easter psa video launched on youtube in april the video spotlights the difference between a young girl s brain on princess vs her brain on engineering it s an eye opener to the gender norms placed on young girls at an early age and uses wheels axles and engineering to highlight some statistics related to the dearth of women in engineering
goldieblox vs the big sister machine
goldieblox vs the big sister machine launched on youtube in november demonstrating the need for female role models inspired by ingenuity and creativity in the video big sister prescribes her ideals of beauty and perfection to young girls little sister a girl inspired by goldie rebels against the mantra breaking the girls free and leading them to a world of possibilities the video is set to metric's help i'm alive hit
lightning strikes
goldieblox released their first single and animated music video lightning strikes in december the track is an original song written and performed by emily haines the lead singer of canadian rock band metric the video and song feature goldie a strong female character who comes up with a great idea and strives to accomplish it despite whatever set backs occur along the way goldieblox and emily created the song as a message to girls showing them that with confidence and ingenuity they can accomplish anything
advertisements
intuit s small business big game super bowl commercial
in february goldieblox won intuit s small business big game contest earning a second commercial spot during the broadcast of super bowl xlviii the commercial airtime was valued at million and with the advertisement goldieblox became the first small business to air an ad in the super bowl the ad was set to a parody of the slade quiet riot song come on feel the noize changing the words to come on bring the toys the ad depicted hundreds of little girls ditching their pink toys while singing more than pink pink pink we want to think and that girls build like all the boys
awards
goldieblox is a recipient of the below awards
partnerships
in goldieblox joined the macy's thanksgiving day parade with a kid powered float called the girl powered spinning machine goldieblox's float was the first ever specifically designed to demonstrate the principles of engineering with active cranks gears and pulleys the company will also be participating in the and parades

gap analysis

in management literature gap analysis involves the comparison of actual performance with potential or desired performance if an organization does not make the best use of current resources or forgoes investment in capital or technology it may produce or perform below its potential this concept is similar to an economy's being below the production possibilities frontier
gap analysis identifies gaps between the optimized allocation and integration of the inputs resources and the current allocation level this may reveal areas that can be improved gap analysis involves determining documenting and approving the difference between business requirements and current capabilities gap analysis naturally flows from benchmarking and from other assessments once the general expectation of performance in an industry is understood it is possible to compare that expectation with the company's current level of performance this comparison becomes the gap analysis such analysis can be performed at the strategic or at the operational level of an organization
gap analysis is a formal study of what a business is doing currently and where it wants to go in the future it can be conducted in different perspectives as follows
gap analysis provides a foundation for measuring investment of time money and human resources required to achieve a particular outcome e g to turn the salary payment process from paper based to paperless with the use of a system note that gap analysis has also been used as a means of classifying how well a product or solution meets a targeted need or set of requirements in this case gap can be used as a ranking of good average or poor this terminology appears in the prince project management publication from the ogc office of government commerce
the need for new products or additions to existing lines may emerge from portfolio analysis in particular from the use of the boston consulting group growth share matrix or the need may emerge from the regular process of following trends in the requirements of consumers at some point a gap emerges between what existing products offer and what the consumer demands the organization must fill that gap to survive and grow
gap analysis can identify gaps in the market thus comparing forecast profits to desired profits reveals the planning gap this represents a goal for new activities in general and new products in particular
the planning gap can be divided into three main elements
usage gap
the usage gap is the gap between the total potential for the market and actual current usage by all consumers in the market data for this calculation includes
existing usage
existing consumer usage makes up the total current market from which market shares for example are calculated it usually derives from marketing research most accurately from panel research but also from adhoc work sometimes it may be available from figures that governments or industries have collected however these are often based on categories that make bureaucratic sense but are less helpful in marketing terms the usage gap is thus
this is an important calculation many if not most marketers accept existing market size suitably projected their forecast timescales as the boundary for expansion plans though this is often the most realistic assumption it may impose an unnecessary limit on horizons for example the original market for video recorders was limited to professional users who could afford high prices only after some time did the technology extend to the mass market
in the public sector where service providers usually enjoy a monopoly the usage gap is probably the most important factor in activity development however persuading more consumers to take up family benefits for example is probably more important to the relevant government department than opening more local offices
usage gap is most important for brand leaders if a company has a significant share of the whole market they may find it worthwhile to invest in making the market bigger this option is not generally open to minor players though they may still profit by targeting specific offerings as market extensions
all other gaps relate to the difference between existing sales market share and total sales of the market as a whole the difference is the competitor share these gaps therefore relate to competitive activity
product gap
the product gap also called the segment or positioning gap is that part of the market a particular organization is excluded from because of product or service characteristics this may be because the market is segmented and the organization does not have offerings in some segments or because the organization positions its offerings in a way that effectively excludes certain potential consumers because competitive offerings are much better placed for these consumers
this segmentation may result from deliberate policy segmentation and positioning are powerful marketing techniques but the trade off against better focus is that market segments may effectively be put beyond reach on the other hand product gap can occur by default the organization has thought out its positioning its offerings drifted to a particular market segment
the product gap may be the main element of the planning gap where an organization can have productive input hence the emphasis on the importance of correct positioning
gap analysis to develop a better process
a gap analysis can also be used to analyze gaps in processes and the gulf between the existing outcome and the desired outcome
this step process can be illustrated by the example below
a gap analysis can also be used to compare one process to others performed elsewhere which are often identified through benchmarking in this usage one compares each process side by side and step by step and then notes the differences one then analyzes each deviation to determine if there is any benefit to changing to the alternate process the results of this analysis in the context of the benefits and detriments of changing processes may support the maintenance of the current process the wholesale adoption of an alternate process or a fusion of different aspects of each process

energy and environmental engineering

energy and environmental engineering is a branch of engineering which seeks to efficiently use energy and to maintain the environment energy engineers require knowledge across many disciplines careers include built environment renewable and traditional energy industries
in this area solar radiation is important and must be understood solar radiation affects the earths weather and daylight available this affects not only the earths environment but also the smaller internal environments which we create
energy engineering requires at least an understanding of mechanics thermodynamics mathematics materials stoichiometry electrical machines manufacturing processes and energy systems
environmental engineering can be branched into two main areas internal environments and outdoor environments
internal environments may consist of housing or offices or other commercial properties in this area the environmental engineering sometimes stands for the designing of building services to condition the internal environment to a comfortable state or the removal of excess pollutants such as carbon dioxide or other harmful substances
external environments may be water courses air land or seas and may require new strategies for harnessing energy or the creation of treatment facilities for polluting technologies
this broad degree area covers many areas but is mainly mechanically and electrically biased it seeks to explore cleaner more efficient ways of using fossil fuels while investigating and developing systems using renewable and sustainable resources such as solar wind and wave energy

maximum allowable operating pressure

maximum allowable operating pressure or maop refers to the wall strength of a pressurized cylinder such as a pipeline or storage tank and how much pressure the walls may safely hold in normal operation
the maop is less than the mawp maximum allowable working pressure mawp being the maximum pressure based on the design codes that the weakest component of a pressure vessel can handle commonly standard wall thickness components are used in fabricating pressurised equipment and hence are able to withstand pressures above their design pressure
design pressure is the maximum pressure a pressurised item can be exposed to due to the availability of standard wall thickness materials many components will have a mawp higher than the required design pressure
relief valves are set at the design pressure of the pressurised item and sized to prevent the pressurised item being overpressured depending on the design code that the pressurised item is designed an overpressure allowance can be used when sizing the relief valve this is for pd and asme section viii div with an additional allowance in asme section viii for a fire relief case asme have different criteria for steam boilers

engineering studies

engineering studies is an interdisciplinary branch of social sciences and humanities devoted to the study of engineers and their activities studying engineers refers among other to the history and the sociology of their profession its institutionalization and organization the social composition and structure of the population of engineers their training their trajectory etc a subfield is for instance women in engineering studying engineering refers to the study of their activities and practices their knowledge and ontologies their role into the society their engagement
engineering studies investigate how social political economical cultural and historical dynamics affect technological research design engineering and innovation and how these in turn affect society economics politics and culture
sometimes engineering studies refers to engineering education

non recurring engineering

non recurring engineering nre refers to the one time cost to research develop design and test a new product when budgeting for a project nre must be considered to analyze if a new product will be profitable even though a company will pay for nre on a project only once nre costs can be prohibitively high and the product will need to sell well enough to produce a return on the initial investment nre is unlike production costs which must be paid constantly to maintain production of a product it is a form of fixed cost in economics terms once a system is designed any number of units can be manufactured without increasing nre cost
in a project type manufacturing company large parts possibly all of the project represent nre in this case the nre costs are likely to be included in the first project's costs this can also be called research and design r d if the firm cannot recover these costs it must consider funding part of these from reserves possibly take a project loss in the hope that the investment can be recovered from further profit on future projects
the concept of full product nre as described above may lead readers to believe that nre expenses are necessarily high however focused nre wherein small amounts of nre money can yield large returns by making existing product changes is an option to consider as well small adjustment to an existing assembly may be considered to use a less expensive or improved sub component or to replace a sub component which is no longer available in the world of embedded firmware nre may be invested in code development to fix problems or to add features where the costs to implement are a very small percentages of an immediate return chrysler found such a way to repair a transmission problem by investing trivial nre dollars into computer firmware to fix a mechanical problem to save some tens of millions of dollars in mechanical repairs to transmissions in the field
mastery of nre concepts as financial investments are a most excellent loss control tool and must be considered a key part of manufacturing profit enhancement

engineer's day

engineer's day is observed in several countries on various dates of the year


manipal international university

manipal international university miu located in putra nilai negeri sembilan malaysia is a member of the manipal global education group and a full fledged malaysian university offering multidisciplinary programs with a focus in the fields of science engineering and management business
the manipal education group has been responsible for producing some of the brightest minds in asia for the past years the group has a network of six campuses and affiliations with universities worldwide building on the success of the in malaysia the manipal education group brings its multidisciplinary expertise to malaysia through miu
history of manipal group
in t m a pai founded india's first private medical school kasturba medical college and five years later the manipal institute of technology was formed ramdas pai took over the management in after the death of t m a pai initially all degrees were awarded by karnataka university and later mysore university from to they were awarded by mangalore university the current organizational structure was formed in when manipal university then known as the manipal academy of higher education was accorded deemed university status by the university grants commission the university is certified as an iso organization in it rebranded itself as manipal university the legal name remains the manipal academy of higher education
academics
manipal international university offers the following programmes
engineering
bachelor of computer engineering hons the it industry in malaysia has been booming since the introduction of the multimedia super corridor in the mid s several of the world s major it players have hubs in malaysia the computer engineering students are trained not only to create build and manage it systems and products to cater to specific needs in the industry but also to be sensitive towards how these systems are integrated within organisations the students in the bachelor of computer engineering hons will not only be trained to be effective professionals but also to possess the necessary soft skills to be high achievers in this competitive industry
bachelor of civil engineering hons the bachelor of civil engineering hons in manipal international university encompasses an intensive study of the various engineering courses associated with this stream together with an emphasis on professional and personal development
the civil engineers are responsible for the design and construction of physical structures such as buildings roads dams and malaysia's world class infrastructure is built on the backbone of expertise in the field of civil engineering this will become even more crucial as the country accelerates towards vision
bachelor of chemical engineering hons chemical engineers rely on their knowledge of how chemicals and biological processes behave in order to solve industrial problems and create improve products the skills and knowledge gained during this course help them to find work in the personal care consumer goods food petrochemical and waste management industries
as the oil palm and petrochemical industry continues to dominate the malaysian economy chemical engineers are highly sought after in this country
bachelor of electrical electronics engineering hons malaysia s place in the field of electrical electronics engineering hons has been well established as major industry players have set up manufacturing plants throughout peninsula malaysia
this program prepares students for a rewarding career in the electronics industry not just through technical know how but also through business savvy
the bachelor of electrical and electronics engineering hons covers the generation and transmission of electrical power as well as the complexities of electronic circuits systems and power
bachelor of electronics engineering communication hons the bachelor of electronics engineering communication hons is designed to leverage on miu s strength in electronics engineering specialization in the field of modern telecommunications
through this program students learn the fundamentals of electronics engineering within the context of advanced communication technologies such as fibre optics telecommunications hardware and microwaves
successful completion of this program provides students with the requisite knowledge to enter into one of the fastest growing engineering disciplines at the moment
business
bachelor of accounting hons the bachelor of accounting at miu provides a comprehensive foundation for students to get a strong start for a career in accounting and finance the program is designed to teach students the foundations of corporate accounting and accounting systems
students will also be exposed to business strategy communication public service accounting auditing and taxation
bachelor of business administration hons the bachelor of business administration hons provides students with the necessary skills to be effective professionals in corporate administration through this year intensive degree program students are trained to understand the various disciplines and best practices for organisational management the program is also designed to incorporate current issues in corporate governance in malaysia in order to produce socially aware and technically competent graduates
bachelor of business administration hons international business students in the bachelor of business administration international business hons are exposed to issues and events that affect the regional and global business landscape they are trained for cross cultural communication and equipped to operate across borders
this major is suited for students who are excited by the prospect of working for or setting up multi national companies students will be exposed to the finer aspects of foreign policy and corporate governance
finance
bachelor of actuarial finance hons the actuarial finance program is considered one of the toughest disciplines for aspiring financial specialists and is therefore reserved for graduates especially interested in this field actuaries are very well compensated
the bachelor of actuarial finance hons prepares students for the exams required to professionally accredit as an actuary students will be expected to excel in statistics calculus probability and advanced modelling and risk management in order to reach their aspirations of become an actuary
sciences
bachelor of computer science hons bachelor of computer science hons program encompasses the computer science and software engineering domains students will be studying modules that include computer studies software engineering management sustainability in it and concepts of green ict
the curriculum is designed to develop students with skill sets in group dynamics team management conceptual development in problem solving and designing projects with environment friendly approaches they will be confident working independently and in a team environment
there are opportunities for students to receive training in niche areas of it that include data analysis database system and networking the curriculum is industry oriented and eco friendly
bachelor of science biotechnology hons the biotechnology industry is one of the fastest growing industries in the world today and is expected to contribute of malaysia s gdp by the year
this is a multidisciplinary field that harnesses biological processes to create and improve products for the betterment of society it is also expected to play a key role in solving current world problems such as food security disease environmental degradation and climate change
the bachelor of science biotechnology hons program provides an overview of the biotechnology industry and common techniques used in r d graduates are trained to fuse their technical expertise with a keen understanding of this fast paced industry
recognition
manipal international university degrees are approved by the malaysian qualifications agency and ministry of higher education as malaysia is part of the washington accord and miu being a malaysian university our engineering programs are recognized by the signatory members of this accord established in the signatories to this accord as of are australia canada chinese taipei china ireland japan korea malaysia new zealand russia singapore south africa turkey the united kingdom and the united states graduates of accredited programs in any of the signatory countries are recognized by the other signatory countries as certified bona fide engineers
the engineering programs in miu are also approved by the engineering accreditation council miu is also awarded a partner for the industry centre of excellence in biotechnology by the ministry of education malaysia all the foundation programs at miu are fully accreditated
campus
manipal international university is registered as south east asia s first green university
miu is highly committed to building a campus based on the best green building technology and has registered to comply with the united states leed leadership in energy environmental design platinum green building index the first building in the campus has been designed and built to meet all the requirements to achieve a platinum certification the acre campus nestled amidst green hills and flanked by a beautiful acre lake is well connected by road and rail and is barely km from downtown kuala lumpur and km from kuala lumpur international airport the campus has top notch infrastructure for engineering and science education with labs and hi tech research facilities the calm serene environment offers students an extremely conducive environment not only to learn but to excel the green building technology adopted in the development covers energy efficiency water efficiency waste management and site ecology the campus forms part of miu s endeavour in promoting technology and innovation and fostering socially responsible corporate citizens of tomorrow
library
the library at manipal international university offer students and faculty a range of text books and reference books magazines journals e magazines e newspapers newspapers computers registered to opac online public access catalogue and studying cubicles
through our staff and facilities we seek to nurture the developing scholar and to instil confidence in dealing with increasing amounts of information in an ever changing array of formats students may contact our library staff to enrol in workshops to become familiar with the library and its resources library instructions guide students to obtain information more efficiently and evaluate it more thoroughly
hostels
miu student hostel is housed at anggerik court in putra nilai kilometres from the miu campus the hostel is within a gated and secured apartment with facilities such as park playground and ample parking space amenities such as restaurants and eateries clinics laundromat and convenient stores are also within easy reach regular shuttle buses are provided for students to commute between the hostel and campus during school days
labs and workshops
a capacious workshop and various lab facilities available in miu provide rich and practical educational environment for the students not only do they provide ample opportunities to move beyond the classroom and get hands on experience but also make the students industry ready
biotechnology chemistry lab
miu complies with the provisions of safety and health as prescribed under the occupational safety and health act for the laboratory facilities dedicated to biotechnology program three laboratories molecular biology microbiology and proteomics are well equipped to carry out basic biotechnology experiments the lab includes equipments such incubators ph meters water baths centrifuges electrophoresis apparatus spectrophotometers ultraviolet visible incubator shakers stirrers hotplates analytical balances top loading balances autoclaves freezers chillers laminar flow chambers and fumehoods specialised facilities are also provided in the molecular biology lab like gradient thermocycler electrophoresis apparatus and gel documentation system and refrigerated centrifuge the proteomics lab includes protein enzyme technology facilities chromatographic apparatus and enzyme linked immunosorbent assay reader elisa reader light microscopes stereo microscopes and specialised incubators for shake and plate cultures are installed in the microbiology lab
computer center
it caters to all the computing needs of the students faculty and staffs for their research and teaching not only does it support but also enhances the educational mission of the institute thus enriching the learning experience and supporting academic activities in order to keep up with the latest developments in technlogy the hardwares are upgraded on regular basis
electronic center
dedicated to both undergraduate and postgraduate study it plays an important role in understanding and appreciating concepts of electronic circuits power electronics applications and energy conversion the students get hands on experience in analogue system and digital system the laboratory also
mechanical workshop
mechanical department has a sprawling workshop with machine shop foundry smithy sheet metal work plumbing welding and carpentry sections
strength of materials lab
the undergraduate students of civil engineering mechanical engineering and related branches undergo a course mandatorily in material testing lab it is equipped with equipments machines to test basic materials of steel copper brass etc and also houses universal testing machines compression testing machines etc
clubs
there are several student run clubs and societies at manipal international university the student bodies are divided into clubs and societies are managed by the students themselves and are governed by the student affairs council the following are the list of clubs and societies
it is the apex body which controls all the aspects of student life including other clubs and societies
manipal itc is a team of degree students with rich experience in software development web and mobile application development film and explainer's video making and they also lead researchers in fields like artificial intelligence and robotics
international students society iss is society of international students at miu they organize social events activities and trips throughout the academic year
miu engineering society mes is a multidisciplinary organization of engineering disciplines dedicated to advancing the knowledge understanding and practice of engineering mes members represent the mainstream of manipal engineering faculty

surface science

surface science is the study of physical and chemical phenomena that occur at the interface of two phases including solid liquid interfaces solid gas interfaces solid vacuum interfaces and liquid gas interfaces it includes the fields of surface chemistry and surface physics some related practical applications are classed as surface engineering the science encompasses concepts such as heterogeneous catalysis semiconductor device fabrication fuel cells self assembled monolayers and adhesives surface science is closely related to interface and colloid science interfacial chemistry and physics are common subjects for both the methods are different in addition interface and colloid science studies macroscopic phenomena that occur in heterogeneous systems due to peculiarities of interfaces
history
the field of surface chemistry started with heterogeneous catalysis pioneered by paul sabatier on hydrogenation and fritz haber on the haber process irving langmuir was also one of the founders of this field and the scientific journal on surface science langmuir bears his name the langmuir adsorption equation is used to model monolayer adsorption where all surface adsorption sites have the same affinity for the adsorbing species gerhard ertl in described for the first time the adsorption of hydrogen on a palladium surface using a novel technique called leed similar studies with platinum nickel and iron followed most recent developments in surface sciences include the nobel prize of chemistry winner gerhard ertl's advancements in surface chemistry specifically
his investigation of the interaction between carbon monoxide molecules and platinum surfaces
surface chemistry
surface chemistry can be roughly defined as the study of chemical reactions at interfaces it is closely related to surface engineering which aims at modifying the chemical composition of a surface by incorporation of selected elements or functional groups that produce various desired effects or improvements in the properties of the surface or interface surface chemistry also overlaps with electrochemistry surface science is of particular importance to the field of heterogeneous catalysis
the adhesion of gas or liquid molecules to the surface is known as adsorption this can be due to either chemisorption or by physisorption these too are included in surface chemistry
the behavior of a solution based interface is affected by the surface charge dipoles energies and their distribution within the electrical double layer
surface physics
surface physics can be roughly defined as the study of physical changes that occur at interfaces it overlaps with surface chemistry some of the things investigated by surface physics include surface states surface diffusion surface reconstruction surface phonons and plasmons epitaxy and surface enhanced raman scattering the emission and tunneling of electrons spintronics and the self assembly of nanostructures on surfaces
analysis techniques
the study and analysis of surfaces involves both physical and chemical analysis techniques
several modern methods probe the topmost nm of surfaces exposed to vacuum these include x ray photoelectron spectroscopy auger electron spectroscopy low energy electron diffraction electron energy loss spectroscopy thermal desorption spectroscopy ion scattering spectroscopy secondary ion mass spectrometry dual polarization interferometry and other surface analysis methods included in the list of materials analysis methods many of these techniques require vacuum as they rely on the detection of electrons or ions emitted from the surface under study moreover in general ultra high vacuum in the range of pascal pressure or better it is necessary to reduce surface contamination by residual gas by reducing the number of molecules reaching the sample over a given time period at mpa torr it only takes second to cover a surface with a contaminant so much lower pressures are needed for measurements
purely optical techniques can be used to study interfaces under a wide variety of conditions reflection absorption infrared dual polarisation interferometry surface enhanced raman and sum frequency generation spectroscopies can be used to probe solid vacuum as well as solid gas solid liquid and liquid gas surfaces dual polarization interferometry is used to quantify the order and disruption in birefringent thin films this has been used for example to study the formation of lipid bilayers and their interaction with membrane proteins
modern physical analysis methods include scanning tunneling microscopy stm and a family of methods descended from it these microscopies have considerably increased the ability and desire of surface scientists to measure the physical structure of many surfaces for example they make it possible to follow reactions at the solid gas interface in real space if those proceed on a time scale accessible by the instrument

microphysics

the term microphysics refers to areas of physics that study phenomena that take place on the microscopic scale length scales smaller than mm such as

classical physics

classical physics refers to theories of physics that predate modern more complete or more widely applicable theories if a currently accepted theory is considered to be modern and its introduction represented a major paradigm shift then previous theories or new theories based on the older paradigm will often be referred to as classical
as such the definition of a classical theory depends on context classical physical concepts are often used when modern theories are unnecessarily complex for a particular situation
overview
classical theory has at least two distinct meanings in physics in the context of quantum mechanics classical theory refers to theories of physics that do not use the quantisation paradigm particularly classical mechanics including relativity likewise classical field theories such as general relativity and classical electromagnetism are those that do not incorporate any quantum mechanics in the context of general and special relativity classical theories are those that obey galilean relativity
among the branches of theory included in classical physics are
comparison with modern physics
in contrast to classical physics modern physics is a slightly looser term which may refer to just quantum physics or to th and st century physics in general modern physics includes quantum theory and relativity when applicable
a physical system can be considered in the classical limit when they satisfy conditions such that the laws of classical physics are approximately valid in practice physical objects larger than atoms and molecules can be well understood with classical mechanics including the objects in the macroscopic and astronomical realm beginning at the atomic level the laws of classical physics break down and generally do not provide a correct description of nature electromagnetic fields and forces can be described well by classical electrodynamics at length scales and field strengths large enough that quantum mechanical effects are negligible unlike quantum physics classical physics is generally characterized by the principle of complete determinism although deterministic interpretations of quantum mechanics do exist
from the point of view of classical physics as non relativistic physics the predictions of general and special relativity are significantly different than those of classical theories particularly concerning the passage of time the geometry of space the motion of bodies in free fall and the propagation of light traditionally light was reconciled with classical mechanics by assuming the existence of a stationary medium through which light propagated the luminiferous aether which was later shown not to exist
mathematically classical physics equations are ones in which planck's constant does not appear according to the correspondence principle and ehrenfest's theorem as a system becomes larger or more massive the classical dynamics tends to emerge with some exceptions such as superfluidity this is why we can usually ignore quantum mechanics when dealing with everyday objects instead the classical description will suffice however one of the most vigorous on going fields of research in physics is classical quantum correspondence this field of research is concerned with the discovery of how the laws of quantum physics give rise to classical physics in the limit of the large scales of the classical level
computer modeling and manual calculation modern and classic comparison
today a computer performs millions of arithmetic operations in seconds to solve a classical differential equation while newton one of the fathers of the differential calculus would take hours to solve the same equation by manual calculation even if he were the discoverer of that particular equation
computer modeling would use quantum and relativistics physics classic physics is considered the limit of quantum mechanics for large number of particles on the other hand classic mechanics part of classic physics is derived from relativistic mechanics for velocities much smaller than that of light one can neglect the terms with c and higher in the denominator these formulas then reduce to the standard definitions of newtonian kinetic energy and momentum this is as it should be for special relativity must agree with newtonian mechanics at low velocities computer modeling has to be as real as possible classical physics would introduce an error as in the superfluidity case in order to produce reliable models of the world we can not use classic physics in today's modeling it is true that quantum theories consume time and computer resources which could be reduced by using classical equations but we can not sacrifice reliability in order to save time
computer modeling would use only the energy criteria to determine which theory to use relativity or quantum theory when considering any object the object can have any number of particles and become a system of particles the speed and size of an object or a system of particles are only used for academics purposes or engineer calculus civil engineers use classical physics to build anything from a house to a bridge a physicist would select a classical calculus at the beginning of an experiment to have an approximation before the real calculus process began see again the four major domains of modern physics diagram
in a computer model there is no need to use the speed of the object if classical physics is excluded low energy objects would be handled by quantum theory and high energy objects by relativity theory

experimental physics

experimental physics is the category of disciplines and sub disciplines in the field of physics that are concerned with the observation of physical phenomena and experiments methods vary from discipline to discipline from simple experiments and observations such as the cavendish experiment to more complicated ones such as the large hadron collider
overview
experimental physics regroup all the disciplines of physics that are concerned with data acquisition data acquisition methods and the detailed conceptualization beyond simple thought experiments and realization of laboratory experiments it is often put in contrast with theoretical physics which is more concerned with predicting and explaining the physical behaviour of nature than the acquisition of knowledge about it
although experimental and theoretical physics are concerned with different aspects of nature they both share the same goal of understanding it and have a symbiotic relation the former provides data about the universe which can then be analyzed in order to be understood while the latter provides explanations for the data and thus offers insight on how to better acquire data and on how to set up experiments theoretical physics can also offer insight on what data is needed in order to gain a better understanding of the universe and on what experiments to devise in order to obtain it
history
as a distinct field experimental physics was established in early modern europe during what is known as the scientific revolution by physicists such as galileo galilei christiaan huygens johannes kepler blaise pascal and sir isaac newton in the early th century galileo made extensive use of experimentation to validate physical theories which is the key idea in the modern scientific method galileo formulated and successfully tested several results in dynamics in particular the law of inertia which later became the first law in newton's laws of motion in galileo's two new sciences a dialogue between the characters simplicio and salviati discuss the motion of a ship as a moving frame and how that ship's cargo is indifferent to its motion huygens used the motion of a boat along a dutch canal to illustrate an early form of the conservation of momentum
experimental physics is considered to have culminated with the publication of the philosophiae naturalis principia mathematica in by sir isaac newton in newton published the principia detailing two comprehensive and successful physical theories newton's laws of motion from which arise classical mechanics and newton's law of universal gravitation which describes the fundamental force of gravity both theories agreed well with experiment the principia also included several theories in fluid dynamics
from the late th century onward thermodynamics was developed by physicist and chemist boyle young and many others in bernoulli used statistical arguments with classical mechanics to derive thermodynamic results initiating the field of statistical mechanics in thompson demonstrated the conversion of mechanical work into heat and in joule stated the law of conservation of energy in the form of heat as well as mechanical energy ludwig boltzmann in the nineteenth century is responsible for the modern form of statistical mechanics
besides classical mechanics and thermodynamics another great field of experimental inquiry within physics was the nature of electricity observations in the th and eighteenth century by scientists such as robert boyle stephen gray and benjamin franklin created a foundation for later work these observations also established our basic understanding of electrical charge and current by john dalton had discovered that atoms of different elements have different weights and proposed the modern theory of the atom
it was hans christian rsted who first proposed the connection between electricity and magnetism after observing the deflection of a compass needle by a nearby electric current by the early s michael faraday had demonstrated that magnetic fields and electricity could generate each other in james clerk maxwell presented to the royal society a set of equations that described this relationship between electricity and magnetism maxwell's equations also predicted correctly that light is an electromagnetic wave starting with astronomy the principles of natural philosophy crystallized into fundamental laws of physics which were enunciated and improved in the succeeding centuries by the th century the sciences had segmented into multiple fields with specialized researchers and the field of physics although logically pre eminent no longer could claim sole ownership of the entire field of scientific research
current experiments
some examples of prominent experimental physics projects are
method
experimental physics uses two main methods of experimental research controlled experiments and natural experiments controlled experiments are often used in laboratories as laboratories can offer a controlled environment natural experiments are used for example in astrophysics when observing celestial objects where control of the variables in effect is impossible
famous experiments
famous experiments include
experimental techniques
some well known experimental techniques include
prominent experimental physicists
famous experimental physicists include
timelines
see the timelines below for listings of physics experiments

physics

physics from from ph sis nature is the natural science that involves the study of matter and its motion through space and time along with related concepts such as energy and force more broadly it is the general analysis of nature conducted in order to understand how the universe behaves
physics is one of the oldest academic disciplines perhaps the oldest through its inclusion of astronomy over the last two millennia physics was a part of natural philosophy along with chemistry certain branches of mathematics and biology but during the scientific revolution in the th century the natural sciences emerged as unique research programs in their own right physics intersects with many interdisciplinary areas of research such as biophysics and quantum chemistry and the boundaries of physics are not rigidly defined new ideas in physics often explain the fundamental mechanisms of other sciences while opening new avenues of research in areas such as mathematics and philosophy
physics also makes significant contributions through advances in new technologies that arise from theoretical breakthroughs for example advances in the understanding of electromagnetism or nuclear physics led directly to the development of new products that have dramatically transformed modern day society such as television computers domestic appliances and nuclear weapons advances in thermodynamics led to the development of industrialization and advances in mechanics inspired the development of calculus
history
ancient astronomy
astronomy is the oldest of the natural sciences the earliest civilizations dating back to beyond bce such as the sumerians ancient egyptians and the indus valley civilization all had a predictive knowledge and a basic understanding of the motions of the sun moon and stars the stars and planets were often a target of worship believed to represent their gods while the explanations for these phenomena were often unscientific and lacking in evidence these early observations laid the foundation for later astronomy
according to asger aaboe the origins of western astronomy can be found in mesopotamia and all western efforts in the exact sciences are descended from late babylonian astronomy egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies while greek poet homer wrote of various celestial objects in his iliad and odyssey later greek astronomers provided names which are still used today for most constellations visible from the northern hemisphere
natural philosophy
natural philosophy has its origins in greece during the archaic period bc bc when pre socratic philosophers like thales rejected non naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause they proposed ideas verified by reason and observation and many of their hypotheses proved successful in experiment for example atomism was found to be correct approximately years after it was first proposed by leucippus and his pupil democritus
classical physics
physics became a separate science when early modern europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics
major developments in this period include the replacement of the geocentric model of the solar system with the helio centric copernican model the laws governing the motion of planetary bodies determined by johannes kepler between and pioneering work on telescopes and observational astronomy by galileo galilei in the th and th centuries and isaac newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name newton also developed calculus the mathematical study of change which provided new mathematical methods for solving physical problems
the discovery of new laws in thermodynamics chemistry and electromagnetics resulted from greater research efforts during the industrial revolution as energy needs increased the laws comprising classical physics remain very widely used for objects on everyday scales travelling at non relativistic speeds since they provide a very close approximation in such situations and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales however inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the th century
modern physics
modern physics began in the early th century with the work of max planck in quantum theory and albert einstein's theory of relativity both of these theories came about due to inaccuracies in classical mechanics in certain situations classical mechanics predicted a varying speed of light which could not be resolved with the constant speed predicted by maxwell's equations of electromagnetism this discrepancy was corrected by einstein's theory of special relativity which replaced classical mechanics for fast moving bodies and allowed for a constant speed of light black body radiation provided another problem for classical physics which was corrected when planck proposed that light comes in individual packets known as photons this along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals led to the theory of quantum mechanics taking over from classical physics at very small scales
quantum mechanics would come to be pioneered by werner heisenberg erwin schr dinger and paul dirac from this early work and work in related fields the standard model of particle physics was derived following the discovery of a particle with properties consistent with the higgs boson at cern in all fundamental particles predicted by the standard model and no others appear to exist however physics beyond the standard model with theories such as supersymmetry is an active area of research
philosophy
in many ways physics stems from ancient greek philosophy from thales first attempt to characterize matter to democritus deduction that matter ought to reduce to an invariant state the ptolemaic astronomy of a crystalline firmament and aristotle's book physics an early book on physics which attempted to analyze and define motion from a philosophical point of view various greek philosophers advanced their own theories of nature physics was known as natural philosophy until the late th century
by the th century physics was realized as a discipline distinct from philosophy and the other sciences physics as with the rest of science relies on philosophy of science to give an adequate description of the scientific method the scientific method employs a priori reasoning as well as a posteriori reasoning and the use of bayesian inference to measure the validity of a given theory
the development of physics has answered many questions of early philosophers but has also raised new questions study of the philosophical issues surrounding physics the philosophy of physics involves issues such as the nature of space and time determinism and metaphysical outlooks such as empiricism naturalism and realism
many physicists have written about the philosophical implications of their work for instance laplace who championed causal determinism and erwin schr dinger who wrote on quantum mechanics the mathematical physicist roger penrose has been called a platonist by stephen hawking a view penrose discusses in his book the road to reality hawking refers to himself as an unashamed reductionist and takes issue with penrose's views
core theories
though physics deals with a wide variety of systems certain theories are used by all physicists each of these theories were experimentally tested numerous times and found correct as an approximation of nature within a certain domain of validity for instance the theory of classical mechanics accurately describes the motion of objects provided they are much larger than atoms and moving at much less than the speed of light these theories continue to be areas of active research and a remarkable aspect of classical mechanics known as chaos was discovered in the th century three centuries after the original formulation of classical mechanics by isaac newton
these central theories are important tools for research into more specialised topics and any physicist regardless of their specialisation is expected to be literate in them these include classical mechanics quantum mechanics thermodynamics and statistical mechanics electromagnetism and special relativity
classical physics
classical physics includes the traditional branches and topics that were recognised and well developed before the beginning of the th century classical mechanics acoustics optics thermodynamics and electromagnetism classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics study of the forces on a body or bodies not subject to an acceleration kinematics study of motion without regard to its causes and dynamics study of motion and the forces that affect it mechanics may also be divided into solid mechanics and fluid mechanics known together as continuum mechanics the latter including such branches as hydrostatics hydrodynamics aerodynamics and pneumatics acoustics is the study of how sound is produced controlled transmitted and received important modern branches of acoustics include ultrasonics the study of sound waves of very high frequency beyond the range of human hearing bioacoustics the physics of animal calls and hearing and electroacoustics the manipulation of audible sound waves using electronics optics the study of light is concerned not only with visible light but also with infrared and ultraviolet radiation which exhibit all of the phenomena of visible light except visibility e g reflection refraction interference diffraction dispersion and polarization of light heat is a form of energy the internal energy possessed by the particles of which a substance is composed thermodynamics deals with the relationships between heat and other forms of energy electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early th century an electric current gives rise to a magnetic field and a changing magnetic field induces an electric current electrostatics deals with electric charges at rest electrodynamics with moving charges and magnetostatics with magnetic poles at rest
modern physics
classical physics is generally concerned with matter and energy on the normal scale of observation while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale for example atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified the physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter this branch of physics is also known as high energy physics because of the extremely high energies necessary to produce many types of particles in large particle accelerators on this scale ordinary commonsense notions of space time matter and energy are no longer valid
the two chief theories of modern physics present a different picture of the concepts of space time and matter from that presented by classical physics quantum theory is concerned with the discrete rather than continuous nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena the theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer the special theory of relativity is concerned with relative uniform motion in a straight line and the general theory of relativity with accelerated motion and its connection with gravitation both quantum theory and the theory of relativity find applications in all areas of modern physics
difference between classical and modern physics
while physics aims to discover universal laws its theories lie in explicit domains of applicability loosely speaking the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light outside of this domain observations do not match their predictions albert einstein contributed the framework of special relativity which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light max planck erwin schr dinger and others introduced quantum mechanics a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales later quantum field theory unified quantum mechanics and special relativity general relativity allowed for a dynamical curved spacetime with which highly massive systems and the large scale structure of the universe can be well described general relativity has not yet been unified with the other fundamental descriptions several candidate theories of quantum gravity are being developed
relation to other fields
prerequisites
mathematics is the language used for compact description of the order in nature especially the laws of physics this was noted and advocated by pythagoras plato galileo and newton
physics theories use mathematics to obtain order and provide precise formulas precise or estimated solutions quantitative results and predictions experiment results in physics are numerical measurements technologies based on mathematics like computation have made computational physics an active area of research
ontology is a prerequisite for physics but not for mathematics it means physics is ultimately concerned with descriptions of the real world while mathematics is concerned with abstract patterns even beyond the real world thus physics statements are synthetic while mathematical statements are analytic mathematics contains hypotheses while physics contains theories mathematics statements have to be only logically true while predictions of physics statements must match observed and experimental data
the distinction is clear cut but not always obvious for example mathematical physics is the application of mathematics in physics its methods are mathematical but its subject is physical the problems in this field start with a mathematical model of a physical situation and a mathematical description of a physical law every mathematical statement used for solution has a hard to find physical meaning the final mathematical solution has an easier to find meaning because it is what the solver is looking for
physics is a branch of fundamental science not practical science physics is also called the fundamental science because the subject of study of all branches of natural science like chemistry astronomy geology and biology are constrained by laws of physics similar to how chemistry is often called the central science because of its role in linking the physical sciences for example chemistry studies properties structures and reactions of matter chemistry's focus on the atomic scale distinguishes it from physics structures are formed because particles exert electrical forces on each other properties include physical characteristics of given substances and reactions are bound by laws of physics like conservation of energy mass and charge
physics is applied in industries like engineering and medicine
application and influence
applied physics is a general term for physics research which is intended for a particular use an applied physics curriculum usually contains a few classes in an applied discipline like geology or electrical engineering it usually differs from engineering in that an applied physicist may not be designing something in particular but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem
the approach is similar to that of applied mathematics applied physicists can also be interested in the use of physics for scientific research for instance people working on accelerator physics might seek to build better particle detectors for research in theoretical physics
physics is used heavily in engineering for example statics a subfield of mechanics is used in the building of bridges and other static structures the understanding and use of acoustics results in sound control and better concert halls similarly the use of optics creates better optical devices an understanding of physics makes for more realistic flight simulators video games and movies and is often critical in forensic investigations
with the standard consensus that the laws of physics are universal and do not change with time physics can be used to study things that would ordinarily be mired in uncertainty for example in the study of the origin of the earth one can reasonably model earth's mass temperature and rate of rotation as a function of time allowing one to extrapolate forward and backward in time and so predict prior and future conditions it also allows for simulations in engineering which drastically speed up the development of a new technology
but there is also considerable interdisciplinarity in the physicist's methods so many other important fields are influenced by physics e g the fields of econophysics and sociophysics
research
scientific method
physicists use the scientific method to test the validity of a physical theory using a methodical approach to compare the implications of the theory in question with the associated conclusions drawn from experiments and observations conducted to test it experiments and observations are collected and compared with the predictions and hypotheses made by a theory thus aiding in the determination or the validity invalidity of the theory
a scientific law is a concise verbal or mathematical statement of a relation which expresses a fundamental principle of some theory such as newton's law of universal gravitation
theory and experiment
theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena although theory and experiment are developed separately they are strongly dependent upon each other progress in physics frequently comes about when experimentalists make a discovery that existing theories cannot explain or when new theories generate experimentally testable predictions which inspire new experiments
physicists who work at the interplay of theory and experiment are called phenomenologists phenomenologists look at the complex phenomena observed in experiment and work to relate them to fundamental theory
theoretical physics has historically taken inspiration from philosophy electromagnetism was unified this way beyond the known universe the field of theoretical physics also deals with hypothetical issues such as parallel universes a multiverse and higher dimensions theorists invoke these ideas in hopes of solving particular problems with existing theories they then explore the consequences of these ideas and work toward making testable predictions
experimental physics expands and is expanded by engineering and technology experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging mri and transistors feynman has noted that experimentalists may seek areas which are not well explored by theorists
scope and aims
physics covers a wide range of phenomena from elementary particles such as quarks neutrinos and electrons to the largest superclusters of galaxies included in these phenomena are the most basic objects composing all other things therefore physics is sometimes called the fundamental science physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena thus physics aims to both connect the things observable to humans to root causes and then connect these causes together
for example the ancient chinese observed that certain rocks lodestone were attracted to one another by some invisible force this effect was later called magnetism and was first rigorously studied in the th century a little earlier than the chinese the ancient greeks knew of other objects such as amber that when rubbed with fur would cause a similar invisible attraction between the two this was also first studied rigorously in the th century and came to be called electricity thus physics had come to understand two observations of nature in terms of some root cause electricity and magnetism however further work in the th century revealed that these two forces were just two different aspects of one force electromagnetism this process of unifying forces continues today and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction physics hopes to find an ultimate reason theory of everything for why nature is as it is see section current research below for more information
research fields
contemporary research in physics can be broadly divided into condensed matter physics atomic molecular and optical physics particle physics astrophysics geophysics and biophysics some physics departments also support physics education research and physics outreach
since the th century the individual fields of physics have become increasingly specialized and today most physicists work in a single field for their entire careers universalists such as albert einstein and lev landau who worked in multiple fields of physics are now very rare
the major fields of physics along with their subfields and the theories they employ are shown in the following table
condensed matter
condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter in particular it is concerned with the condensed phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong
the most familiar examples of condensed phases are solids and liquids which arise from the bonding by way of the electromagnetic force between atoms more exotic condensed phases include the superfluid and the bose einstein condensate found in certain atomic systems at very low temperature the superconducting phase exhibited by conduction electrons in certain materials and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices
condensed matter physics is the largest field of contemporary physics historically condensed matter physics grew out of solid state physics which is now considered one of its main subfields the term condensed matter physics was apparently coined by philip anderson when he renamed his research group previously solid state theory in in the division of solid state physics of the american physical society was renamed as the division of condensed matter physics condensed matter physics has a large overlap with chemistry materials science nanotechnology and engineering
atomic molecular and optical physics
atomic molecular and optical physics amo is the study of matter matter and light matter interactions on the scale of single atoms and molecules the three areas are grouped together because of their interrelationships the similarity of methods used and the commonality of their relevant energy scales all three areas include both classical semi classical and quantum treatments they can treat their subject from a microscopic view in contrast to a macroscopic view
atomic physics studies the electron shells of atoms current research focuses on activities in quantum control cooling and trapping of atoms and ions low temperature collision dynamics and the effects of electron correlation on structure and dynamics atomic physics is influenced by the nucleus see e g hyperfine splitting but intra nuclear phenomena such as fission and fusion are considered part of high energy physics
molecular physics focuses on multi atomic structures and their internal and external interactions with matter and light optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm
high energy physics particle physics and nuclear physics
particle physics is the study of the elementary constituents of matter and energy and the interactions between them in addition particle physicists design and develop the high energy accelerators detectors and computer programs necessary for this research the field is also called high energy physics because many elementary particles do not occur naturally but are created only during high energy collisions of other particles
currently the interactions of elementary particles and fields are described by the standard model the model accounts for the known particles of matter quarks and leptons that interact via the strong weak and electromagnetic fundamental forces dynamics are described in terms of matter particles exchanging gauge bosons gluons w and z bosons and photons respectively the standard model also predicts a particle known as the higgs boson in july cern the european laboratory for particle physics announced the detection of a particle consistent with the higgs boson an integral part of a higgs mechanism
nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei the most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology but the research has provided application in many fields including those in nuclear medicine and magnetic resonance imaging ion implantation in materials engineering and radiocarbon dating in geology and archaeology
astrophysics
astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure stellar evolution the origin of the solar system and related problems of cosmology because astrophysics is a broad subject astrophysicists typically apply many disciplines of physics including mechanics electromagnetism statistical mechanics thermodynamics quantum mechanics relativity nuclear and particle physics and atomic and molecular physics
the discovery by karl jansky in that radio signals were emitted by celestial bodies initiated the science of radio astronomy most recently the frontiers of astronomy have been expanded by space exploration perturbations and interference from the earth's atmosphere make space based observations necessary for infrared ultraviolet gamma ray and x ray astronomy
physical cosmology is the study of the formation and evolution of the universe on its largest scales albert einstein's theory of relativity plays a central role in all modern cosmological theories in the early th century hubble's discovery that the universe is expanding as shown by the hubble diagram prompted rival explanations known as the steady state universe and the big bang
the big bang was confirmed by the success of big bang nucleosynthesis and the discovery of the cosmic microwave background in the big bang model rests on two theoretical pillars albert einstein's general relativity and the cosmological principle cosmologists have recently established the cdm model of the evolution of the universe which includes cosmic inflation dark energy and dark matter
numerous possibilities and discoveries are anticipated to emerge from new data from the fermi gamma ray space telescope over the upcoming decade and vastly revise or clarify existing models of the universe in particular the potential for a tremendous discovery surrounding dark matter is possible over the next several years fermi will search for evidence that dark matter is composed of weakly interacting massive particles complementing similar experiments with the large hadron collider and other underground detectors
ibex is already yielding new astrophysical discoveries no one knows what is creating the ena energetic neutral atoms ribbon along the termination shock of the solar wind but everyone agrees that it means the textbook picture of the heliosphere in which the solar system's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing galactic wind of the interstellar medium in the shape of a comet is wrong
current research
research in physics is continually progressing on a large number of fronts
in condensed matter physics an important unsolved theoretical problem is that of high temperature superconductivity many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers
in particle physics the first pieces of experimental evidence for physics beyond the standard model have begun to appear foremost among these are indications that neutrinos have non zero mass these experimental results appear to have solved the long standing solar neutrino problem and the physics of massive neutrinos remains an area of active theoretical and experimental research particle accelerators have begun probing energy scales in the tev range in which experimentalists are hoping to find evidence for the higgs boson and supersymmetric particles
theoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity a program ongoing for over half a century have not yet been decisively resolved the current leading candidates are m theory superstring theory and loop quantum gravity
many astronomical and cosmological phenomena have yet to be satisfactorily explained including the existence of ultra high energy cosmic rays the baryon asymmetry the acceleration of the universe and the anomalous rotation rates of galaxies
although much progress has been made in high energy quantum and astronomical physics many everyday phenomena involving complexity chaos or turbulence are still poorly understood complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved examples include the formation of sandpiles nodes in trickling water the shape of water droplets mechanisms of surface tension catastrophes and self sorting in shaken heterogeneous collections
these complex phenomena have received growing attention since the s for several reasons including the availability of modern mathematical methods and computers which enabled complex systems to be modeled in new ways complex physics has become part of increasingly interdisciplinary research as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems in horace lamb said
external links
general
organizations

statistical mechanics

statistical mechanics is a branch of theoretical physics and chemistry and mathematical physics that studies using probability theory the average behaviour of a mechanical system where the state of the system is uncertain
the classical view of the universe was that its fundamental laws are mechanical in nature and that all physical systems are therefore governed by mechanical laws at a microscopic level these laws are precise equations of motion that map any given initial state to a corresponding future state at a later time there is however a disconnection between these laws and everyday life experiences as we do not find it necessary nor even theoretically possible to know exactly at a microscopic level the simultaneous positions and velocities of each molecule while carrying out processes at the human scale for example when performing a chemical reaction statistical mechanics is a collection of mathematical tools that are used to fill this disconnection between the laws of mechanics and the practical experience of incomplete knowledge
a common use of statistical mechanics is in explaining the thermodynamic behaviour of large systems microscopic mechanical laws do not contain concepts such as temperature heat or entropy however statistical mechanics shows how these concepts arise from the natural uncertainty that arises about the state of a system when that system is prepared in practice the benefit of using statistical mechanics is that it provides exact methods to connect thermodynamic quantities such as heat capacity to microscopic behaviour whereas in classical thermodynamics the only available option would be to just measure and tabulate such quantities for various materials statistical mechanics also makes it possible to extend the laws of thermodynamics to cases which are not considered in classical thermodynamics for example microscopic systems and other mechanical systems with few degrees of freedom this branch of statistical mechanics which treats and extends classical thermodynamics is known as statistical thermodynamics or equilibrium statistical mechanics
statistical mechanics also finds use outside equilibrium an important subbranch known as non equilibrium statistical mechanics deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances examples of such processes include chemical reactions or flows of particles and heat unlike with equilibrium there is no exact formalism that applies to non equilibrium statistical mechanics in general and so this branch of statistical mechanics remains an active area of theoretical research
principles mechanics and ensembles
in physics there are two types of mechanics usually examined classical mechanics and quantum mechanics for both types of mechanics the standard mathematical approach is to consider two ingredients
using these two ingredients the state at any other time past or future can in principle be calculated
whereas ordinary mechanics only considers the behaviour of a single state statistical mechanics introduces the statistical ensemble which is a large collection of virtual independent copies of the system in various states the statistical ensemble is a probability distribution over all possible states of the system in classical statistical mechanics the ensemble is a probability distribution over phase points as opposed to a single phase point in ordinary mechanics usually represented as a distribution in a phase space with canonical coordinates in quantum statistical mechanics the ensemble is a probability distribution over pure states and can be compactly summarized as a density matrix
as is usual for probabilities the ensemble can be interpreted in different ways
these two meanings are equivalent for many purposes and will be used interchangeably in this article
however the probability is interpreted each state in the ensemble evolves over time according to the equation of motion thus the ensemble itself the probability distribution over states also evolves as the virtual systems in the ensemble continually leave one state and enter another the ensemble evolution is given by the liouville equation classical mechanics or the von neumann equation quantum mechanics these equations are simply derived by the application of the mechanical equation of motion separately to each virtual system contained in the ensemble with the probability of the virtual system being conserved over time as it evolves from state to state
one special class of ensemble is those ensembles that do not evolve over time these ensembles are known as equilibrium ensembles and their condition is known as statistical equilibrium statistical equilibrium occurs if for each state in the ensemble the ensemble also contains all of its future and past states with probabilities equal to that state the study of equilibrium ensembles of isolated systems is the focus of statistical thermodynamics non equilibrium statistical mechanics addresses the more general case of ensembles that change over time and or ensembles of non isolated systems
statistical thermodynamics
the primary goal of statistical thermodynamics also known as equilibrium statistical mechanics is to explain the classical thermodynamics of materials in terms of the properties of their constituent particles and the interactions between them in other words statistical thermodynamics provides a connection between the macroscopic properties of materials in thermodynamic equilibrium and the microscopic behaviours and motions occurring inside the material
as an example one might ask what is it about a thermodynamic system of nh molecules that determines the free energy characteristic of that compound classical thermodynamics does not provide the answer if for example we were given spectroscopic data of this body of gas molecules such as bond length bond angle bond rotation and flexibility of the bonds in nh we should see that the free energy could not be other than it is to prove this true we need to bridge the gap between the microscopic realm of atoms and molecules and the macroscopic realm of classical thermodynamics statistical mechanics demonstrates how the thermodynamic parameters of a system such as temperature and pressure are related to microscopic behaviours of such constituent atoms and molecules
although we may understand a system generically in general we lack information about the state of a specific instance of that system for this reason the notion of statistical ensemble a probability distribution over possible states is necessary furthermore in order to reflect that the material is in a thermodynamic equilibrium it is necessary to introduce a corresponding statistical mechanical definition of equilibrium the analogue of thermodynamic equilibrium in statistical thermodynamics is the ensemble property of statistical equilibrium described in the previous section an additional assumption in statistical thermodynamics is that the system is isolated no varying external forces are acting on the system so that its total energy does not vary over time a sufficient but not necessary condition for statistical equilibrium with an isolated system is that the probability distribution is a function only of conserved properties total energy total particle numbers etc
fundamental postulate
there are many different equilibrium ensembles that can be considered and only some of them correspond to thermodynamics an additional postulate is necessary to motivate why the ensemble for a given system should have one form or another
a common approach found in many textbooks is to take the equal a priori probability postulate this postulate states that
the equal a priori probability postulate therefore provides a motivation for the microcanonical ensemble described below there are various arguments in favour of the equal a priori probability postulate
other fundamental postulates for statistical mechanics have also been proposed
in any case the reason for establishing the microcanonical ensemble is mainly axiomatic the microcanonical ensemble itself is mathematically awkward to use for real calculations and even very simple finite systems can only be solved approximately however it is possible to use the microcanonical ensemble to construct a hypothetical infinite thermodynamic reservoir that has an exactly defined notion of temperature and chemical potential once this reservoir has been established it can be used to justify exactly the canonical ensemble or grand canonical ensemble see below for any other system by considering the contact of this system with the reservoir these other ensembles are those actually used in practical statistical mechanics calculations as they are mathematically simpler and also correspond to a much more realistic situation energy not known exactly
three thermodynamic ensembles
there are three equilibrium ensembles with a simple form that can be defined for any isolated system bounded inside a finite volume these are the most often discussed ensembles in statistical thermodynamics in the macroscopic limit defined below they all correspond to classical thermodynamics
statistical fluctuations and the macroscopic limit
the thermodynamic ensembles most significant difference is that they either admit uncertainty in the variables of energy or particle number or that those variables are fixed to particular values while this difference can be observed in some cases for macroscopic systems the thermodynamic ensembles are usually observationally equivalent
the limit of large systems in statistical mechanics is known as the thermodynamic limit in the thermodynamic limit the microcanonical canonical and grand canonical ensembles tend to give identical predictions about thermodynamic characteristics this means that one can specify either total energy or temperature and arrive at the same result likewise one can specify either total particle number or chemical potential given these considerations the best ensemble to choose for the calculation of the properties of a macroscopic system is usually just the ensemble which allows the result to be derived most easily
important cases where the thermodynamic ensembles do not give identical results include
in these cases the correct thermodynamic ensemble must be chosen as there are observable differences between these ensembles not just in the size of fluctuations but also in average quantities such as the distribution of particles the correct ensemble is that which corresponds to the way the system has been prepared and characterized in other words the ensemble that reflects the knowledge about that system
illustrative example a gas
the above concepts can be illustrated for the specific case of one liter of ammonia gas at standard conditions note that statistical thermodynamics is not restricted to the study of macroscopic gases and the example of a gas is given here to illustrate concepts statistical mechanics and statistical thermodynamics apply to all mechanical systems including microscopic systems and to all phases of matter liquids solids plasmas gases nuclear matter quark matter
a simple way to prepare one litre sample of ammonia in a standard condition is to take a very large reservoir of ammonia at those standard conditions and connect it to a previously evacuated one litre container after ammonia gas has entered the container and the container has been given time to reach thermodynamic equilibrium with the reservoir the container is then sealed and isolated in thermodynamics this is a repeatable process resulting in a very well defined sample of gas with a precise description we now consider the corresponding precise description in statistical thermodynamics
although this process is well defined and repeatable in a macroscopic sense we have no information about the exact locations and velocities of each and every molecule in the container of gas moreover we do not even know exactly how many molecules are in the container even supposing we knew exactly the average density of the ammonia gas in general we do not know how many molecules of the gas happened to be inside our container at the moment when we sealed it the sample is in equilibrium and is in equilibrium with the reservoir we could reconnect it to the reservoir for some time and then re seal it and our knowledge about the state of the gas would not change in this case our knowledge about the state of the gas is precisely described by the grand canonical ensemble provided we have an accurate microscopic model of the ammonia gas we could in principle compute all thermodynamic properties of this sample of gas by using the distribution provided by the grand canonical ensemble
hypothetically we could use an extremely sensitive weight scale to measure exactly the mass of the container before and after introducing the ammonia gas so that we can exactly know the number of ammonia molecules after we make this measurement then our knowledge about the gas would correspond to the canonical ensemble finally suppose by some hypothetical apparatus we can measure exactly the number of molecules and also measure exactly the total energy of the system supposing furthermore that this apparatus gives us no further information about the molecules positions and velocities our knowledge about the system would correspond to the microcanonical ensemble
even after making such measurements however our expectations about the behaviour of the gas do not change appreciably this is because the gas sample is macroscopic and approximates very well the thermodynamic limit so the different ensembles behave similarly this can be demonstrated by considering how small the actual fluctuations would be
suppose that we knew the number density of ammonia gas was exactly molecules per liter inside the reservoir of ammonia gas used to fill the one litre container in describing the container with the grand canonical ensemble then the average number of molecules would be formula and the uncertainty standard deviation in the number of molecules would be formula assuming poisson distribution which is relatively very small compared to the total number of molecules upon measuring the particle number thus arriving at a canonical ensemble we should find very nearly molecules for example the probability of finding more than or less than molecules would be about in
calculation methods
once the characteristic state function for an ensemble has been calculated for a given system that system is solved macroscopic observables can be extracted from the characteristic state function calculating the characteristic state function of a thermodynamic ensemble is not necessarily a simple task however since it involves considering every possible state of the system while some hypothetical systems have been exactly solved the most general and realistic case is too complex for exact solution various approaches exist to approximate the true ensemble and allow calculation of average quantities
exact
there are some cases which allow exact solutions
monte carlo
one approximate approach that is particularly well suited to computers is the monte carlo method which examines just a few of the possible states of the system with the states chosen randomly with a fair weight as long as these states form a representative sample of the whole set of states of the system the approximate characteristic function is obtained as more and more random samples are included the errors are reduced to an arbitrarily low level
non equilibrium statistical mechanics
there are many physical phenomena of interest that involve quasi thermodynamic processes out of equilibrium for example
all of these processes occur over time with characteristic rates and these rates are of importance for engineering the field of non equilibrium statistical mechanics is concerned with understanding these non equilibrium processes at the microscopic level statistical thermodynamics can only be used to calculate the final result after the external imbalances have been removed and the ensemble has settled back down to equilibrium
in principle non equilibrium statistical mechanics could be mathematically exact ensembles for an isolated system evolve over time according to deterministic equations such as liouville's equation or its quantum equivalent the von neumann equation these equations are the result of applying the mechanical equations of motion independently to each state in the ensemble unfortunately these ensemble evolution equations inherit much of the complexity of the underlying mechanical motion and so exact solutions are very difficult to obtain moreover the ensemble evolution equations are fully reversible and do not destroy information the ensemble's gibbs entropy is preserved in order to make headway in modelling irreversible processes it is necessary to add additional ingredients besides probability and reversible mechanics
non equilibrium mechanics is therefore an active area of theoretical research as the range of validity of these additional assumptions continues to be explored a few approaches are described in the following subsections
stochastic methods
one approach to non equilibrium statistical mechanics is to incorporate stochastic random behaviour into the system stochastic behaviour destroys information contained in the ensemble while this is technically inaccurate aside from hypothetical situations involving black holes a system cannot in itself cause loss of information the randomness is added to reflect that information of interest becomes converted over time into subtle correlations within the system or to correlations between the system and environment these correlations appear as chaotic or pseudorandom influences on the variables of interest by replacing these correlations with randomness proper the calculations can be made much easier
near equilibrium methods
another important class of non equilibrium statistical mechanical models deals with systems that are only very slightly perturbed from equilibrium with very small perturbations the response can be analysed in linear response theory a remarkable result as formalized by the fluctuation dissipation theorem is that the response of a system when near equilibrium is precisely related to the fluctuations that occur when the system is in total equilibrium essentially a system that is slightly away from equilibrium whether put there by external forces or by fluctuations relaxes towards equilibrium in the same way since the system cannot tell the difference or know how it came to be away from equilibrium
this provides an indirect avenue for obtaining numbers such as ohmic conductivity and thermal conductivity by extracting results from equilibrium statistical mechanics since equilibrium statistical mechanics is mathematically well defined and in some cases more amenable for calculations the fluctuation dissipation connection can be a convenient shortcut for calculations in near equilibrium statistical mechanics
a few of the theoretical tools used to make this connection include
hybrid methods
an advanced approach uses a combination of stochastic methods and linear response theory as an example one approach to compute quantum coherence effects weak localization conductance fluctuations in the conductance of an electronic system is the use of the green kubo relations with the inclusion of stochastic dephasing by interactions between various electrons by use of the keldysh method
applications outside thermodynamics
the ensemble formalism also can be used to analyze general mechanical systems with uncertainty in knowledge about the state of a system ensembles are also used in
history
in swiss physicist and mathematician daniel bernoulli published hydrodynamica which laid the basis for the kinetic theory of gases in this work bernoulli posited the argument still used to this day that gases consist of great numbers of molecules moving in all directions that their impact on a surface causes the gas pressure that we feel and that what we experience as heat is simply the kinetic energy of their motion
in after reading a paper on the diffusion of molecules by rudolf clausius scottish physicist james clerk maxwell formulated the maxwell distribution of molecular velocities which gave the proportion of molecules having a certain velocity in a specific range this was the first ever statistical law in physics five years later in ludwig boltzmann a young student in vienna came across maxwell s paper and was so inspired by it that he spent much of his life developing the subject further
statistical mechanics proper was initiated in the s with the work of boltzmann much of which was collectively published in his lectures on gas theory boltzmann's original papers on the statistical interpretation of thermodynamics the h theorem transport theory thermal equilibrium the equation of state of gases and similar subjects occupy about pages in the proceedings of the vienna academy and other societies boltzmann introduced the concept of an equilibrium statistical ensemble and also investigated for the first time non equilibrium statistical mechanics with his h theorem
the term statistical mechanics was coined by the american mathematical physicist j willard gibbs in probabilistic mechanics might today seem a more appropriate term but statistical mechanics is firmly entrenched shortly before his death gibbs published in elementary principles in statistical mechanics a book which formalized statistical mechanics as a fully general approach to address all mechanical systems macroscopic or microscopic gaseous or non gaseous gibbs methods were initially derived in the framework classical mechanics however they were of such generality that they were found to adapt easily to the later quantum mechanics and still form the foundation of statistical mechanics to this day

modern physics

modern physics is an effort to understand the underlying processes of the interactions of matter utilizing the tools of science engineering it implies that nineteenth century descriptions of phenomena are not sufficient to describe nature as observed with modern instruments it is generally assumed that a consistent description of these observations will incorporate elements of quantum mechanics relativity
small velocities and large distances is usually the realm of classical physics modern physics often involves extreme conditions in practice quantum effects typically involve distances comparable to atoms roughly m while relativistic effects typically involve velocities comparable to the speed of light roughly m s
overview
in a literal sense the term modern physics means up to date physics in this sense a significant portion of so called classical physics is modern however since roughly new discoveries have caused significant paradigm shifts the advent of quantum mechanics qm and of einsteinian relativity er physics that incorporates elements of either qm or er or both is said to be modern physics it is in this latter sense that the term is generally used
modern physics is often encountered when dealing with extreme conditions quantum mechanical effects tend to appear when dealing with lows low temperatures small distances while relativistic effects tend to appear when dealing with highs high velocities large distances the middles being classical behaviour for example when analysing the behaviour of a gas at room temperature most phenomena will involve the classical maxwell boltzmann distribution however near absolute zero the maxwell boltzmann distribution fails to account for the observed behaviour of the gas and the modern fermi dirac or bose einstein distributions have to be used instead
very often it is possible to find or retrieve the classical behaviour from the modern description by analysing the modern description at low speeds and large distances by taking a limit or by making an approximation when doing so the result is called the classical limit
hallmarks of modern physics
these are generally considered to be the topics regarded as the core of the foundation of modern physics

branches of physics

physics deals with the combination of matter and energy it also deals with a wide variety of systems about which theories have been developed that are used by physicists in general theories are experimentally tested numerous times before they are accepted as correct as a description of nature within a certain domain of validity for instance the theory of classical mechanics accurately describes the motion of objects provided they are much larger than atoms and moving at much less than the speed of light these theories continue to be areas of active research for instance a remarkable aspect of classical mechanics known as chaos was discovered in the th century three centuries after the original formulation of classical mechanics by isaac newton these central theories are important tools for research in more specialized topics and any physicist regardless of his or her specialization is expected to be literate in them
classical mechanics
classical mechanics is a model of the physics of forces acting upon bodies it is often referred to as newtonian mechanics after isaac newton and his laws of motion it also includes classical approach as given by hamiltonian and lagrange methods
thermodynamics and statistical mechanics
the first chapter of the feynman lectures on physics is about the existence of atoms which feynman considered to be the most compact statement of physics from which science could easily result even if all other knowledge was lost by modeling matter as collections of hard spheres it is possible to describe the kinetic theory of gases upon which classical thermodynamics is based
thermodynamics studies the effects of changes in temperature pressure and volume on physical systems on the macroscopic scale and the transfer of energy as heat historically thermodynamics developed out of the desire to increase the efficiency of early steam engines
the starting point for most thermodynamic considerations is the laws of thermodynamics which postulate that energy can be exchanged between physical systems as heat or work they also postulate the existence of a quantity named entropy which can be defined for any system in thermodynamics interactions between large ensembles of objects are studied and categorized central to this are the concepts of system and surroundings a system is composed of particles whose average motions define its properties which in turn are related to one another through equations of state properties can be combined to express internal energy and thermodynamic potentials which are useful for determining conditions for equilibrium and spontaneous processes
relativity
the special theory of relativity enjoys a relationship with electromagnetism and mechanics that is the principle of relativity and the principle of stationary action in mechanics can be used to derive maxwell's equations and vice versa
the theory of special relativity was proposed in by albert einstein in his article on the electrodynamics of moving bodies the title of the article refers to the fact that special relativity resolves an inconsistency between maxwell's equations and classical mechanics the theory is based on two postulates that the mathematical forms of the laws of physics are invariant in all inertial systems and that the speed of light in a vacuum is constant and independent of the source or observer reconciling the two postulates requires a unification of space and time into the frame dependent concept of spacetime
general relativity is the geometrical theory of gravitation published by albert einstein in it unifies special relativity newton's law of universal gravitation and the insight that gravitation can be described by the curvature of space and time in general relativity the curvature of spacetime is produced by the energy of matter and radiation
quantum mechanics
quantum mechanics is the branch of physics treating atomic and subatomic systems and their interaction with radiation it is based on the observation that all forms of energy are released in discrete units or bundles called quanta remarkably quantum theory typically permits only probable or statistical calculation of the observed features of subatomic particles understood in terms of wave functions the schr dinger equation plays the role in quantum mechanics that newton's laws and conservation of energy serve in classical mechanics i e it predicts the future behavior of a dynamic system and is a wave equation that is used to solve for wavefunctions
for example the light or electromagnetic radiation emitted or absorbed by an atom has only certain frequencies or wavelengths as can be seen from the line spectrum associated with the chemical element represented by that atom the quantum theory shows that those frequencies correspond to definite energies of the light quanta or photons and result from the fact that the electrons of the atom can have only certain allowed energy values or levels when an electron changes from one allowed level to another a quantum of energy is emitted or absorbed whose frequency is directly proportional to the energy difference between the two levels the photoelectric effect further confirmed the quantization of light
in louis de broglie proposed that not only do light waves sometimes exhibit particle like properties but particles may also exhibit wave like properties two different formulations of quantum mechanics were presented following de broglie's suggestion the wave mechanics of erwin schr dinger involves the use of a mathematical entity the wave function which is related to the probability of finding a particle at a given point in space the matrix mechanics of werner heisenberg makes no mention of wave functions or similar concepts but was shown to be mathematically equivalent to schr dinger's theory a particularly important discovery of the quantum theory is the uncertainty principle enunciated by heisenberg in which places an absolute theoretical limit on the accuracy of certain measurements as a result the assumption by earlier scientists that the physical state of a system could be measured exactly and used to predict future states had to be abandoned quantum mechanics was combined with the theory of relativity in the formulation of paul dirac other developments include quantum statistics quantum electrodynamics concerned with interactions between charged particles and electromagnetic fields and its generalization quantum field theory
interdisciplinary fields
to the interdisciplinary fields which define partially sciences of their own belong e g the
summary
the table below lists the core theories along with many of the concepts they employ

glossary of classical physics

this article is a glossary of classical physics it is some of the most common terms in classical physics and how they are used

outline of physics

the following outline is provided as an overview of and topical guide to physics
physics natural science that involves the study of matter and its motion through spacetime along with related concepts such as energy and force more broadly it is the general analysis of nature conducted in order to understand how the universe behaves
what type of thing is physics
physics can be described as all of the following
history of physics
history of physics history of the physical science that studies matter and its motion through space time and related concepts such as energy and force
general concepts of physics
basic principles of physics
physics branch of science that studies matter and its motion through space and time along with related concepts such as energy and force physics is one of the fundamental sciences because the other natural sciences like biology geology etc deal with systems that seem to obey the laws of physics according to physics the physical laws of matter energy and the fundamental forces of nature govern the interactions between particles and physical entities such as planets molecules atoms or the subatomic particles some of the basic pursuits of physics which include some of the most prominent developments in modern science in the last millennium include
gravity light physical system physical observation physical quantity physical state physical unit physical theory physical experiment
theoretical concepts
mass energy equivalence particle physical field physical interaction physical law fundamental force physical constant wave
overview
this is a list of the primary theories in physics major subtopics and concepts

chemical species

chemical species are atoms molecules molecular fragments ions etc subjected to a chemical process or to a measurement generally a chemical species can be defined as an ensemble of chemically identical molecular entities that can explore the same set of molecular energy levels on a characteristic or delineated time scale the term may be applied equally to a set of chemically identical atomic or molecular structural units in a solid array
in supramolecular chemistry chemical species are those supramolecular structures whose interactions and associations are brought about via intermolecular bonding and debonding actions and function to form the basis of this branch of chemistry

sonochemistry

in chemistry the study of sonochemistry is concerned with understanding the effect of ultrasound in forming acoustic cavitation in liquids resulting in the initiation or enhancement of the chemical activity in the solution therefore the chemical effects of ultrasound do not come from a
direct interaction of the ultrasonic sound wave with the molecules in the solution the simplest explanation for this is that sound waves propagating through a liquid at ultrasonic frequencies do so with a wavelength that is significantly longer than that of the bond length between atoms in the molecule therefore the sound wave cannot affect that vibrational energy of the bond and can therefore not directly increase the internal energy of a molecule instead sonochemistry arises from acoustic cavitation the formation growth and implosive collapse of bubbles in a liquid the collapse of these bubbles is an almost adiabatic process thereby resulting in the massive build up of energy inside the bubble resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid the high temperatures and pressures result in the chemical excitation of any matter that was inside of or in the immediate surroundings of the bubble as it rapidly imploded a broad variety of outcomes can result from acoustic cavitation including sonoluminescence increased chemical activity in the solution due to the formation of primary and secondary radical reactions and increase chemical activity through the formation of new relatively stable chemical species that can diffuse further into the solution to create chemical effects for example the formation of hydrogen peroxide from the combination of two hydroxyl radicals formed following the dissociation of water vapor inside the collapsing bubbles what water is exposed to ultrasound
the influence of sonic waves traveling through liquids was first reported by robert williams wood and alfred lee loomis in the experiment was about the frequency of the energy that it took for sonic waves to penetrate the barrier of water he came to the conclusion that sound does travel faster in water but because of the water's density compared to our earth's atmosphere it was incredibly hard to get the sonic waves into the water after lots of research they decided that the best way to disperse sound into the water was to make loud noises into the water by creating bubbles that were made at the same time as the sound one of the easier ways that they put sound into the water was they simply yelled but another road block they ran into was the ratio of the amount of time it took for the lower frequency waves to penetrate the bubbles walls and access the water around the bubble and then time from that point to the point on the other end of the body of water but despite the revolutionary ideas of this article it was left mostly unnoticed sonochemistry experienced a renaissance in the s with the advent of inexpensive and reliable generators of high intensity ultrasound
upon irradiation with high intensity sound or ultrasound acoustic cavitation usually occurs cavitation the formation growth and implosive collapse of bubbles irradiated with sound is the impetus for sonochemistry and sonoluminescence bubble collapse in liquids produces enormous amounts of energy from the conversion of kinetic energy of the liquid motion into heating the contents of the bubble the compression of the bubbles during cavitation is more rapid than thermal transport which generates a short lived localized hot spot experimental results have shown that these bubbles have temperatures around k pressures of roughly atm and heating and cooling rates above k s these cavitations can create extreme physical and chemical conditions in otherwise cold liquids
with liquids containing solids similar phenomena may occur with exposure to ultrasound once cavitation occurs near an extended solid surface cavity collapse is nonspherical and drives high speed jets of liquid to the surface these jets and associated shock waves can damage the now highly heated surface liquid powder suspensions produce high velocity interparticle collisions these collisions can change the surface morphology composition and reactivity
three classes of sonochemical reactions exist homogeneous sonochemistry of liquids heterogeneous sonochemistry of liquid liquid or solid liquid systems and overlapping with the aforementioned sonocatalysis sonoluminescence is typically regarded as a special case of homogeneous sonochemistry the chemical enhancement of reactions by ultrasound has been explored and has beneficial applications in mixed phase synthesis materials chemistry and biomedical uses because cavitation can only occur in liquids chemical reactions are not seen in the ultrasonic irradiation of solids or solid gas systems
for example in chemical kinetics it has been observed that ultrasound can greatly enhance chemical reactivity in a number of systems by as much as a million fold effectively acting as a catalyst by exciting the atomic and molecular modes of the system such as the vibrational rotational and translational modes in addition in reactions that use solids ultrasound breaks up the solid pieces from the energy released from the bubbles created by cavitation collapsing through them this gives the solid reactant a larger surface area for the reaction to proceed over increasing the observed rate of reaction
while the application of ultrasound often generates mixtures of products a paper published in in the journal nature described the use of ultrasound to selectively affect a certain cyclobutane ring opening reaction atul kumar has reported multicomponent reaction hantzsch ester synthesis in aqueous micelles using ultrasound
some water pollutants especially chlorinated organic compounds can be destroyed sonochemically
sonochemistry can be performed by using a bath usually used for ultrasonic cleaning or with a high power probe called an ultrasonic horn

chemical library

a chemical library or compound library is a collection of stored chemicals usually used ultimately in high throughput screening or industrial manufacture the chemical library can consist in simple terms of a series of stored chemicals each chemical has associated information stored in some kind of database with information such as the chemical structure purity quantity and physiochemical characteristics of the compound
purpose
in drug discovery high throughput screening it is desirable to screen a drug target against a selection of chemicals that try to take advantage of as much of the appropriate chemical space as possible the chemical space of all possible chemical structures is extraordinarily large most stored chemical libraries do not typically have a fully represented or sampled chemical space mostly because of storage and cost concerns however since many molecular interactions cannot be predicted the wider the chemical space that is sampled by the chemical library the better the chance that high throughput screening will find a hit a chemical with an appropriate interaction in a biological model that might be developed into a drug
an example of a chemical library in drug discovery would be a series of chemicals known to inhibit kinases or in industrial processes a series of catalysts known to polymerize resins
generation of chemical libraries
chemical libraries are usually generated for a specific goal and larger chemical libraries could be made of several groups of smaller libraries stored in the same location in the drug discovery process for instance a wide range of organic chemicals are needed to test against models of disease in high throughput screening therefore most of the chemical synthesis needed to generate chemical libraries in drug discovery is based on organic chemistry a company that is interested in screening for kinase inhibitors in cancer may limit their chemical libraries and synthesis to just those types of chemicals known to have affinity for atp binding sites or allosteric sites
generally however most chemical libraries focus on large groups of varied organic chemical series where an organic chemist can make many variations on the same molecular scaffold or molecular backbone sometimes chemicals can be purchased from outside vendors as well and included into an internal chemical library
depending upon their scope and design chemical libraries can also be classified as diverse oriented drug like lead like peptide mimetic natural product like targeted against a specific family of biological targets such kinases gpcrs proteases ppi etc among the compound libraries should be annotated the fragment compound libraries which are mainly used for fragment based drug discovery fbdd
design and optimization of chemical libraries
chemical libraries are usually designed by chemists and chemoinformatics scientists and synthesized by organic chemistry and medicinal chemistry the method of chemical library generation usually depends on the project and there are many factors to consider when using rational methods to select screening compounds typically a range of chemicals is screened against a particular drug target or disease model and the preliminary hits or chemicals that show the desired activity are re screened to verify their activity once they are qualified as a hit by their repeatability and activity these particular chemicals are registered and analysed commonalities among the different chemical groups are studied as they are often reflective of a particular chemical subspace additional chemistry work may be needed to further optimize the chemical library in the active portion of the subspace when it is needed more synthesis is completed to extend out the chemical library in that particular subspace by generating more compounds that are very similar to the original hits this new selection of compounds within this narrow range are further screened and then taken on to more sophisticated models for further validation in the drug discovery hit to lead process
storage and management
the chemical space of all possible organic chemicals is large and increases exponentially with the size of the molecule most chemical libraries do not typically have a fully represented chemical space mostly because of storage and cost concerns
because of the expense and effort involved in chemical synthesis the chemicals must be correctly stored and banked away for later use to prevent early degradation each chemical has a particular shelf life and storage requirement and in a good sized chemical library there is a timetable by which library chemicals are disposed of and replaced on a regular basis some chemicals are fairly unstable radioactive volatile or flammable and must be stored under careful conditions in accordance with safety standards such as osha
most chemical libraries are managed with information technologies such as barcoding and relational databases additionally robotics are necessary to fetch compounds in larger chemical libraries
because a chemical library's individual entries can easily reach up into the millions of compounds the management of even modest sized chemical libraries can be a full time endeavor compound management is one such field that attempts to manage and upkeep these chemical libraries as well as maximizing safety and effectiveness in their management

cheminformatics

cheminformatics also known as chemoinformatics chemioinformatics and chemical informatics is the use of computer and informational techniques applied to a range of problems in the field of chemistry these in silico techniques are used in for example pharmaceutical companies in the process of drug discovery these methods can also be used in chemical and allied industries in various other forms
history
the term chemoinformatics was defined by f k brown in
chemoinformatics is the mixing of those information resources to transform data into information and information into knowledge for the intended purpose of making better decisions faster in the area of drug lead identification and optimization
since then both spellings have been used and some have evolved to be established as cheminformatics while european academia settled in for chemoinformatics the recent establishment of the journal of cheminformatics is a strong push towards the shorter variant
basics
cheminformatics combines the scientific working fields of chemistry computer science and information science for example in the areas of topology chemical graph theory information retrieval and data mining in the chemical space cheminformatics can also be applied to data analysis for various industries like paper and pulp dyes and such allied industries
applications
storage and retrieval
the primary application of cheminformatics is in the storage indexing and search of information relating to compounds the efficient search of such stored information includes topics that are dealt with in computer science as data mining information retrieval information extraction and machine learning related research topics include
file formats
the in silico representation of chemical structures uses specialized formats such as the xml based chemical markup language or smiles these representations are often used for storage in large chemical databases while some formats are suited for visual representations in or dimensions others are more suited for studying physical interactions modeling and docking studies
virtual libraries
chemical data can pertain to real or virtual molecules virtual libraries of compounds
may be generated in various ways to explore chemical space and hypothesize novel
compounds with desired properties
virtual libraries of classes of compounds drugs natural products diversity oriented synthetic products were recently generated using the fog fragment optimized growth algorithm
this was done by using cheminformatic tools to train transition probabilities of a markov chain on authentic classes of compounds and then using the markov chain to generate novel compounds that were similar to the training database
virtual screening
in contrast to high throughput screening virtual screening involves computationally
screening in silico libraries of compounds by means of various methods such as
docking to identify members likely to possess desired properties
such as biological activity against a given target in some cases combinatorial chemistry is used in the development of the library to increase the efficiency in mining the chemical space more commonly a diverse library of small molecules or natural products is screened
quantitative structure activity relationship qsar
this is the calculation of quantitative structure activity relationship and quantitative structure property relationship values used to predict the activity of compounds from their structures in this context there is also a strong relationship to chemometrics chemical expert systems are also relevant since they represent parts of chemical knowledge as an in silico representation there is a relatively new concept of matched molecular pair analysis or predcition driven mmpa which is coupled with qsar model in order to identify activity cliff

mathematical chemistry

mathematical chemistry is the area of research engaged in novel applications of mathematics to chemistry it concerns itself principally with the mathematical modeling of chemical phenomena mathematical chemistry has also sometimes been called computer chemistry but should not be confused with computational chemistry
major areas of research in mathematical chemistry include chemical graph theory which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure property relationships and chemical aspects of group theory which finds applications in stereochemistry and quantum chemistry
the history of the approach may be traced back to the th century georg helm published a treatise titled the principles of mathematical chemistry the energetics of chemical phenomena in some of the more contemporary periodical publications specializing in the field are match communications in mathematical and in computer chemistry first published in and the journal of mathematical chemistry first published in in a series of annual conferences math chem comp taking place in dubrovnik was
initiated by the late ante graovac
the basic models for mathematical chemistry are molecular graph and topological index
in the international academy of mathematical chemistry iamc was founded in dubrovnik croatia by milan randi the academy members are from all over the world comprising six scientists awarded with a nobel prize

radiochemistry

radiochemistry is the chemistry of radioactive materials where radioactive isotopes of elements are used to study the properties and chemical reactions of non radioactive isotopes often within radiochemistry the absence of radioactivity leads to a substance being described as being inactive as the isotopes are stable much of radiochemistry deals with the use of radioactivity to study ordinary chemical reactions this is very different from radiation chemistry where the radiation levels are kept too low to influence the chemistry
radiochemistry includes the study of both natural and man made radioisotopes
main decay modes
all radioisotopes are unstable isotopes of elements undergo nuclear decay and emit some form of radiation the radiation emitted can be one of three types called alpha beta or gamma radiation
alpha radiation the emission of an alpha particle which contains protons and neutrons from an atomic nucleus when this occurs the atom's atomic mass will decrease by units and atomic number will decrease by
beta radiation the transmutation of a neutron into an electron and a proton after this happens the electron is emitted from the nucleus into the electron cloud
gamma radiation the emission of electromagnetic energy such as x rays from the nucleus of an atom this usually occurs during alpha or beta radioactive decay
these three types of radiation can be distinguished by their difference in penetrating power
alpha can be stopped quite easily by a few centimetres in air or a piece of paper and is equivalent to a helium nucleus beta can be cut off by an aluminium sheet just a few millimetres thick and are electrons gamma is the most penetrating of the three and is a massless chargeless high energy photon gamma radiation requires an appreciable amount of heavy metal radiation shielding usually lead or barium based to reduce its intensity
activation analysis
by neutron irradiation of objects it is possible to induce radioactivity this activation of stable isotopes to create radioisotopes is the basis of neutron activation analysis one of the most interesting objects which has been studied in this way is the hair of napoleon's head which have been examined for their arsenic content
a series of different experimental methods exist these have been designed to enable the measurement of a range of different elements in different matrices to reduce the effect of the matrix it is common to use the chemical extraction of the wanted element and or to allow the radioactivity due to the matrix elements to decay before the measurement of the radioactivity since the matrix effect can be corrected for by observing the decay spectrum little or no sample preparation is required for some samples making neutron activation analysis less susceptible to contamination
the effects of a series of different cooling times can be seen if a hypothetical sample which contains sodium uranium and cobalt in a ratio was subjected to a very short pulse of thermal neutrons the initial radioactivity would be dominated by the na activity half life h but with increasing time the np half life d after formation from parent u with half life min and finally the co activity yr would predominate
biology application
one biological application is the study of dna using radioactive phosphorus in these experiments stable phosphorus is replaced by the chemical identical radioactive p and the resulting radioactivity is used in analysis of the molecules and their behaviour
another example is the work which was done on the methylation of elements such as sulfur selenium tellurium and polonium by living organisms it has been shown that bacteria can convert these elements into volatile compounds it is thought that methylcobalamin vitamin b alkylates these elements to create the dimethyls it has been shown that a combination of cobaloxime and inorganic polonium in sterile water forms a volatile polonium compound while a control experiment which did not contain the cobalt compound did not form the volatile polonium compound for the sulfur work the isotope s was used while for polonium po was used in some related work by the addition of co to the bacterial culture followed by isolation of the cobalamin from the bacteria and the measurement of the radioactivity of the isolated cobalamin it was shown that the bacteria convert available cobalt into methylcobalamin
environmental
radiochemistry also includes the study of the behaviour of radioisotopes in the environment for instance a forest or grass fire can make radioisotopes become mobile again in these experiments fires were started in the exclusion zone around chernobyl and the radioactivity in the air downwind was measured
it is important to note that a vast number of processes are able to release radioactivity into the environment for example the action of cosmic rays on the air is responsible for the formation of radioisotopes such as c and p the decay of ra forms rn which is a gas which can diffuse through rocks before entering buildings and dissolve in water and thus enter drinking water in addition human activities such as bomb tests accidents and normal releases from industry have resulted in the release of radioactivity
chemical form of the actinides
the environmental chemistry of some radioactive elements such as plutonium is complicated by the fact that solutions of this element can undergo disproportionation and as a result many different oxidation states can coexist at once some work has been done on the identification of the oxidation state and coordination number of plutonium and the other actinides under different conditions this includes work on both solutions of relatively simple complexes and work on colloids two of the key matrixes are soil rocks and concrete in these systems the chemical properties of plutonium have been studied using methods such as exafs and xanes
movement of colloids
while binding of a metal to the surfaces of the soil particles can prevent its movement through a layer of soil it is possible for the particles of soil which bear the radioactive metal can migrate as colloidal particles through soil this has been shown to occur using soil particles labeled with cs these have been shown to be able to move through cracks in the soil
normal background
radioactivity is present everywhere and has been since the formation of the earth according to the international atomic energy agency one kilogram of soil typically contains the following amounts of the following three natural radioisotopes bq k typical range bq bq ra typical range bq bq u typical range bq and bq th typical range bq
action of microorganisms
the action of micro organisms can fix uranium thermoanaerobacter can use chromium vi iron iii cobalt iii manganese iv and uranium vi as electron acceptors while acetate glucose hydrogen lactate pyruvate succinate and xylose can act as electron donors for the metabolism of the bacteria in this way the metals can be reduced to form magnetite fe o siderite feco rhodochrosite mnco and uraninite uo other researchers have also worked on the fixing of uranium using bacteria francis r livens et al working at manchester have suggested that the reason why geobacter sulfurreducens can reduce cations to uranium dioxide is that the bacteria reduce the uranyl cations to which then undergoes disproportionation to form and uo this reasoning was based at least in part on the observation that is not converted to an insoluble neptunium oxide by the bacteria

glossary of chemistry terms

this page is a glossary of chemistry terms chemistry has an extensive vocabulary and a significant amount of jargon this is a list of chemical terms including laboratory tools glassware and equipment chemistry itself is a physical science concerned with the composition structure and properties of matter as well as the changes it undergoes during chemical reactions
note all periodic table references refer to the iupac style of the periodic table

astrochemistry

astrochemistry is the study of the abundance and reactions of chemical elements and molecules in the universe and their interaction with radiation the discipline is an overlap of astronomy and chemistry the word astrochemistry may be applied to both the solar system and the interstellar medium the study of the abundance of elements and isotope ratios in solar system objects such as meteorites is also called cosmochemistry while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics the formation atomic and chemical composition evolution and fate of molecular gas clouds is of special interest because it is from these clouds that solar systems form
spectroscopy
one particularly important experimental tool in astrochemistry is spectroscopy the use of telescopes to measure the absorption and emission of light from molecules and atoms in various environments by comparing astronomical observations with laboratory measurements astrochemists can infer the elemental abundances chemical composition and temperatures of stars and interstellar clouds this is possible because ions atoms and molecules have characteristic spectra that is the absorption and emission of certain wavelengths colors of light often not visible to the human eye however these measurements have limitations with various types of radiation radio infrared visible ultraviolet etc able to detect only certain types of species depending on the chemical properties of the molecules interstellar formaldehyde was the first organic molecule detected in the interstellar medium
perhaps the most powerful technique for detection of individual chemical species is radio astronomy which has resulted in the detection of over a hundred interstellar species including radicals and ions and organic i e carbon based compounds such as alcohols acids aldehydes and ketones one of the most abundant interstellar molecules and among the easiest to detect with radio waves due to its strong electric dipole moment is co carbon monoxide in fact co is such a common interstellar molecule that it is used to map out molecular regions the radio observation of perhaps greatest human interest is the claim of interstellar glycine the simplest amino acid but with considerable accompanying controversy one of the reasons why this detection was controversial is that although radio and some other methods like rotational spectroscopy are good for the identification of simple species with large dipole moments they are less sensitive to more complex molecules even something relatively small like amino acids
moreover such methods are completely blind to molecules that have no dipole for example by far the most common molecule in the universe is h hydrogen gas but it does not have a dipole moment so it is invisible to radio telescopes moreover such methods cannot detect species that are not in the gas phase since dense molecular clouds are very cold k to c to f most molecules in them other than hydrogen are frozen i e solid instead hydrogen and these other molecules are detected using other wavelengths of light hydrogen is easily detected in the ultraviolet uv and visible ranges from its absorption and emission of light the hydrogen line moreover most organic compounds absorb and emit light in the infrared ir so for example the detection of methane in the atmosphere of mars was achieved using an ir ground based telescope nasa's meter infrared telescope facility atop mauna kea hawaii nasa also has an airborne ir telescope called sofia and an ir space telescope called spitzer somewhat related to the recent detection of methane in the atmosphere of mars scientists reported in june that measuring the ratio of hydrogen and methane levels on mars may help determine the likelihood of life on mars according to the scientists low h ch ratios less than approximately indicate that life is likely present and active other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres
infrared astronomy has also revealed that the interstellar medium contains a suite of complex gas phase carbon compounds called polyaromatic hydrocarbons often abbreviated pahs or pacs these molecules composed primarily of fused rings of carbon either neutral or in an ionized state are said to be the most common class of carbon compound in the galaxy they are also the most common class of carbon molecule in meteorites and in cometary and asteroidal dust cosmic dust these compounds as well as the amino acids nucleobases and many other compounds in meteorites carry deuterium and isotopes of carbon nitrogen and oxygen that are very rare on earth attesting to their extraterrestrial origin the pahs are thought to form in hot circumstellar environments around dying carbon rich red giant stars
infrared astronomy has also been used to assess the composition of solid materials in the interstellar medium including silicates kerogen like carbon rich solids and ices this is because unlike visible light which is scattered or absorbed by solid particles the ir radiation can pass through the microscopic interstellar particles but in the process there are absorptions at certain wavelengths that are characteristic of the composition of the grains as above with radio astronomy there are certain limitations e g n is difficult to detect by either ir or radio astronomy
such ir observations have determined that in dense clouds where there are enough particles to attenuate the destructive uv radiation thin ice layers coat the microscopic particles permitting some low temperature chemistry to occur since hydrogen is by far the most abundant molecule in the universe the initial chemistry of these ices is determined by the chemistry of the hydrogen if the hydrogen is atomic then the h atoms react with available o c and n atoms producing reduced species like h o ch and nh however if the hydrogen is molecular and thus not reactive this permits the heavier atoms to react or remain bonded together producing co co cn etc these mixed molecular ices are exposed to ultraviolet radiation and cosmic rays which results in complex radiation driven chemistry lab experiments on the photochemistry of simple interstellar ices have produced amino acids the similarity between interstellar and cometary ices as well as comparisons of gas phase compounds have been invoked as indicators of a connection between interstellar and cometary chemistry this is somewhat supported by the results of the analysis of the organics from the comet samples returned by the stardust mission but the minerals also indicated a surprising contribution from high temperature chemistry in the solar nebula
research
research is progressing on the way in which interstellar and circumstellar molecules form and interact and this research could have a profound impact on our understanding of the suite of molecules that were present in the molecular cloud when our solar system formed which contributed to the rich carbon chemistry of comets and asteroids and hence the meteorites and interstellar dust particles which fall to the earth by the ton every day
the sparseness of interstellar and interplanetary space results in some unusual chemistry since symmetry forbidden reactions cannot occur except on the longest of timescales for this reason molecules and molecular ions which are unstable on earth can be highly abundant in space for example the h ion astrochemistry overlaps with astrophysics and nuclear physics in characterizing the nuclear reactions which occur in stars the consequences for stellar evolution as well as stellar generations indeed the nuclear reactions in stars produce every naturally occurring chemical element as the stellar generations advance the mass of the newly formed elements increases a first generation star uses elemental hydrogen h as a fuel source and produces helium he hydrogen is the most abundant element and it is the basic building block for all other elements as its nucleus has only one proton gravitational pull toward the center of a star creates massive amounts of heat and pressure which cause nuclear fusion through this process of merging nuclear mass heavier elements are formed carbon oxygen and silicon are examples of elements that form in stellar fusion after many stellar generations very heavy elements are formed e g iron and lead
in october scientists reported that cosmic dust contains complex organic matter amorphous organic solids with a mixed aromatic aliphatic structure that could be created naturally and rapidly by stars
on august and in a world first astronomers at copenhagen university reported the detection of a specific sugar molecule glycolaldehyde in a distant star system the molecule was found around the protostellar binary iras which is located from earth glycolaldehyde is needed to form ribonucleic acid or rna which is similar in function to dna this finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets eventually arriving on young planets early in their formation
in september nasa scientists reported that polycyclic aromatic hydrocarbons pahs subjected to interstellar medium ism conditions are transformed through hydrogenation oxygenation and hydroxylation to more complex organics a step along the path toward amino acids and nucleotides the raw materials of proteins and dna respectively further as a result of these transformations the pahs lose their spectroscopic signature which could be one of the reasons for the lack of pah detection in interstellar ice grains particularly the outer regions of cold dense clouds or the upper molecular layers of protoplanetary disks
in february nasa announced the creation of an improved spectral database for tracking polycyclic aromatic hydrocarbons pahs in the universe according to scientists more than of the carbon in the universe may be associated with pahs possible starting materials for the formation of life pahs seem to have been formed shortly after the big bang are widespread throughout the universe and are associated with new stars and exoplanets
on august astronomers released studies using the atacama large millimeter submillimeter array alma for the first time that detailed the distribution of hcn hnc h co and dust inside the comae of comets c f lemmon and c s ison
for the study of the recourses of chemical elements and molecules in the universe is developed the mathematical model of the molecules composition distribution in the interstellar environment on thermodynamic potentials by professor m yu dolomatov using methods of the probability theory the mathematical and physical statistics and the equilibrium thermodynamics based on this model are estimated the resources of life related molecules amino acids and the nitrogenous bases in the interstellar medium the possibility of the oil hydrocarbons molecules formation is shown the given calculations confirm sokolov s and hoyl s hypotheses about the possibility of the oil hydrocarbons formation in space results are confirmed by data of astrophysical supervision and space researches

stereochemistry

stereochemistry a subdiscipline of chemistry involves the study of the relative spatial arrangement of atoms that form the structure of molecules and their manipulation an important branch of stereochemistry is the study of chiral molecules
stereochemistry is also known as d chemistry because the prefix stereo means three dimensionality
the study of stereochemistry focuses on stereoisomers and spans the entire spectrum of organic inorganic biological physical and especially supramolecular chemistry stereochemistry includes methods for determining and describing these relationships the effect on the physical or biological properties these relationships impart upon the molecules in question and the manner in which these relationships influence the reactivity of the molecules in question dynamic stereochemistry
history
louis pasteur could rightly be described as the first stereochemist having observed in that salts of tartaric acid collected from wine production vessels could rotate plane polarized light but that salts from other sources did not this property the only physical property in which the two types of tartrate salts differed is due to optical isomerism in jacobus henricus van t hoff and joseph le bel explained optical activity in terms of the tetrahedral arrangement of the atoms bound to carbon
significance
cahn ingold prelog priority rules are part of a system for describing a molecule's stereochemistry they rank the atoms around a stereocenter in a standard way allowing the relative position of these atoms in the molecule to be described unambiguously a fischer projection is a simplified way to depict the stereochemistry around a stereocenter
thalidomide example
an often cited example of the importance of stereochemistry relates to the thalidomide disaster thalidomide is a pharmaceutical drug first prepared in in germany prescribed for treating morning sickness in pregnant women the drug was discovered to be teratogenic causing serious genetic damage to early embryonic growth and development leading to limb deformation in babies some of the several proposed mechanisms of teratogenecity involve a different biological function for the r and the s thalidomide enantiomers in the human body however thalidomide undergoes racemization even if only one of the two enantiomers is administered as a drug the other enantiomer is produced as a result of metabolism accordingly it is incorrect to state that one of the stereoisomer is safe while the other is teratogenic thalidomide is currently used for the treatment of other diseases notably cancer and leprosy strict regulations and controls have been enabled to avoid its use by pregnant women and prevent developmental deformations this disaster was a driving force behind requiring strict testing of drugs before making them available to the public
definitions
many definitions that describe a specific conformer iupac gold book exist developed by william klyne and vladimir prelog constituting their klyne prelog system of nomenclature
torsional strain results from resistance to twisting about a bond

amateur chemistry

amateur chemistry or home chemistry is the pursuit of chemistry as a private hobby amateur chemistry is usually done with whatever chemicals are available at disposal at the privacy of one's home it should not be confused with clandestine chemistry which involves the illicit production of controlled drugs notable amateur chemists include oliver sacks and sir edward elgar
history
origins
amateur chemistry shares its early history with that of chemistry in general pioneers of modern chemistry such as robert boyle and antoine lavoisier were gentleman scientists who pursued their research independently from their source of income only with the coming of the industrial era and the rise of universities as research institutions did any significant distinction between amateurs and professionals emerge
nevertheless amateur progress lasted well into the th century for example in charles martin hall co invented the hall h roult process for extracting aluminium from its oxide whilst working in a woodshed behind his family home
the history of amateur chemistry ties in well with that of chemistry in general the history of chemistry represents a time span from ancient history to the present by bc civilizations used technologies that would eventually form the basis to the various branches of chemistry these processes include extracting metals from ores making pottery and glazes fermenting beer and wine extracting chemicals from plants for medicine and perfume rendering fat into soap making glass and making alloys like bronze
chemistry as a hobby
throughout much of the th century amateur chemistry was an unexceptional hobby with high quality chemistry sets readily available and laboratory suppliers freely selling to hobbyists for example linus pauling had no difficulty in procuring potassium cyanide at the age of eleven however due to increasing concerns about terrorism drugs and safety suppliers became increasingly reluctant to sell to amateurs and chemistry sets were steadily toned down this trend has gradually continued leaving hobbyists in many parts of the world without access to most reagents
restrictions
whilst the hobby is probably legal in most jurisdictions the relationship between amateur chemists and law enforcement agencies is often fraught hobbyists are often affected by laws intended to fight drugs and terrorism furthermore many chemical supply houses refuse to sell to amateurs with such policies sometimes being stated openly
canada
in canada a wide range of basic laboratory reagents such as nitric acid and hydrogen peroxide are restricted as explosives precursors
germany
german amateur chemists have been raided by the police despite not being in the possession of illegal chemicals
united states
in the united states some regions have stringent regulations concerning the ownership of chemicals and equipment for example texas requires the registration of even the most basic laboratory glassware
united nuclear an amateur science supplier based in new mexico was raided on behest of the u s consumer product safety commission and subsequently fined for selling illegal fireworks components
the united states drug enforcement administration maintains lists regarding the classification of illicit drugs which contain chemicals that are used to manufacture the controlled substances illicit drugs the lists are designated within the controlled substances act paragraphs list i and list ii

crystal chemistry

crystal chemistry is the study of the principles of chemistry behind crystals and their use in describing structure property relations in solids the principles that govern the assembly of crystal and glass structures are described models of many of the technologically important crystal structures zinc blende alumina quartz perovskite are studied and the effect of crystal structure on the various fundamental mechanisms responsible for many physical properties are discussed
the objectives of the field include
topics studied are

geochemistry


geochemistry is the science that uses the tools and principles of chemistry to explain the mechanisms behind major geological systems such as the earth's crust and its oceans the realm of geochemistry extends beyond the earth encompassing the entire solar system and has made important contributions to the understanding of a number of processes including mantle convection the formation of planets and the origins of granite and basalt
history
the term geochemistry was first used by the swiss german chemist christian friedrich sch nbein in in his paper sch nbein predicted the birth of a new field of study stating
in a word a comparative geochemistry ought to be launched before geochemistry can become geology and before the mystery of the genesis of our planets and their inorganic matter may be revealed
the field began to be realised a short time after sch nbein's work but his term geochemistry was initially used neither by geologists nor chemists and there was much debate over which of the two sciences should be the dominant partner there was little collaboration between geologists and chemists and the field of geochemistry remained small and unrecognised in the late th century a swiss man by the name of victor goldschmidt was born who later became known as the father of geochemistry his paper geochemische verteilungsgesetze der elemente on the distribution of elements in nature has been referred to as the start of geochemistry during the early th century a number of geochemists produced work that began to popularise the field including frank wigglesworth clarke who had begun to investigate the abundances of various elements within the earth and how the quantities were related to atomic weight the composition of meteorites and their differences to terrestrial rocks was being investigated as early as and in oliver c farrington hypothesised although there were differences that the relative abundances should still be the same this was the beginnings of the field of cosmochemistry and has contributed much of what we know about the formation of the earth and the solar system
subfields
some subsets of geochemistry are
victor goldschmidt is considered by most to be the father of modern geochemistry and the ideas of the subject were formed by him in a series of publications from under the title geochemische verteilungsgesetze der elemente geochemical laws of distribution of the elements
chemical characteristics
the more common rock constituents are nearly all oxides chlorides sulfides and fluorides are the only important exceptions to this and their total amount in any rock is usually much less than f w clarke has calculated that a little more than of the earth's crust consists of oxygen it occurs principally in combination as oxides of which the chief are silica alumina iron oxides and various carbonates calcium carbonate magnesium carbonate sodium carbonate and potassium carbonate the silica functions principally as an acid forming silicates and all the commonest minerals of igneous rocks are of this nature from a computation based on analyses of numerous kinds of rocks clarke arrived at the following as the average percentage composition of the earths crust sio al o fe o feo mgo cao na o k o h o tio p o total all the other constituents occur only in very small quantities usually much less than
these oxides combine in a haphazard way for example potash potassium carbonate and soda sodium carbonate combine to produce feldspars in some cases they may take other forms such as nepheline leucite and muscovite but in the great majority of instances they are found as feldspar phosphoric acid with lime calcium carbonate forms apatite titanium dioxide with ferrous oxide gives rise to ilmenite part of the lime forms lime feldspar magnesium carbonate and iron oxides with silica crystallize as olivine or enstatite or with alumina and lime form the complex ferro magnesian silicates of which the pyroxenes amphiboles and biotites are the chief any excess of silica above what is required to neutralize the bases will separate out as quartz excess of alumina crystallizes as corundum these must be regarded only as general tendencies it is possible by rock analysis to say approximately what minerals the rock contains but there are numerous exceptions to any rule
mineral constitution
hence we may say that except in acid or siliceous rocks containing greater than of silica are known as felsic rocks and quartz is not abundant in basic rocks containing of silica or less it is rare for them to contain as much silicon these are referred to as mafic rocks if magnesium and iron are above average while silica is low olivine may be expected where silica is present in greater quantity over ferro magnesian minerals such as augite hornblende enstatite or biotite occur rather than olivine unless potash is high and silica relatively low leucite will not be present for leucite does not occur with free quartz nepheline likewise is usually found in rocks with much soda and comparatively little silica with high alkalis soda bearing pyroxenes and amphiboles may be present the lower the percentage of silica and alkali's the greater is the prevalence of plagioclase feldspar as contracted with soda or potash feldspar the earth crust is composed of silicate minerals and their abundance in the earth is as follows plagioclase feldspar alkali feldspar quartz pyroxene amphiboles micas clay minerals after this the remaining silicate minerals make up another of the earths crust only of the earth is composed of non silicate minerals such as carbonate oxides and sulfides
the other determining factor namely the physical conditions attending consolidation plays on the whole a smaller part yet is by no means negligible as a few instances will prove certain minerals are practically confined to deep seated intrusive rocks e g microcline muscovite diallage leucite is very rare in plutonic masses many minerals have special peculiarities in microscopic character according to whether they crystallized in depth or near the surface e g hypersthene orthoclase quartz there are some curious instances of rocks having the same chemical composition but consisting of entirely different minerals e g the hornblendite of gran in norway which contains only hornblende has the same composition as some of the camptonites of the same locality that contain feldspar and hornblende of a different variety in this connection we may repeat what has been said above about the corrosion of porphyritic minerals in igneous rocks in rhyolites and trachytes early crystals of hornblende and biotite may be found in great numbers partially converted into augite and magnetite hornblende and biotite were stable under the pressures and other conditions below the surface but unstable at higher levels in the ground mass of these rocks augite is almost universally present but the plutonic representatives of the same magma granite and syenite contain biotite and hornblende far more commonly than augite
felsic intermediate and mafic igneous rocks
those rocks that contain the most silica and on crystallizing yield free quartz form a group generally designated the felsic rocks those again that contain least silica and most magnesia and iron so that quartz is absent while olivine is usually abundant form the mafic group the intermediate rocks include those characterized by the general absence of both quartz and olivine an important subdivision of these contains a very high percentage of alkalis especially soda and consequently has minerals such as nepheline and leucite not common in other rocks it is often separated from the others as the alkali or soda rocks and there is a corresponding series of mafic rocks lastly a small sub group rich in olivine and without feldspar has been called the ultramafic rocks they have very low percentages of silica but much iron and magnesia
except these last practically all rocks contain felspars or feldspathoid minerals in the acid rocks the common feldspars are orthoclase perthite microcline and oligoclase all having much silica and alkalis in the mafic rocks labradorite anorthite and bytownite prevail being rich in lime and poor in silica potash and soda augite is the most common ferro magnesian in mafic rocks but biotite and hornblende are on the whole more frequent in felsic rocks
rocks that contain leucite or nepheline either partly or a wholly replacing felspar are not included in this table they are essentially of intermediate or of mafic character we might in consequence regard them as varieties of syenite diorite gabbro etc in which feldspathoid minerals occur and indeed there are many transitions between syenites of ordinary type and nepheline or leucite syenite and between gabbro or dolerite and theralite or essexite but as many minerals develop in these alkali rocks that are uncommon elsewhere it is convenient in a purely formal classification like that outlined here to treat the whole assemblage as a distinct series
this classification is based essentially on the mineralogical constitution of the igneous rocks any chemical distinctions between the different groups though implied are relegated to a subordinate position it is admittedly artificial but it has grown up with the growth of the science and is still adopted as the basis on which more minute subdivisions are erected the subdivisions are by no means of equal value the syenites for example and the peridotites are far less important than the granites diorites and gabbros moreover the effusive andesites do not always correspond to the plutonic diorites but partly also to the gabbros as the different kinds of rock regarded as aggregates of minerals pass gradually into one another transitional types are very common and are often so important as to receive special names the quartz syenites and nordmarkites may be interposed between granite and syenite the tonalites and adamellites between granite and diorite the monzoaites between syenite and diorite norites and hyperites between diorite and gabbro and so on
geochemistry of trace metals in the ocean
trace metals readily form complexes with major ions in the ocean including hydroxide and chloride and their chemical speciation changes depending on whether the environment is oxidized or reduced benjamin defines complexes of metals with more than one type of ligand other than water as mixed ligand complexes in some cases a ligand contains more than one donor atom forming very strong complexes also called chelates the ligand is the chelator one of the most common chelators is edta ethylenediaminetetraacetic acid which can replace six molecules of water and form strong bonds with metals that have a plus two charge with stronger complexation lower activity of the free metal ion is observed one consequence of the lower reactivity of complexed metals compared to the same concentration of free metal is that the chelation tends to stabilize metals in the aqueous solution instead of in solids
concentrations of the trace metals cadmium copper molybdenum manganese rhenium uranium and vanadium in sediments record the redox history of the oceans within aquatic environments cadmium ii can either be in the form cdcl aq in oxic waters or cds s in a reduced environment thus higher concentrations of cd in marine sediments may indicate low redox potential conditions in the past for copper ii a prevalent form is cucl aq within oxic environments and cus s and cu s within reduced environments the reduced seawater environment leads to two possible oxidation states of copper cu i and cu ii molybdenum is present as the mo vi oxidation state as moo aq in oxic environments mo v and mo iv are present in reduced environments in the forms moo aq and mos s rhenium is present as the re vii oxidation state as reo within oxic conditions but is reduced to re iv which may form reo or res uranium is in oxidation state vi in uo co aq and is found in the reduced form uo s vanadium is in several forms in oxidation state v v hvo and h vo its reduced forms can include vo vo oh and v oh these relative dominance of these species depends on ph
in the water column of the ocean or deep lakes vertical profiles of dissolved trace metals are characterized as following conservative type nutrient type or scavenged type distributions across these three distributions trace metals have different residence times and are used to varying extents by planktonic microorganisms trace metals with conservative type distributions have high concentrations relative to their biological use one example of a trace metal with a conservative type distribution is molybdenum it has a residence time within the oceans of around x years and is generally present as the molybdate anion moo molybdenum interacts weakly with particles and displays an almost uniform vertical profile in the ocean relative to the abundance of molybdenum in the ocean the amount required as a metal cofactor for enzymes in marine phytoplankton is negligible
trace metals with nutrient type distributions are strongly associated with the internal cycles of particulate organic matter especially the assimilation by plankton the lowest dissolved concentrations of these metals are at the surface of the ocean where they are assimilated by plankton as dissolution and decomposition occur at greater depths concentrations of these trace metals increase residence times of these metals such as zinc are several thousand to one hundred thousand years finally an example of a scavenged type trace metal is aluminium which has strong interactions with particles as well as a short residence time in the ocean the residence times of scavenged type trace metals are around to years the concentrations of these metals are highest around bottom sediments hydrothermal vents and rivers for aluminium atmospheric dust provides the greatest source of external inputs into the ocean
iron and copper show hybrid distributions in the ocean they are influenced by recycling and intense scavenging iron is a limiting nutrient in vast areas of the oceans and is found in high abundance along with manganese near hydrothermal vents here many iron precipitates are found mostly in the forms of iron sulfides and oxidized iron oxyhydroxide compounds concentrations of iron near hydrothermal vents can be up to one million times the concentrations found in the open ocean
using electrochemical techniques it is possible to show that bioactive trace metals zinc cobalt cadmium iron and copper are bound by organic ligands in surface seawater these ligand complexes serve to lower the bioavailability of trace metals within the ocean for example copper which may be toxic to open ocean phytoplankton and bacteria can form organic complexes the formation of these complexes reduces the concentrations of bioavailable inorganic complexes of copper that could be toxic to sea life at high concentrations unlike copper zinc toxicity in marine phytoplankton is low and there is no advantage to increasing the organic binding of zn in high nutrient low chlorophyll regions iron is the limiting nutrient with the dominant species being from fe iii binding with organic complexations

nuclear chemistry

nuclear chemistry is the subfield of chemistry dealing with radioactivity nuclear processes such as nuclear transmutation and nuclear properties
it is the chemistry of radioactive elements such as the actinides radium and radon together with the chemistry associated with equipment such as nuclear reactors which are designed to perform nuclear processes this includes the corrosion of surfaces and the behavior under conditions of both normal and abnormal operation such as during an accident an important area is the behavior of objects and materials after being placed into a nuclear waste storage or disposal site
it includes the study of the chemical effects resulting from the absorption of radiation within living animals plants and other materials the radiation chemistry controls much of radiation biology as radiation has an effect on living things at the molecular scale to explain it another way the radiation alters the biochemicals within an organism the alteration of the biomolecules then changes the chemistry which occurs within the organism this change in chemistry then can lead to a biological outcome as a result nuclear chemistry greatly assists the understanding of medical treatments such as cancer radiotherapy and has enabled these treatments to improve
it includes the study of the production and use of radioactive sources for a range of processes these include radiotherapy in medical applications the use of radioactive tracers within industry science and the environment and the use of radiation to modify materials such as polymers
it also includes the study and use of nuclear processes in non radioactive areas of human activity for instance nuclear magnetic resonance nmr spectroscopy is commonly used in synthetic organic chemistry and physical chemistry and for structural analysis in macromolecular chemistry
history
after the discovery of x rays by wilhelm r ntgen many scientists began to work on ionizing radiation one of these was henri becquerel who investigated the relationship between phosphorescence and the blackening of photographic plates when becquerel working in france discovered that with no external source of energy the uranium generated rays which could blacken or fog the photographic plate radioactivity was discovered marie curie working in paris and her husband pierre curie isolated two new radioactive elements from uranium ore they used radiometric methods to identify which stream the radioactivity was in after each chemical separation they separated the uranium ore into each of the different chemical elements that were known at the time and measured the radioactivity of each fraction they then attempted to separate these radioactive fractions further to isolate a smaller fraction with a higher specific activity radioactivity divided by mass in this way they isolated polonium and radium it was noticed in about that high doses of radiation could cause an injury in humans henri becquerel had carried a sample of radium in his pocket and as a result he suffered a high localised dose which resulted in a radiation burn this injury resulted in the biological properties of radiation being investigated which in time resulted in the development of medical treatments
ernest rutherford working in canada and england showed that radioactive decay can be described by a simple equation a linear first degree derivative equation now called first order kinetics implying that a given radioactive substance has a characteristic half life the time taken for the amount of radioactivity present in a source to diminish by half he also coined the terms alpha beta and gamma rays he converted nitrogen into oxygen and most importantly he supervised the students who did the geiger marsden experiment gold leaf experiment which showed that the plum pudding model of the atom was wrong in the plum pudding model proposed by j j thomson in the atom is composed of electrons surrounded by a cloud of positive charge to balance the electrons negative charge to rutherford the gold foil experiment implied that the positive charge was confined to a very small nucleus leading first to the rutherford model and eventually to the bohr model of the atom where the positive nucleus is surrounded by the negative electrons
in marie curie's daughter ir ne joliot curie and her husband were the first to create artificial radioactivity they bombarded boron with alpha particles to make the neutron poor isotope nitrogen this isotope emitted positrons in addition they bombarded aluminium and magnesium with neutrons to make new radioisotopes
main areas
radiochemistry is the chemistry of radioactive materials where radioactive isotopes of elements are used to study the properties and chemical reactions of non radioactive isotopes often within radiochemistry the absence of radioactivity leads to a substance being described as being inactive as the isotopes are stable
for further details please see the page on radiochemistry
radiation chemistry
radiation chemistry is the study of the chemical effects of radiation on matter this is very different from radiochemistry as no radioactivity needs to be present in the material which is being chemically changed by the radiation an example is the conversion of water into hydrogen gas and hydrogen peroxide
chemistry for nuclear power
radiochemistry radiation chemistry and nuclear chemical engineering play a very important role for uranium and thorium fuel precursors synthesis starting from ores of these elements fuel fabrication coolant chemistry fuel reprocessing radioactive waste treatment and storage monitoring of radioactive elements release during reactor operation and radioactive geological storage etc
study of nuclear reactions
a combination of radiochemistry and radiation chemistry is used to study nuclear reactions such as fission and fusion some early evidence for nuclear fission was the formation of a short lived radioisotope of barium which was isolated from neutron irradiated uranium ba with a half life of minutes and ba with a half life of days are major fission products of uranium at the time it was thought that this was a new radium isotope as it was then standard radiochemical practice to use a barium sulfate carrier precipitate to assist in the isolation of radium more recently a combination of radiochemical methods and nuclear physics has been used to try to make new superheavy elements it is thought that islands of relative stability exist where the nuclides have half lives of years thus enabling weighable amounts of the new elements to be isolated for more details of the original discovery of nuclear fission see the work of otto hahn
the nuclear fuel cycle
this is the chemistry associated with any part of the nuclear fuel cycle including nuclear reprocessing the fuel cycle includes all the operations involved in producing fuel from mining ore processing and enrichment to fuel production front end of the cycle it also includes the in pile behaviour use of the fuel in a reactor before the back end of the cycle the back end includes the management of the used nuclear fuel in either a spent fuel pool or dry storage before it is disposed of into an underground waste store or reprocessed
normal and abnormal conditions
the nuclear chemistry associated with the nuclear fuel cycle can be divided into two main areas one area is concerned with operation under the intended conditions while the other area is concerned with maloperation conditions where some alteration from the normal operating conditions has occurred or more rarely an accident is occurring
reprocessing
law
in the united states it is normal to use fuel once in a power reactor before placing it in a waste store the long term plan is currently to place the used civilian reactor fuel in a deep store this non reprocessing policy was started in march because of concerns about nuclear weapons proliferation president jimmy carter issued a presidential directive which indefinitely suspended the commercial reprocessing and recycling of plutonium in the united states this directive was likely an attempt by the united states to lead other countries by example but many other nations continue to reprocess spent nuclear fuels the russian government under president vladimir putin repealed a law which had banned the import of used nuclear fuel which makes it possible for russians to offer a reprocessing service for clients outside russia similar to that offered by bnfl
purex chemistry
the current method of choice is to use the purex liquid liquid extraction process which uses a tributyl phosphate hydrocarbon mixture to extract both uranium and plutonium from nitric acid this extraction is of the nitrate salts and is classed as being of a solvation mechanism for example the extraction of plutonium by an extraction agent s in a nitrate medium occurs by the following reaction
pu aq no aq sorganic pu no s organic
a complex bond is formed between the metal cation the nitrates and the tributyl phosphate and a model compound of a dioxouranium vi complex with two nitrates and two triethyl phosphates has been characterised by x ray crystallography
when the nitric acid concentration is high the extraction into the organic phase is favoured and when the nitric acid concentration is low the extraction is reversed the organic phase is stripped of the metal it is normal to dissolve the used fuel in nitric acid after the removal of the insoluble matter the uranium and plutonium are extracted from the highly active liquor it is normal to then back extract the loaded organic phase to create a medium active liquor which contains mostly uranium and plutonium with only small traces of fission products this medium active aqueous mixture is then extracted again by tributyl phosphate hydrocarbon to form a new organic phase the metal bearing organic phase is then stripped of the metals to form an aqueous mixture of only uranium and plutonium the two stages of extraction are used to improve the purity of the actinide product the organic phase used for the first extraction will suffer a far greater dose of radiation the radiation can degrade the tributyl phosphate into dibutyl hydrogen phosphate the dibutyl hydrogen phosphate can act as an extraction agent for both the actinides and other metals such as ruthenium the dibutyl hydrogen phosphate can make the system behave in a more complex manner as it tends to extract metals by an ion exchange mechanism extraction favoured by low acid concentration to reduce the effect of the dibutyl hydrogen phosphate it is common for the used organic phase to be washed with sodium carbonate solution to remove the acidic degradation products of the tributyl phosphate
new methods being considered for future use
the purex process can be modified to make a urex ur anium ex traction process which could be used to save space inside high level nuclear waste disposal sites such as yucca mountain nuclear waste repository by removing the uranium which makes up the vast majority of the mass and volume of used fuel and recycling it as reprocessed uranium
the urex process is a purex process which has been modified to prevent the plutonium being extracted this can be done by adding a plutonium reductant before the first metal extraction step in the urex process of the uranium and of technetium are separated from each other and the other fission products and actinides the key is the addition of acetohydroxamic acid aha to the extraction and scrub sections of the process the addition of aha greatly diminishes the extractability of plutonium and neptunium providing greater proliferation resistance than with the plutonium extraction stage of the purex process
adding a second extraction agent octyl phenyl n n dibutyl carbamoylmethyl phosphine oxide cmpo in combination with tributylphosphate tbp the purex process can be turned into the truex tr ans u ranic ex traction process this is a process which was invented in the usa by argonne national laboratory and is designed to remove the transuranic metals am cm from waste the idea is that by lowering the alpha activity of the waste the majority of the waste can then be disposed of with greater ease in common with purex this process operates by a solvation mechanism
as an alternative to truex an extraction process using a malondiamide has been devised the diamex diam ide ex traction process has the advantage of avoiding the formation of organic waste which contains elements other than carbon hydrogen nitrogen and oxygen such an organic waste can be burned without the formation of acidic gases which could contribute to acid rain the diamex process is being worked on in europe by the french cea the process is sufficiently mature that an industrial plant could be constructed with the existing knowledge of the process in common with purex this process operates by a solvation mechanism
selective actinide extraction sanex as part of the management of minor actinides it has been proposed that the lanthanides and trivalent minor actinides should be removed from the purex raffinate by a process such as diamex or truex in order to allow the actinides such as americium to be either reused in industrial sources or used as fuel the lanthanides must be removed the lanthanides have large neutron cross sections and hence they would poison a neutron driven nuclear reaction to date the extraction system for the sanex process has not been defined but currently several different research groups are working towards a process for instance the french cea is working on a bis triaiznyl pyridine btp based process
other systems such as the dithiophosphinic acids are being worked on by some other workers
this is the universal ex traction process which was developed in russia and the czech republic it is a process designed to remove all of the most troublesome sr cs and minor actinides radioisotopes from the raffinates left after the extraction of uranium and plutonium from used nuclear fuel the chemistry is based upon the interaction of caesium and strontium with poly ethylene oxide poly ethylene glycol and a cobalt carborane anion known as chlorinated cobalt dicarbollide the actinides are extracted by cmpo and the diluent is a polar aromatic such as nitrobenzene other dilents such as meta nitrobenzotrifluoride and phenyl trifluoromethyl sulfone have been suggested as well
absorption of fission products on surfaces
another important area of nuclear chemistry is the study of how fission products interact with surfaces this is thought to control the rate of release and migration of fission products both from waste containers under normal conditions and from power reactors under accident conditions it is interesting to note that like chromate and molybdate the tco anion can react with steel surfaces to form a corrosion resistant layer in this way these metaloxo anions act as anodic corrosion inhibitors the formation of tco on steel surfaces is one effect which will retard the release of tc from nuclear waste drums and nuclear equipment which has been lost before decontamination e g submarine reactors lost at sea this tco layer renders the steel surface passive inhibiting the anodic corrosion reaction the radioactive nature of technetium makes this corrosion protection impractical in almost all situations it has also been shown that tco anions react to form a layer on the surface of activated carbon charcoal or aluminium a short review of the biochemical properties of a series of key long lived radioisotopes can be read on line
tc in nuclear waste may exist in chemical forms other than the tco anion these other forms have different chemical properties
similarly the release of iodine in a serious power reactor accident could be retarded by absorption on metal surfaces within the nuclear plant
spinout areas
some methods first developed within nuclear chemistry and physics have become so widely used within chemistry and other physical sciences that they may be best thought of as separate from normal nuclear chemistry for example the isotope effect is used so extensively to investigate chemical mechanisms and the use of cosmogenic isotopes and long lived unstable isotopes in geology that it is best to consider much of isotopic chemistry as separate from nuclear chemistry
kinetics use within mechanistic chemistry
the mechanisms of chemical reactions can be investigated by observing how the kinetics of a reaction is changed by making an isotopic modification of a substrate known as the kinetic isotope effect this is now a standard method in organic chemistry briefly replacing normal hydrogen protons by deuterium within a molecule causes the molecular vibrational frequency of x h for example c h n h and o h bonds to decrease which leads to a decrease in vibrational zero point energy this can lead to a decrease in the reaction rate if the rate determining step involves breaking a bond between hydrogen and another atom thus if the reaction changes in rate when protons are replaced by deuteriums it is reasonable to assume that the breaking of the bond to hydrogen is part of the step which determines the rate
uses within geology biology and forensic science
cosmogenic isotopes are formed by the interaction of cosmic rays with the nucleus of an atom these can be used for dating purposes and for use as natural tracers in addition by careful measurement of some ratios of stable isotopes it is possible to obtain new insights into the origin of bullets ages of ice samples ages of rocks and the diet of a person can be identified from a hair or other tissue sample see isotope geochemistry and isotopic signature for further details
biology
within living things isotopic labels both radioactive and nonradioactive can be used to probe how the complex web of reactions which makes up the metabolism of an organism converts one substance to another for instance a green plant uses light energy to convert water and carbon dioxide into glucose by photosynthesis if the oxygen in the water is labeled then the label appears in the oxygen gas formed by the plant and not in the glucose formed in the chloroplasts within the plant cells
for biochemical and physiological experiments and medical methods a number of specific isotopes have important applications
by organic synthesis it is possible to create a complex molecule with a radioactive label that can be confined to a small area of the molecule for short lived isotopes such as c very rapid synthetic methods have been developed to permit the rapid addition of the radioactive isotope to the molecule for instance a palladium catalysed carbonylation reaction in a microfluidic device has been used to rapidly form amides and it might be possible to use this method to form radioactive imaging agents for pet imaging
nuclear magnetic resonance nmr
nmr spectroscopy uses the net spin of nuclei in a substance upon energy absorption to identify molecules this has now become a standard spectroscopic tool within synthetic chemistry one major use of nmr is to determine the bond connectivity within an organic molecule
nmr imaging also uses the net spin of nuclei commonly protons for imaging this is widely used for diagnostic purposes in medicine and can provide detailed images of the inside of a person without inflicting any radiation upon them in a medical setting nmr is often known simply as magnetic resonance imaging as the word nuclear has negative connotations for many people
see also
important publications in nuclear chemistry

the central science

chemistry is often called the central science because of its role in connecting the physical sciences which include chemistry with the life sciences and applied sciences such as medicine and engineering the nature of this relationship is one of the main topics in the philosophy of chemistry and in scientometrics the phrase was popularized by its use in a textbook by theodore l brown and h eugene lemay titled chemistry the central science which was first published in with a thirteenth edition published in
the central role of chemistry can be seen in the systematic and hierarchical classification of the sciences by auguste comte in which each discipline provides a more general framework for the area it precedes mathematics astronomy physics chemistry physiology and medicine social sciences balaban and klein have more recently proposed a diagram showing partial ordering of sciences in which chemistry may be argued is the central science since it provides a significant degree of branching in forming these connections the lower field cannot be fully reduced to the higher ones it is recognized that the lower fields possess emergent ideas and concepts that do not exist in the higher fields of science
thus chemistry is built on an understanding of laws of physics that govern particles such as atoms protons electrons thermodynamics etc although it has been shown that it has not been fully reduced to quantum mechanics concepts such as the periodicity of the elements and chemical bonds in chemistry are emergent in that they are more than the underlying forces that are defined by physics
in the same way biology cannot be fully reduced to chemistry despite the fact that the machinery that is responsible for life is composed of molecules for instance the machinery of evolution may be described in terms of chemistry by the understanding that it is a mutation in the order of genetic base pairs in the dna of an organism however chemistry cannot fully describe the process since it does not contain concepts such as natural selection that are responsible for driving evolution chemistry is fundamental to biology since it provides a methodology for studying and understanding the molecules that compose cells
connections made by chemistry are formed through various sub disciplines that utilize concepts from multiple scientific disciplines chemistry and physics are both needed in the areas of physical chemistry nuclear chemistry and theoretical chemistry chemistry and biology intersect in the areas of biochemistry medicinal chemistry molecular biology chemical biology molecular genetics and immunochemistry chemistry and the earth sciences intersect in areas like geochemistry and hydrology

forensic chemistry

forensic chemistry is the application of chemistry to law enforcement or the failure of products or processes many different analytical methods may be used to reveal what chemical changes occurred during an incident and so help reconstruct the sequence of events forensic chemistry is unique among chemical sciences in that its research practice and presentation must meet the needs of both the scientific and the legal communities as such forensic chemistry research is applied and derivative by nature and design and it emphasizes metrology and validation
methods
one particularly useful method for the simultaneous separation identification and quantitation of one or more individual components of an unknown substance or mixture is the use of a gas chromatograph mass spectrometer gc ms a gc ms is actually two instruments that are attached together physically and together comprising one of the so called tandem or hyphenated techniques
the gas chromatograph gc is essentially a hot c temperature controlled oven holding a bent or coiled specially packed or coated glass column between one and a few dozen meters long a small volume typically a few microliters of a drug sample or other unknown substance that has been dissolved in an organic solvent such as chloroform or methanol is quickly injected into the hot column volatile components in the sample are vaporized by the heat of the oven and are forced toward the end of the column by the flow of an inert carrier gas typically helium the special chemical component s within the column bind to substances contained in the moving vaporized sample mixture with slightly different force as a result different substances eventually are eluted i e emerge from the end of the column in differing amounts of time which is known as the retention time the retention time of various components so eluted can then be compared to those of known standard molecules eluted using the same method column length polarity flow rate of carrier gas temperature program while this comparison provides presumptive identification of the presence of a particular compound of interest in the unknown sample in general the gc portion of the technique is used as a separation and quantitation tool not an identification tool
to provide positive identification of the sample components the column eluent is then fed into a mass spectrometer ms these highly complex instruments use one or more methods bombardment with electrons high heat electrical force to break apart molecules into ions these ions are separated by their mass commonly with the use of a quadrupole mass analyzer or quadrupole ion trap and detected by an electron multiplier this provides a distinctive fragmentation pattern which functions as a sort of fingerprint for each compound the resulting patterns are then compared to a reference sample for identification purposes
spectroscopy
another instrument used to aide in identification of compounds is the fourier transform infrared spectrophotometer ftir the sample is bombarded with infrared radiation polar bonds found in organic compounds have a natural frequency of vibration similar to the frequency of infrared radiation when the frequency of the infrared radiation matches the natural frequency of the bond the amplitude of the vibration increases and the infrared is absorbed the output of an infrared spectrophotometer charts the amount of light absorbed vs the wavelength typically with units of percent transmission and wavenumbers cm because both the frequency and the intensity of absorption are dependent on the type of bond a skilled chemist can determine the functional groups present by examining the infrared spectrum
as with the gcms the ftir spectrum can be compared to that of a known sample thus providing evidence for the identification of a compound spectroscopy can also help to identify materials used in failed products especially polymers additives and fillers samples can be taken by dissolution or by cutting a thin slice using a microtome from the specimen under examination surfaces can be examined using attenuated total reflectance spectroscopy and the method has also been adapted to the optical microscope with infra red microspectroscopy
ultraviolet visible near infrared spectroscopy is used to test for certain drugs of abuse uv visible nir microspectrophotometers are instruments able to measure the spectra of microscopic samples the uv visible nir microspectrophotometer is used to compare known and questioned samples of trace evidence such as fibers and paint chips they are also used in the analysis of inks and papers of questioned documents and to measure the color of microscopic glass fragments as these samples are not altered uv visible nir microspectroscopy is considered a non destructive technique
thermoplastics can be analysed using characterization techniques such as infra red spectroscopy ultraviolet visible spectroscopy nuclear magnetic resonance spectroscopy and an environmental scanning electron microscope failed samples can either be dissolved in a suitable solvent and examined directly uv ir and nmr spectroscopy or be a thin film cast from solvent or cut using microtomy from the solid product infra red spectroscopy is especially useful for assessing oxidation of polymers such as the polymer degradation caused by faulty injection moulding the spectrum shows the characteristic carbonyl group produced by oxidation of polypropylene which made the product brittle it was a critical part of a crutch and when it failed the user fell and injured herself very seriously the spectrum was obtained from a thin film cast from a solution of a sample of the plastic taken from the failed forearm crutch
sample integrity
forensic chemists usually perform their analytical work in a sterile laboratory decreasing the risk of sample contamination in order to prevent tampering forensic chemists must keep track of a chain of custody for each sample a chain of custody is a document that stays with the evidence at all times among other information contains signatures and identification of all the people involved in transport storage and analysis of the evidence
this makes it far more difficult for intentional tampering to occur it also acts as a detailed record of the location of the evidence at all times for record keeping purposes it increases the reliability of a forensic chemist's work and increases the strength of the evidence in court
a distinction is made between destructive and non destructive analytical methods destructive methods involve taking a sample from the object of interest and so injures the object most spectroscopic techniques fall into this category by contrast a non destructive method conserves the integrity of the object and is generally preferred by forensic examiners for example optical microscopy and microspectroscopy cannot injure the sample so they are considered non destructive techniques
luminol
a method frequently used in forensic chemistry is that employing luminol as preemptory test a derivative of phthalic acid which reacts with metal cations and hence to detect traces of blood the process involves mixing luminol with a polar solution dependent upon the method used to create the luminol base which is spread carefully in places where it is thought that there are remnants of blood after all other evidence has be collected due to its destructive properties
thus typically the iron shaped cation found in the heme group of hemoglobin reacts with luminol observing a blue luminescence of the reaction itself is carried out however due to the nature of luminol there are other metal ions that it can react with to produce false positives for this reason alone that is why it is used only to determine the possibility of blood being present
in this process the final product is the aminophthalate anion which is in an excited state upon returning to the ground state or basal releases energy in the form of light which is known as blue luminescence
the reaction described has a very slow cin tic in fact it is the iron in the heme group of hemoglobin which catalyzes the process another note to keep in mind when working with luminol is that the reagent is only viable for a maximum of ten minutes and the darker the room the better the test
examples
polymers for example can be attacked by aggressive chemicals and if under load then cracks will grow by the mechanism of stress corrosion cracking perhaps the oldest known example is the ozone cracking of rubbers where traces of ozone in the atmosphere attack double bonds in the chains of the materials elastomers with double bonds in their chains include natural rubber nitrile rubber and styrene butadiene rubber they are all highly susceptible to ozone attack and can cause problems like car fires from rubber fuel lines and tire blow outs nowadays anti ozonants are widely added to these polymers so the incidence of cracking has dropped however not all safety critical rubber products are protected and since it only takes a few parts per billion of ozone to start attack failures are still occurring
another highly reactive gas is chlorine which will attack susceptible polymers such as acetal resin and polybutylene pipework there have been many examples of such pipes and acetal fittings failing in properties in the usa as a result of chlorine induced cracking in essence the gas attacks sensitive parts of the chain molecules especially secondary tertiary or allylic carbon atoms oxidizing the chains and ultimately causing chain cleavage the root cause is traces of chlorine in the water supply added for its anti bacterial action attack occurring even at parts per million traces of the dissolved gas
most step growth polymers can suffer hydrolysis in the presence of water often a reaction catalysed by acid or alkali nylon for example will degrade and crack rapidly if exposed to strong acids a phenomenon well known to those who accidentally spill acid onto their shirts or tights polycarbonate is susceptible to alkali hydrolysis the reaction simply depolymerising the material polyesters are prone to degrade when treated with strong acids and in all these cases care must be taken to dry the raw materials for processing at high temperatures to prevent the problem from occurring
many polymers are also attacked by uv radiation at vulnerable points in their chain structures thus polypropylene suffers severe cracking in sunlight unless anti oxidants are added the point of attack occurs at the tertiary carbon atom present in every repeat unit causing oxidation and finally chain breakage

clandestine chemistry

clandestine chemistry is chemistry carried out in secret and particularly in illegal drug laboratories larger labs are usually run by gangs or organized crime intending to produce for distribution on the black market smaller labs can be run by individual chemists working clandestinely in order to synthesize smaller amounts of controlled substances or simply out of a hobbyist interest in chemistry often because of the difficulty ascertaining the purity of other illegally synthesized drugs obtained on the black market the term clandestine lab is generally used in any situation involving the production of illicit compounds regardless of whether the facilities being used qualify as a true laboratory
history
ancient forms of clandestine chemistry included the manufacturing of poisons
another old form of clandestine chemistry is the illegal brewing and distillation of alcohol this is frequently done to avoid taxation on spirits
from to the united states prohibited the sale manufacture or transportation of alcoholic beverages this opened a door for brewers to supply their own town with alcohol just like modern day drug labs distilleries were placed in rural areas the term moonshine generally referred to corn whiskey that is a whiskey like liquor made from corn today american made corn whiskey can be labeled or sold under that name or as bourbon or tennessee whiskey depending on the details of the production process
precursor chemicals
prepared substances as opposed to those that occur naturally in a consumable form such as cannabis and psilocybin mushrooms require reagents some drugs like cocaine and morphine are extracted from plant sources and refined with aid of chemicals semi synthetic drugs such as heroin are made starting from alkaloids extracted from plant sources which are the precursors for further synthesis in the case of heroin a mixture of alkaloids is extracted from the opium poppy papaver somniferum by placing small incisions in its bulb a milky fluid bleeds out of the incisions which is then left to dry out and scraped off the bulbs yielding raw opium morphine one of many alkaloids in opium is then extracted out of the opium by precipitation and turned into heroin by heating it with acetic anhydride for several hours other drugs such as methamphetamine and mdma are normally made from commercially available chemicals though both can also be made from naturally occurring precursors methamphetamine is also sometimes made from ephedrine one of the naturally occurring alkaloids in ephedra ephedra sinica mdma can be made from safrole the major constituent of several etheric oils like sassafras governments have adopted a strategy of chemical control as part of their overall drug control and enforcement plans chemical control offers a means of attacking illicit drug production and disrupting the process before the drugs have entered the market
because many legitimate industrial chemicals such as anhydrous ammonia and iodine are also necessary in the processing and synthesis of most illicitly produced drugs preventing the diversion of these chemicals from legitimate commerce to illicit drug manufacturing is a difficult job governments often place restrictions on the purchase of large quantities of chemicals that can be used in the production of illicit drugs usually requiring licences or permits to ensure that the purchaser has a legitimate need for them
suppliers of precursor chemicals
chemicals critical to the production of cocaine heroin and synthetic drugs are produced in many countries throughout the world many manufacturers and suppliers exist in europe china india the united states and a host of other countries
historically chemicals critical to the synthesis or manufacture of illicit drugs are introduced into various venues via legitimate purchases by companies that are registered and licensed to do business as chemical importers or handlers once in a country or state the chemicals are diverted by rogue importers or chemical companies by criminal organizations and individual violators or acquired as a result of coercion on the part of drug traffickers in response to stricter international controls drug traffickers have increasingly been forced to divert chemicals by mislabeling the containers forging documents establishing front companies using circuitous routing hijacking shipments bribing officials or smuggling products across international borders
enforcement of controls on precursor chemicals
general
the multilateral chemical reporting initiative encourages governments to exchange information on a voluntary basis in order to monitor international chemical shipments over the past decade key international bodies like the commission on narcotic drugs and the u n general assembly's special session ungass have addressed the issue of chemical diversion in conjunction with u s efforts these organizations raised specific concerns about potassium permanganate and acetic anhydride
to facilitate the international flow of information about precursor chemicals the united states through its relationship with the inter american drug control abuse commission cicad continues to evaluate the use of precursor chemicals and assist countries in strengthening controls many nations still lack the capacity to determine whether the import or export of precursor chemicals is related to legitimate needs or illicit drugs the problem is complicated by the fact that many chemical shipments are either brokered or transshipped through third countries in an attempt to disguise their purpose or destination
the international narcotics control board incb has opted to organize an international conference with the goal of devising a specific action plan to counter the traffic in mdma precursor chemicals in july the incb requested the assistance of dea in planning an international conference on preventing the diversion of chemicals used in the production of amphetamine type stimulants ats including mdma ecstasy and methamphetamine
despite this long history of law enforcement actions restrictions of chemicals and even covert military actions many illicit drugs are still widely available all over the world
cocaine
operation purple is a u s dea driven international chemical control initiative designed to reduce the illicit manufacture of cocaine in the andean region identifying rogue firms and suspect individuals gathering intelligence on diversion methods trafficking trends and shipping routes and taking administrative civil and or criminal action as appropriate critical to the success of this operation is the communication network that gives notification of shipments and provides the government of the importer sufficient time to verify the legitimacy of the transaction and take appropriate action the effects of this initiative have been dramatic and far reaching operation purple has exposed a significant vulnerability among traffickers and has grown to include almost thirty nations according to the dea operation purple has been highly effective at interfering with cocaine production however illicit chemists always find new methods to evade the dea's scrutiny
in countries where strict chemical controls have been put in place illicit drug production has been seriously affected for example few of the chemicals needed to process coca leaf into cocaine are manufactured in bolivia or peru most are smuggled in from neighbouring countries with advanced chemical industries or diverted from a smaller number of licit handlers increased interdiction of chemicals in peru and bolivia has contributed to final product cocaine from those countries being of lower minimally oxidized quality
as a result bolivian lab operators are now using inferior substitutes such as cement instead of lime and sodium bicarbonate instead of ammonia and recycled solvents like ether some non solvent fuels such as gasoline kerosene and diesel fuel are even used in place of solvents
manufacturers are attempting to streamline a production process that virtually eliminates oxidation to produce cocaine base some laboratories are not using sulfuric acid during the maceration state consequently less cocaine alkaloid is extracted from the leaf producing less cocaine hydrochloride the powdered cocaine marketed for overseas consumption
heroin
similarly heroin producing countries depend on supplies of acetic anhydride from the international market this heroin precursor continues to account for the largest volume of internationally seized chemicals according to the international narcotics control board since july there have been several notable seizures of acetic anhydride in turkey amounting to nearly seventeen metric tons and turkmenistan totaling seventy three metric tons
acetic anhydride aa the most commonly used chemical agent in heroin processing is virtually irreplaceable according to the dea mexico remains the only heroin source route to heroin laboratories in afghanistan authorities in uzbekistan turkmenistan kyrgyzstan and kazakhstan routinely seize ton quantity shipments of diverted acetic anhydride
the lack of acetic anhydride has caused clandestine chemists in some countries to substitute it for lower quality precursors such as acetic acid and results in the formation of impure black tar heroin that contains a mixture of drugs not found in heroin made with pure chemicals
dea's operation topaz is a coordinated international strategy targeting acetic anhydride in place since march a total of thirty one countries are currently organized participants in the program in addition to regional participants the dea reports that as of june some consignments of acetic anhydride had been tracked totaling kilograms as of july there has been approximately shipments of aa totaling kilograms either stopped or seized
methamphetamine
the methamphetamine situation changed in the mid s with the entrance of mexican organized crime into production and distribution according to the dea the seizure of metric tons of pseudoephedrine the primary precursor chemical used in the production of methamphetamine in texas revealed that mexican trafficking groups were producing methamphetamine on an unprecedented scale
amphetamines
clandestine chemistry made its mark in the late s when amphetamines became controlled substances in many countries
methamphetamine was a favorite among biker gangs but after phenylacetone became a schedule ii controlled immediate precursor in it was harder for underground chemists to manufacture methamphetamine
frustrated underground chemists searched for alternative methods for producing methamphetamine the two predominant methods which appeared both involve the reduction of ephedrine or pseudoephedrine to methamphetamine at the time neither was a watched chemical and pills containing the substance could be bought by the thousands without raising any kind of suspicion
in the s ephedrine pseudoephedrine became a closely watched precursor by the dea making it somewhat more difficult for underground chemists to produce methamphetamine many individual states have enacted precursor control laws which limit the sale of over the counter cold medications which contain ephedrine or pseudoephedrine
dea el paso intelligence center data is showing a distinct downward trend in the seizure quantities of clandestine drug labs for the illicit manufacture of methamphetamine from a high of in seizure quantities on the other hand are steadily increasing since according to data from stride see table to the right
explosives
clandestine chemistry does not limit itself only to drugs it is also associated with explosives and other illegal chemicals of the explosives manufactured illegally nitroglycerin and acetone peroxide are easiest to produce due to the ease with which the precursors can be acquired
uncle fester is a writer who commonly writes about different aspects of clandestine chemistry secrets of methamphetamine manufacture is among one of his most popular books and is considered required reading for dea agents more of his books deal with other aspects of clandestine chemistry including explosives and poisons fester is however considered by many to be a faulty and unreliable source for information in regard to the clandestine manufacture of chemicals

radioanalytical chemistry

radioanalytical chemistry focuses on the analysis of sample for their radionuclide content various methods are employed to purify and identify the radioelement of interest through chemical methods and sample measurement techniques
history
the field of radioanalytical chemistry was originally developed by marie curie with contributions by ernest rutherford and frederick soddy they developed chemical separation and radiation measurement techniques on terrestrial radioactive substances during the twenty years that followed the concepts of radionuclides was born since curie's time applications of radioanalytical chemistry have proliferated modern advances in nuclear and radiochemistry research have allowed practitioners to apply chemistry and nuclear procedures to elucidate nuclear properties and reactions used radioactive substances as tracers and measure radionuclides in many different types of samples
the importance of radioanalytical chemistry spans many fields including chemistry physics medicine pharmacology biology ecology hydrology geology forensics atmospheric sciences health protection archeology and engineering applications include forming and characterizing new elements determining the age of materials and creating radioactive reagents for specific tracer use in tissues and organs the ongoing goal of radioanalytical researchers is to develop more radionuclides and lower concentrations in people and the environment
radiation decay modes
alpha particle decay
alpha decay is characterized by the emission of an alpha particle a he nucleus the mode of this decay causes the parent nucleus to decrease by two protons and two neutrons this type of decay follows the relation
formula
beta particle decay
beta decay is characterized by the emission of a neutrino and a negatron which is equivalent to an electron this process occurs when a nucleus has an excess of neutrons with respect to protons as compared to the stable isobar this type of transition converts a neutron into a proton similarly a positron is released when a proton is converted into a neutron these decays follows the relation
formula
formula
gamma ray decay
gamma ray emission is follows the previously discussed modes of decay when the decay leaves a daughter nucleus in an excited state this nucleus is capable of further de excitation to a lower energy state by the release of a photon this decay follows the relation
formula
radiation detection principles
gas ionization detectors
gaseous ionization detectors collect and record the electrons freed from gaseous atoms and molecules by the interaction of radiation released by the source a voltage potential is applied between two electrodes within a sealed system since the gaseous atoms are ionized after they interact with radiation they are attracted to the anode which produces a signal it is important to vary the applied voltage such that the response falls within a critical proportional range
solid state detectors
the operating principle of semiconductor detectors is similar to gas ionization detectors expect instead of ionization gas atoms free electrons and holes are produced which create a signal at the electrodes the advantage of solid state detectors is the greater resolution of the resultant energy spectrum usually nai tl detectors are used for more precise applications ge li and si li detectors have been developed for extra sensitive measurements high pure germanium detectors are used under a liquid nitrogen environment
scintillation detectors
scintillation detectors uses a photo luminescent source such as zns which interacts with radiation when a radioactive particle decays and strikes the photo luminescent material a photon is released this photon is multiplied in a photomultiplier tube which converts light into an electrical signal this signal is then processed and converted into a channel by comparing the number of counts to the energy level typically in kev or mev the type of decay can be determined
chemical separation techniques
due to radioactive nucleotides have similar properties to their stable inactive counterparts similar analytical chemistry separation techniques can be used these separation methods include precipitation ion exchange liquid liquid extraction solid phase extraction distillation and electrodeposition
radioanalytical chemistry principles
sample loss by radiocolloidal behaviour
samples with very low concentrations are difficult to measure accurately due to the radioactive atoms unexpectedly depositing on surfaces sample loss at trace levels may be due to adhesion to container walls and filter surface sites by ionic or electrostatic adsorption as well as metal foils and glass slides sample loss is an ever present concern especially at the beginning of the analysis path where sequential steps may compound these losses
various solutions are known to circumvent these losses which include adding an inactive carrier or adding a tracer research has also shown that that pretreatment of glassware and plastic surfaces can reduce radionuclide sorption by saturating the sites
carrier or tracer addition
due to the inherent nature of radionuclides yielding low concentrations a common technique to improve yields is the addition of carrier ions or tracers isotope dilution involves the addition of a known amount of radionuclide tracer to the sample that contains a known stable element this is done at the start of the analysis procedure so once the final measurements are taken sample loss is considered this procedure avoids the need for any quantitative recovery which greatly simplifies the analytical process
carrier addition is the reverse technique of tracer addition instead of isotope dilution a known mass of stable carrier ion is added to radionuclide sample solution the carrier reagent must be calibrated prior to addition to the sample to verify the resultant measurements the expected yield is compared to the actual yield any loss in yield is analogous to any losses in the radioactive sample typically the amount of carrier added is conventionally selected for the ease of weighing such that the accuracy of the resultant weight is within for alpha particles special techniques must be applied to obtain the required thin sample sources
quality assurance
as this is an analytical chemistry technique quality control is an important factor to maintain a laboratory must produce trustworthy results this can be accomplished by a laboratories continual effort to maintain instrument calibration measurement reproducibility and applicability of analytical methods in all laboratories there must be a quality assurance plan this plan describes the quality system and procedures in place to obtain consistent results such results must be authentic appropriately documented and technically defensible such elements of quality assurance include organization personnel training laboratory operating procedures procurement documents chain of custody records standard certificates analytical records standard procedures qc sample analysis program and results instrument testing and maintenance records results of performance demonstration projects results of data assessment audit reports and record retention policies
the cost of quality assurance is continually on the rise but the benefits far outweigh this cost the average quality assurance workload was risen from to a modern load of this heightened focus on quality assurance ensures that quality measurements that are reliable are achieved the cost of failure far outweighs the cost of prevention and appraisal finally results must be scientifically defensible by adhering to stringent regulations in the event of a lawsuit

chemical technologist

chemical technologists and technicians abbr chem techs are workers who provide technical support or services in chemical related fields they may work under direct supervision or may work independently depending on their specific position and duties their work environments differ widely and include but are not limited to laboratories and industrial settings as such it is nearly impossible to generalize the duties of chem techs as their individual jobs vary greatly biochemical techs often do similar work in biochemistry
technologists
chemical technologists are more likely than technicians to participate in the actual design of experiments and may be involved in the interpretation of experimental data they may also be responsible for the operation of chemical processes in large plants and may even assist chemical engineers in the design of the same
some post secondary education is generally required to be either a chemical technician or technologist occasionally a company may be willing to provide a high school graduate with training to become a chemical technician but more often a two year degree will be required chemical technologists generally require completion of a specific college program either two year or four year in chemical biochemical or chemical engineering technology or a closely related discipline
they usually work under or with a scientist such as a chemist or biochemist
technicians
chemical or biochemical technicians often work in clinical medical laboratories conducting routine analyses of medical samples such as blood and urine industries which employ chem techs include chemical petrochemical and pharmaceutical industries companies within these industries can be concerned with manufacturing research and development r d consulting quality control and a variety of other areas also chem techs working for these companies may be used to conduct quality control and other routine analyses or assist in chemical and biochemical research including analyses industrial chemistry environmental protection and even chemical engineering
duties
as a general rule chemical technologists are more likely to be provided with greater autonomy and more complex responsibilities than chemical technicians
chemical technicians
the most common work done by chemical technicians is in r d they often work in a laboratory environment under the supervision of a chemist or a chemical engineer
they may typically assist in setting up and conducting chemical experiments and may operate lab equipment under supervision they are expected to maintain established quality control standards they may also compile records for analytical studies and sometimes are involved in writing reports on studies
national certification for chemical technologists and technicians is required in some countries

chemistry education

chemistry education or chemical education is a comprehensive term that refers to the study of the teaching and learning of chemistry in all schools colleges and universities topics in chemistry education might include understanding how students learn chemistry how best to teach chemistry and how to improve learning outcomes by changing teaching methods and appropriate training of chemistry instructors within many modes including classroom lecture demonstrations and laboratory activities there is a constant need to update the skills of teachers engaged in teaching chemistry and so chemistry education speaks to this need
overview
there are at least four different philosophical perspectives that describe how the work in chemistry education is carried out the first is what one might call a practitioner s perspective wherein the individuals who are responsible for teaching chemistry teachers instructors professors are the ones who ultimately define chemistry education by their actions
a second perspective is defined by a self identified group of chemical educators faculty members and instructors who as opposed to declaring their primary interest in a typical area of laboratory research organic inorganic biochemistry etc take on an interest in contributing suggestions essays observations and other descriptive reports of practice into the public domain through journal publications books and presentations dr robert l lichter then executive director of the camille and henry dreyfus foundation speaking in a plenary session at the th biennial conference on chemical education recent bcce meetings posed the question why do terms like chemical educator even exist in higher education when there is a perfectly respectable term for this activity namely chemistry professor one criticism of this view is that few professors bring any formal preparation in or background about education to their jobs and so lack any professional perspective on the teaching and learning enterprise particularly discoveries made about effective teaching and how students learn
a third perspective is chemical education research cer following the example of physics education research per cer tends to take the theories and methods developed in pre college science education research which generally takes place in schools of education and applies them to understanding comparable problems in post secondary settings in addition to pre college settings like science education researchers cer practitioners tend to study the teaching practices of others as opposed to focusing on their own classroom practices chemical education research is typically carried out in situ using human subjects from secondary and post secondary schools chemical education research utilizes both quantitative and qualitative data collection methods quantitative methods typically involve collecting data that can then be analyzed using various statistical methods qualitative methods include interviews observations journaling and other methods common to social science research
finally there is an emergent perspective called the scholarship of teaching and learning sotl although there is debate on how to best define sotl one of the primary practices is for mainstream faculty members organic inorganic biochemistry etc to develop a more informed view of their practices how to carry out research and reflection on their own teaching and about what constitutes deep understanding in student learning
work in chemistry education then derives from some combination of these perspectives
fear of chemistry classes
chemistry courses are required for many university students especially for students who are studying science some students find chemistry classes and lab work stressful this anxiety has been called chemophobia fears commonly center on academic performance the difficulty of learning chemical equations and fear of getting lab chemicals on the hands women students were more anxious than men previous exposure to learning chemistry was associated with lower anxiety
academic journals on chemistry education
there are many journals where papers related to chemistry education can be found or published historically the circulation of many of these journals was limited to the country of publication some concentrate on chemistry at different education levels schools vs universities while others cover all education levels most of these journals carry a mixture of articles that range from reports on classroom and laboratory practices to educational research
much research in chemistry education is also published in journals in the wider science education field

chemical similarity

chemical similarity or molecular similarity refers to the similarity of chemical elements molecules or chemical compounds with respect to either structural or functional qualities i e the effect that the chemical compound has on reaction partners in inorganic or biological settings biological effects and thus also similarity of effects are usually quantified using the biological activity of a compound in general terms function can be related to the chemical activity of compounds among others
the notion of chemical similarity or molecular similarity is one of the most important concepts in chemoinformatics it plays an important role in modern approaches to predicting the properties of chemical compounds designing chemicals with a predefined set of properties and especially in conducting drug design studies by screening large databases containing structures of available or potentially available chemicals these studies are based on the similar property principle of johnson and maggiora which states similar compounds have similar properties
similarity measures
chemical similarity is often described as an inverse of a measure of distance in descriptor space distance measures can be classified into euclidean measures and non euclidean measures depending on whether the triangle inequality holds examples for inverse euclidean distance measures are molecule kernels that measure the structural similarity of chemical compounds
similarity search and virtual screening
the similarity based virtual screening a kind of ligand based virtual screening assumes that all compounds in a database that are similar to a query compound have similar biological activity although this hypothesis is not always valid quite often the set of retrieved compounds is considerably enriched with actives to achieve high efficacy of similarity based screening of databases containing millions of compounds molecular structures are usually represented by molecular screens structural keys or by fixed size or variable size molecular fingerprints molecular screens and fingerprints can contain both d and d information however the d fingerprints which are a kind of binary fragment descriptors dominate in this area fragment based structural keys like mdl keys are sufficiently good for handling small and medium sized chemical databases whereas processing of large databases is performed with fingerprints having much higher information density fragment based daylight bci and unity d tripos fingerprints are the best known examples the most popular similarity measure for comparing chemical structures represented by means of fingerprints is the tanimoto or jaccard coefficient t two structures are usually considered similar if t for daylight fingerprints however it is a common misunderstanding that a similarity of t reflects similar bioactivities in general the myth

marine chemist

a marine chemist is an environmental occupational safety and health professional who is involved in the study of the chemical processes and the composition of marine water bodies
this can involve a wide range of parameters including the monitoring of dissolved inorganic chemicals and the chemistry of particulate organic matter a major use of marine chemistry is through pollution regulation and monitoring in marine environmental protection non departmental government agencies such as the scottish environment protection agency use marine chemists in their monitoring of shellfish growing waters bathing waters coastal and estuarine waters for the regulation and enforcement of governmental environmental policy and treaties such as the water framework directive
a marine chemist is also a trained professional who is responsible for ensuring that repair and construction of marine vessels can be made in safety whenever those repairs might result in fire explosion or exposure to toxic vapors or chemicals by virtue of his or her training experience and education the marine chemist is uniquely qualified as a specialist in confined space safety and atmospheric sampling or monitoring
why do we need marine chemist lets take a moment to examine that question basic ship design open decks and enclosed spaces for cargoes has remained basically unchanged for decades structural materials and methods have changed but in principle the basic design concept has been the same for centuries today's cargoes however have shifted to a greater number of toxic substances that has added the health concern of toxicity to the existing safety concerns of fire and explosion not just during a voyage but even when a vessel is in a shipyard for routine maintenance and repair
in the national fire protection association national fire protection association assumed jurisdiction over the marine chemist program the nfpa continues to oversee the profession which is based on the nfpa standard standard for control of gas hazards on vessels

philosophy of chemistry

the philosophy of chemistry considers the methodology and underlying assumptions of the science of chemistry it is explored by philosophers chemists and philosopher chemist teams for much of its history philosophy of science has been dominated by the philosophy of physics but the philosophical questions that arise from chemistry have received increasing attention since the latter part of the th century
foundations of chemistry
major philosophical questions arise as soon as one attempts to define chemistry and what it studies atoms and molecules are often assumed to be the fundamental units of chemical theory but traditional descriptions of molecular structure and chemical bonding fail to account for the properties of many substances including metals and metal complexes and aromaticity
additionally chemists frequently use non existent chemical entities like resonance structures to explain the structure and reactions of different substances these explanatory tools use the language and graphical representations of molecules to describe the behavior of chemicals and chemical reactions that in reality do not behave as straightforward molecules
some chemists and philosophers of chemistry prefer to think of substances rather than microstructures as the fundamental units of study in chemistry there is not always a one to one correspondence between the two methods of classifying substances for example many rocks exist as mineral complexes composed of multiple ions that do not occur in fixed proportions or spatial relationships to one another
a related philosophical problem is whether chemistry is the study of substances or reactions atoms even in a solid are in perpetual motion and under the right conditions many chemicals react spontaneously to form new products a variety of environmental variables contribute to a substance's properties including temperature and pressure proximity to other molecules and the presence of a magnetic field as schummer puts it substance philosophers define a chemical reaction by the change of certain substances whereas process philosophers define a substance by its characteristic chemical reactions
philosophers of chemistry discuss issues of symmetry and chirality in nature organic i e carbon based molecules are those most often chiral amino acids nucleic acids and sugars all of which are found exclusively as a single enantiomer in organisms are the basic chemical units of life chemists biochemists and biologists alike debate the origins of this homochirality philosophers debate facts regarding the origin of this phenomenon namely whether it emerged contingently amid a lifeless racemic environment or if other processes were at play some speculate that answers can only be found in comparison to extraterrestrial life if it is ever found other philosophers question whether there exists a bias toward assumptions of nature as symmetrical thereby causing resistance to any evidence to the contrary
one of the most topical issues is determining to what extent physics specifically quantum mechanics explains chemical phenomena can chemistry in fact be reduced to physics as has been assumed by many or are there inexplicable gaps some authors roald hoffmann have recently suggested that a number of difficulties exist in the reductionist program with concepts like aromaticity ph reactivity nucleophilicity for example the noted philosopher of science karl popper among others predicted as much
philosophers of chemistry
several philosophers and scientists have focused on the philosophy of chemistry in recent years notably the dutch philosopher jaap van brakel who wrote the philosophy of chemistry in and the maltese philosopher chemist eric scerri editor of the journal foundations of chemistry and author of normative and descriptive philosophy of science and the role of chemistry in philosophy of chemistry among other articles scerri is especially interested in the philosophical foundations of the periodic table and how physics and chemistry intersect in relation to it which he contends is not merely a matter for science but for philosophy
although in other fields of science students of the method are generally not practitioners in the field in chemistry particularly in synthetic organic chemistry intellectual method and philosophical foundations are often explored by investigators with active research programmes elias james corey developed the concept of retrosynthesis published a seminal work the logic of chemical synthesis which deconstructs these thought processes and speculates on computer assisted synthesis other chemists such as k c nicolaou co author of classics in total synthesis have followed in his lead

chemistry

chemistry is a branch of physical science that studies the composition structure properties and change of matter in this realm chemistry deals with such topics as the properties of individual atoms the manner in which atoms form chemical bonds in the formation of compounds the interactions of substances through intermolecular forces to give matter its general properties and the interactions between substances through chemical reactions to form different substances
chemistry is sometimes called the central science because it bridges other natural sciences like physics geology and biology chemistry is a branch of physical science but distinct from physics
the etymology of the word chemistry has been much disputed the history of chemistry can be traced to certain practices known as alchemy which had been practiced for several millennia in various parts of the world
etymology
the word chemistry comes from the word alchemy an earlier set of practices that encompassed elements of chemistry metallurgy philosophy astrology astronomy mysticism and medicine it is commonly thought of as the quest to turn lead or another common starting material into gold alchemy which was practiced around is the study of the composition of waters movement growth embodying disembodying drawing the spirits from bodies and bonding the spirits within bodies zosimos an alchemist was called a chemist in popular speech and later the suffix ry was added to this to describe the art of the chemist as chemistry
the word alchemy in turn is derived from the arabic word al k m in origin the term is borrowed from the greek or this may have egyptian origins many believe that al k m is derived from the greek which is in turn derived from the word chemi or kimi which is the ancient name of egypt in egyptian alternately al k m may derive from meaning cast together
definition
in retrospect the definition of chemistry has changed over time as new discoveries and theories add to the functionality of the science the term chymistry in the view of noted scientist robert boyle in meant the subject of the material principles of mixed bodies in the chemist christopher glaser described chymistry as a scientific art by which one learns to dissolve bodies and draw from them the different substances on their composition and how to unite them again and exalt them to a higher perfection
the definition of the word chemistry as used by georg ernst stahl meant the art of resolving mixed compound or aggregate bodies into their principles and of composing such bodies from those principles in jean baptiste dumas considered the word chemistry to refer to the science concerned with the laws and effects of molecular forces this definition further evolved until in it came to mean the science of substances their structure their properties and the reactions that change them into other substances a characterization accepted by linus pauling more recently in professor raymond chang broadened the definition of chemistry to mean the study of matter and the changes it undergoes
history
early civilizations such as the egyptians babylonians indians amassed practical knowledge concerning the arts of metallurgy pottery and dyes but didn't develop a systematic theory
a basic chemical hypothesis first emerged in classical greece with the theory of four elements as propounded definitively by aristotle stating that that fire air earth and water were the fundamental elements from which everything is formed as a combination greek atomism dates back to bc arising in works by philosophers such as democritus and epicurus in bc the roman philosopher lucretius expanded upon the theory in his book de rerum natura on the nature of things unlike modern concepts of science greek atomism was purely philosophical in nature with little concern for empirical observations and no concern for chemical experiments
in the hellenistic world the art of alchemy first proliferated mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life alchemy was discovered and practised widely throughout the arab world after the muslim conquests and from there diffused into medieval and renaissance europe through latin translations
chemistry as science
under the influence of the new empirical methods propounded by sir francis bacon and others a group of chemists at oxford robert boyle robert hooke and john mayow began to reshape the old alchemical traditions into a scientific discipline boyle in particular is regarded as the founding father of chemistry due to his most important work the classic chemistry text the sceptical chymist where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry he formulated boyle's law rejected the classical four elements and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment
the theory of phlogiston a substance at the root of all combustion was propounded by the german georg ernst stahl in the early th century and was only overturned by the end of the century by the french chemist antoine lavoisier the chemical analogue of newton in physics who did more than any other to establish the new science on proper theoretical footing by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day
prior to his work though many important discoveries had been made specifically relating to the nature of air which was discovered to be composed of many different gases the scottish chemist joseph black the first experimental chemist and the dutchman j b van helmont discovered carbon dioxide or what black called fixed air in henry cavendish discovered hydrogen and elucidated its properties and joseph priestley and independently carl wilhelm scheele isolated pure oxygen
english scientist john dalton proposed the modern theory of atoms that all substances are composed of indivisible atoms of matter and that different atoms have varying atomic weights
the development of the electrochemical theory of chemical combinations occurred in the early th century as the result of the work of two scientists in particular j j berzelius and humphry davy made possible by the prior invention of the voltaic pile by alessandro volta davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current
british william prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen j a r newlands devised an early table of elements which was then developed into the modern periodic table of elements by the german julius lothar meyer and the russian dmitri mendeleev in the s the inert gases later called the noble gases were discovered by william ramsay in collaboration with lord rayleigh at the end of the century thereby filling in the basic structure of the table
organic chemistry was developed by justus von liebig and others following friedrich w hler's synthesis of urea which proved that living organisms were in theory reducible to chemistry other crucial th century advances were an understanding of valence bonding edward frankland in and the application of thermodynamics to chemistry j w gibbs and svante arrhenius in the s
chemical structure
at the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms in j j thomson of cambridge university discovered the electron and soon after the french scientist becquerel as well as the couple pierre and marie curie investigated the phenomenon of radioactivity in a series of pioneering scattering experiments ernest rutherford at the university of manchester discovered the internal structure of the atom and the existence of the proton classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles
his work on atomic structure was improved on by his students the danish physicist niels bohr and henry moseley the electronic theory of chemical bonds and molecular orbitals was developed by the american scientists linus pauling and gilbert n lewis
the year was declared by the united nations as the international year of chemistry it was an initiative of the international union of pure and applied chemistry and of the united nations educational scientific and cultural organization and involves chemical societies academics and institutions worldwide and relied on individual initiatives to organize local and regional activities
principles of modern chemistry
the current model of atomic structure is the quantum mechanical model traditional chemistry starts with the study of elementary particles atoms molecules substances metals crystals and other aggregates of matter this matter can be studied in solid liquid or gas states in isolation or in combination the interactions reactions and transformations that are studied in chemistry are usually the result of interactions between atoms leading to rearrangements of the chemical bonds which hold atoms together such behaviors are studied in a chemistry laboratory
the chemistry laboratory stereotypically uses various forms of laboratory glassware however glassware is not central to chemistry and a great deal of experimental as well as applied industrial chemistry is done without it
a chemical reaction is a transformation of some substances into one or more different substances the basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms it can be symbolically depicted through a chemical equation which usually involves atoms as subjects the number of atoms on the left and the right in the equation for a chemical transformation is equal when the number of atoms on either side is unequal the transformation is referred to as a nuclear reaction or radioactive decay the type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules known as chemical laws
energy and entropy considerations are invariably important in almost all chemical studies chemical substances are classified in terms of their structure phase as well as their chemical compositions they can be analyzed using the tools of chemical analysis e g spectroscopy and chromatography scientists engaged in chemical research are known as chemists most chemists specialize in one or more sub disciplines several concepts are essential for the study of chemistry some of them are
matter
in chemistry matter is defined as anything that has rest mass and volume it takes up space and is made up of particles the particles that make up matter have rest mass as well not all particles have rest mass such as the photon matter can be a pure chemical substance or a mixture of substances
atom
the atom is the basic unit of chemistry it consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud the nucleus is made up of positively charged protons and uncharged neutrons together called nucleons while the electron cloud consists of negatively charged electrons which orbit the nucleus in a neutral atom the negatively charged electrons balance out the positive charge of the protons the nucleus is dense the mass of a nucleon is times that of an electron yet the radius of an atom is about times that of its nucleus
the atom is also the smallest entity that can be envisaged to retain the chemical properties of the element such as electronegativity ionization potential preferred oxidation state s coordination number and preferred types of bonds to form e g metallic ionic covalent
element
a chemical element is a pure substance which is composed of a single type of atom characterized by its particular number of protons in the nuclei of its atoms known as the atomic number and represented by the symbol z the mass number is the sum of the number of protons and neutrons in a nucleus although all the nuclei of all atoms belonging to one element will have the same atomic number they may not necessarily have the same mass number atoms of an element which have different mass numbers are known as isotopes for example all atoms with protons in their nuclei are atoms of the chemical element carbon but atoms of carbon may have mass numbers of or
the standard presentation of the chemical elements is in the periodic table which orders elements by atomic number the periodic table is arranged in groups or columns and periods or rows the periodic table is useful in identifying periodic trends
compound
a compound is a pure chemical substance composed of more than one element the properties of a compound bear little similarity to those of its elements the standard nomenclature of compounds is set by the international union of pure and applied chemistry iupac organic compounds are named according to the organic nomenclature system inorganic compounds are named according to the inorganic nomenclature system in addition the chemical abstracts service has devised a method to index chemical substances in this scheme each chemical substance is identifiable by a number known as its cas registry number
molecule
a molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties that is its potential to undergo a certain set of chemical reactions with other substances however this definition only works well for substances that are composed of molecules which is not true of many substances see below molecules are typically a set of atoms bound together by covalent bonds such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs
thus molecules exist as electrically neutral units unlike ions when this rule is broken giving the molecule a charge the result is sometimes named a molecular ion or a polyatomic ion however the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well separated form such as a directed beam in a vacuum in a mass spectrometer charged polyatomic collections residing in solids for example common sulfate or nitrate ions are generally not considered molecules in chemistry
the inert or noble gas elements helium neon argon krypton xenon and radon are composed of lone atoms as their smallest discrete unit but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way identifiable molecules compose familiar substances such as water air and many organic compounds like alcohol sugar gasoline and the various pharmaceuticals
however not all substances or chemical compounds consist of discrete molecules and indeed most of the solid substances that make up the solid crust mantle and core of the earth are chemical compounds without molecules these other types of substances such as ionic compounds and network solids are organized in such a way as to lack the existence of identifiable molecules per se instead these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance examples of such substances are mineral salts such as table salt solids like carbon and diamond metals and familiar silica and silicate minerals such as quartz and granite
one of the main characteristics of a molecule is its geometry often called its structure while the structure of diatomic triatomic or tetra atomic molecules may be trivial linear angular pyramidal etc the structure of polyatomic molecules that are constituted of more than six atoms of several elements can be crucial for its chemical nature
substance and mixture
a chemical substance is a kind of matter with a definite composition and set of properties a collection of substances is called a mixture examples of mixtures are air and alloys
mole and amount of substance
the mole is a unit of measurement that denotes an amount of substance also called chemical amount the mole is defined as the number of atoms found in exactly kilogram or grams of carbon where the carbon atoms are unbound at rest and in their ground state the number of entities per mole is known as the avogadro constant and is determined empirically to be approximately mol molar concentration is the amount of a particular substance per volume of solution and is commonly reported in moldm
phase
in addition to the specific chemical properties that distinguish different chemical classifications chemicals can exist in several phases for the most part the chemical classifications are independent of these bulk phase classifications however some more exotic phases are incompatible with certain chemical properties a phase is a set of states of a chemical system that have similar bulk structural properties over a range of conditions such as pressure or temperature
physical properties such as density and refractive index tend to fall within values characteristic of the phase the phase of matter is defined by the phase transition which is when energy put into or taken out of the system goes into rearranging the structure of the system instead of changing the bulk conditions
sometimes the distinction between phases can be continuous instead of having a discrete boundary in this case the matter is considered to be in a supercritical state when three states meet based on the conditions it is known as a triple point and since this is invariant it is a convenient way to define a set of conditions
the most familiar examples of phases are solids liquids and gases many substances exhibit multiple solid phases for example there are three phases of solid iron alpha gamma and delta that vary based on temperature and pressure a principal difference between solid phases is the crystal structure or arrangement of the atoms another phase commonly encountered in the study of chemistry is the aqueous phase which is the state of substances dissolved in aqueous solution that is in water
less familiar phases include plasmas bose einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials while most familiar phases deal with three dimensional systems it is also possible to define analogs in two dimensional systems which has received attention for its relevance to systems in biology
bonding
atoms sticking together in molecules or crystals are said to be bonded with one another a chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them more than simple attraction and repulsion the energies and distributions characterize the availability of an electron to bond to another atom
a chemical bond can be a covalent bond an ionic bond a hydrogen bond or just because of van der waals force each of these kinds of bonds is ascribed to some potential these potentials create the interactions which hold atoms together in molecules or crystals in many simple compounds valence bond theory the valence shell electron pair repulsion model vsepr and the concept of oxidation number can be used to explain molecular structure and composition
an ionic bond is formed when a metal loses one or more of its electrons becoming a positively charged cation and the electrons are then gained by the non metal atom becoming a negatively charged anion the two oppositely charged ions attract one another and the ionic bond is the electrostatic force of attraction between them for example sodium na a metal loses one electron to become an na cation while chlorine cl a non metal gains this electron to become cl the ions are held together due to electrostatic attraction and that compound sodium chloride nacl or common table salt is formed
in a covalent bond one or more pairs of valence electrons are shared by two atoms the resulting electrically neutral group of bonded atoms is termed a molecule atoms will share valence electrons in such a way as to create a noble gas electron configuration eight electrons in their outermost shell for each atom atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule however some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration these atoms are said to follow the duet rule and in this way they are reaching the electron configuration of the noble gas helium which has two electrons in its outer shell
similarly theories from classical physics can be used to predict many ionic structures with more complicated compounds such as metal complexes valence bond theory is less applicable and alternative approaches such as the molecular orbital theory are generally used see diagram on electronic orbitals
energy
in the context of chemistry energy is an attribute of a substance as a consequence of its atomic molecular or aggregate structure since a chemical transformation is accompanied by a change in one or more of these kinds of structures it is invariably accompanied by an increase or decrease of energy of the substances involved some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light thus the products of a reaction may have more or less energy than the reactants
a reaction is said to be exergonic if the final state is lower on the energy scale than the initial state in the case of endergonic reactions the situation is the reverse a reaction is said to be exothermic if the reaction releases heat to the surroundings in the case of endothermic reactions the reaction absorbs heat from the surroundings
chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy the speed of a chemical reaction at given temperature t is related to the activation energy e by the boltzmann's population factor formula that is the probability of a molecule to have energy greater than or equal to e at the given temperature t this exponential dependence of a reaction rate on temperature is known as the arrhenius equation
the activation energy necessary for a chemical reaction to occur can be in the form of heat light electricity or mechanical force in the form of ultrasound
a related concept free energy which also incorporates entropy considerations is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction in chemical thermodynamics a reaction is feasible only if the total change in the gibbs free energy is negative formula if it is equal to zero the chemical reaction is said to be at equilibrium
there exist only limited possible states of energy for electrons atoms and molecules these are determined by the rules of quantum mechanics which require quantization of energy of a bound system the atoms molecules in a higher energy state are said to be excited the molecules atoms of substance in an excited energy state are often much more reactive that is more amenable to chemical reactions
the phase of a substance is invariably determined by its energy and the energy of its surroundings when the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them it occurs in a more ordered phase like liquid or solid as is the case with water h o a liquid at room temperature because its molecules are bound by hydrogen bonds whereas hydrogen sulfide h s is a gas at room temperature and standard pressure as its molecules are bound by weaker dipole dipole interactions
the transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance however heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer thus because vibrational and rotational energy levels are more closely spaced than electronic energy levels heat is more easily transferred between substances relative to light or other forms of electronic energy for example ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy
the existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines different kinds of spectra are often used in chemical spectroscopy e g ir microwave nmr esr etc spectroscopy is also used to identify the composition of remote objects like stars and distant galaxies by analyzing their radiation spectra
the term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances
reaction
when a chemical substance is transformed as a result of its interaction with another substance or with energy a chemical reaction is said to have occurred a chemical reaction is therefore a concept related to the reaction of a substance when it comes in close contact with another whether as a mixture or a solution exposure to some form of energy or both it results in some energy exchange between the constituents of the reaction as well as with the system environment which may be designed vessels often laboratory glassware
chemical reactions can result in the formation or dissociation of molecules that is molecules breaking apart to form two or more smaller molecules or rearrangement of atoms within or across molecules chemical reactions usually involve the making or breaking of chemical bonds oxidation reduction dissociation acid base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions
a chemical reaction can be symbolically depicted through a chemical equation while in a non nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal for a nuclear reaction this holds true only for the nuclear particles viz protons and neutrons
the sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism a chemical reaction can be envisioned to take place in a number of steps each of which may have a different speed many reaction intermediates with variable stability can thus be envisaged during the course of a reaction reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions several empirical rules like the woodward hoffmann rules often come in handy while proposing a mechanism for a chemical reaction
according to the iupac gold book a chemical reaction is a process that results in the interconversion of chemical species accordingly a chemical reaction may be an elementary reaction or a stepwise reaction an additional caveat is made in that this definition includes cases where the interconversion of conformers is experimentally observable such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition but it is often conceptually convenient to use the term also for changes involving single molecular entities i e microscopic chemical events
ions and salts
an ion is a charged species an atom or a molecule that has lost or gained one or more electrons when an atom loses an electron and thus has more protons than electrons the atom is a positively charged ion or cation when an atom gains an electron and thus has more electrons than protons the atom is a negatively charged ion or anion cations and anions can form a crystalline lattice of neutral salts such as the na and cl ions forming sodium chloride or nacl examples of polyatomic ions that do not split up during acid base reactions are hydroxide oh and phosphate po
plasma is composed of gaseous matter that has been completely ionized usually through high temperature
acidity and basicity
a substance can often be classified as an acid or a base there are several different theories which explain acid base behavior the simplest is arrhenius theory which states than an acid is a substance that produces hydronium ions when it is dissolved in water and a base is one that produces hydroxide ions when dissolved in water according to br nsted lowry acid base theory acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction by extension a base is the substance which receives that hydrogen ion
a third common theory is lewis acid base theory which is based on the formation of new chemical bonds lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation while a base is a substance which can provide a pair of electrons to form a new bond according to this theory the crucial things being exchanged are charges there are several other ways in which a substance may be classified as an acid or a base as is evident in the history of this concept
acid strength is commonly measured by two methods one measurement based on the arrhenius definition of acidity is ph which is a measurement of the hydronium ion concentration in a solution as expressed on a negative logarithmic scale thus solutions that have a low ph have a high hydronium ion concentration and can be said to be more acidic the other measurement based on the br nsted lowry definition is the acid dissociation constant ka which measures the relative ability of a substance to act as an acid under the br nsted lowry definition of an acid that is substances with a higher ka are more likely to donate hydrogen ions in chemical reactions than those with lower ka values
redox
redox red uction ox idation reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons reduction or losing electrons oxidation substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents oxidants or oxidizers an oxidant removes electrons from another substance similarly substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents reductants or reducers
a reductant transfers electrons to another substance and is thus oxidized itself and because it donates electrons it is also called an electron donor oxidation and reduction properly refer to a change in oxidation number the actual transfer of electrons may never occur thus oxidation is better defined as an increase in oxidation number and reduction as a decrease in oxidation number
equilibrium
although the concept of equilibrium is widely used across sciences in the context of chemistry it arises whenever a number of different states of the chemical composition are possible as for example in a mixture of several chemical compounds that can react with one another or when a substance can be present in more than one kind of phase
a system of chemical substances at equilibrium even though having an unchanging composition is most often not static molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time
chemical laws
chemical reactions are governed by certain laws which have become fundamental concepts in chemistry some of them are
practice
subdisciplines
chemistry is typically divided into several major sub disciplines there are also several main cross disciplinary and more specialized fields of chemistry
other disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study these include inorganic chemistry the study of inorganic matter organic chemistry the study of organic carbon based matter biochemistry the study of substances found in biological organisms physical chemistry the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics and analytical chemistry the analysis of material samples to gain an understanding of their chemical composition and structure many more specialized disciplines have emerged in recent years e g neurochemistry the chemical study of the nervous system see subdisciplines
other fields include agrochemistry astrochemistry and cosmochemistry atmospheric chemistry chemical engineering chemical biology chemo informatics electrochemistry environmental chemistry femtochemistry flavor chemistry flow chemistry geochemistry green chemistry histochemistry history of chemistry hydrogenation chemistry immunochemistry marine chemistry materials science mathematical chemistry mechanochemistry medicinal chemistry molecular biology molecular mechanics nanotechnology natural product chemistry oenology organometallic chemistry petrochemistry pharmacology photochemistry physical organic chemistry phytochemistry polymer chemistry radiochemistry solid state chemistry sonochemistry supramolecular chemistry surface chemistry synthetic chemistry thermochemistry and many others
chemical industry
the chemical industry represents an important economic activity worldwide the global top chemical producers in had sales of us billion with a profit margin of

chemical physics

chemical physics is a subdiscipline of chemistry and physics that investigates physicochemical phenomena using techniques from atomic and molecular physics and condensed matter physics it is the branch of physics that studies chemical processes from the point of view of physics while at the interface of physics and chemistry chemical physics is distinct from physical chemistry in that it focuses more on the characteristic elements and theories of physics meanwhile physical chemistry studies the physical nature of chemistry nonetheless the distinction between the two fields is vague and workers often practice in both fields during the course of their research
what chemical physicists do
chemical physicists commonly probe the structure and dynamics of ions free radicals polymers clusters and molecules areas of study include the quantum mechanical behavior of chemical reactions the process of solvation inter and intra molecular energy flow and single entities such as quantum dots experimental chemical physicists use a variety of spectroscopic techniques to better understand hydrogen bonding electron transfer the formation and dissolution of chemical bonds chemical reactions and the formation of nanoparticles theoretical chemical physicists create simulations of the molecular processes probed in these experiments to both explain results and guide future investigations the goals of chemical physics research include understanding chemical structures and reactions at the quantum mechanical level elucidating the structure and reactivity of gas phase ions and radicals and discovering accurate approximations to make the physics of chemical phenomena computationally accessible chemical physicists are looking for answers to such questions as

atmospheric chemistry

atmospheric chemistry is a branch of atmospheric science in which the chemistry of the earth's atmosphere and that of other planets is studied it is a multidisciplinary field of research and draws on environmental chemistry physics meteorology computer modeling oceanography geology and volcanology and other disciplines research is increasingly connected with other areas of study such as climatology
the composition and chemistry of the atmosphere is of importance for several reasons but primarily because of the interactions between the atmosphere and living organisms the composition of the earth's atmosphere changes as result of natural processes such as volcano emissions lightning and bombardment by solar particles from corona it has also been changed by human activity and some of these changes are harmful to human health crops and ecosystems examples of problems which have been addressed by atmospheric chemistry include acid rain ozone depletion photochemical smog greenhouse gases and global warming atmospheric chemists seek to understand the causes of these problems and by obtaining a theoretical understanding of them allow possible solutions to be tested and the effects of changes in government policy evaluated
atmospheric composition
notes the concentration of co and ch vary by season and location the mean molecular mass of air is g mol
history
the ancient greeks regarded air as one of the four elements but the first scientific studies of atmospheric composition began in the th century chemists such as joseph priestley antoine lavoisier and henry cavendish made the first measurements of the composition of the atmosphere
in the late th and early th centuries interest shifted towards trace constituents with very small concentrations one particularly important discovery for atmospheric chemistry was the discovery of ozone by christian friedrich sch nbein in
in the th century atmospheric science moved on from studying the composition of air to a consideration of how the concentrations of trace gases in the atmosphere have changed over time and the chemical processes which create and destroy compounds in the air two particularly important examples of this were the explanation by sydney chapman and gordon dobson of how the ozone layer is created and maintained and the explanation of photochemical smog by arie jan haagen smit further studies on ozone issues led to the nobel prize in chemistry award shared between paul crutzen mario molina and frank sherwood rowland
in the st century the focus is now shifting again atmospheric chemistry is increasingly studied as one part of the earth system instead of concentrating on atmospheric chemistry in isolation the focus is now on seeing it as one part of a single system with the rest of the atmosphere biosphere and geosphere an especially important driver for this is the links between chemistry and climate such as the effects of changing climate on the recovery of the ozone hole and vice versa but also interaction of the composition of the atmosphere with the oceans and terrestrial ecosystems
methodology
observations lab measurements and modeling are the three central elements in atmospheric chemistry progress in atmospheric chemistry is often driven by the interactions between these components and they form an integrated whole for example observations may tell us that more of a chemical compound exists than previously thought possible this will stimulate new modelling and laboratory studies which will increase our scientific understanding to a point where the observations can be explained
observation
observations of atmospheric chemistry are essential to our understanding routine observations of chemical composition tell us about changes in atmospheric composition over time one important example of this is the keeling curve a series of measurements from to today which show a steady rise in of the concentration of carbon dioxide observations of atmospheric chemistry are made in observatories such as that on mauna loa and on mobile platforms such as aircraft e g the uk's facility for airborne atmospheric measurements ships and balloons observations of atmospheric composition are increasingly made by satellites with important instruments such as gome and mopitt giving a global picture of air pollution and chemistry surface observations have the advantage that they provide long term records at high time resolution but are limited in the vertical and horizontal space they provide observations from some surface based instruments e g lidar can provide concentration profiles of chemical compounds and aerosol but are still restricted in the horizontal region they can cover many observations are available on line in atmospheric chemistry observational databases
lab measurements
measurements made in the laboratory are essential to our understanding of the sources and sinks of pollutants and naturally occurring compounds lab studies tell us which gases react with each other and how fast they react measurements of interest include reactions in the gas phase on surfaces and in water also of high importance is photochemistry which quantifies how quickly molecules are split apart by sunlight and what the products are plus thermodynamic data such as henry's law coefficients
modeling
in order to synthesise and test theoretical understanding of atmospheric chemistry computer models such as chemical transport models are used numerical models solve the differential equations governing the concentrations of chemicals in the atmosphere they can be very simple or very complicated one common trade off in numerical models is between the number of chemical compounds and chemical reactions modelled versus the representation of transport and mixing in the atmosphere for example a box model might include hundreds or even thousands of chemical reactions but will only have a very crude representation of mixing in the atmosphere in contrast d models represent many of the physical processes of the atmosphere but due to constraints on computer resources will have far fewer chemical reactions and compounds models can be used to interpret observations test understanding of chemical reactions and predict future concentrations of chemical compounds in the atmosphere one important current trend is for atmospheric chemistry modules to become one part of earth system models in which the links between climate atmospheric composition and the biosphere can be studied
some models are constructed by automatic code generators e g autochem or kpp in this approach a set of constituents are chosen and the automatic code generator will then select the reactions involving those constituents from a set of reaction databases once the reactions have been chosen the ordinary differential equations ode that describe their time evolution can be automatically constructed

mechanochemistry

mechanochemistry or mechanical chemistry is the coupling of mechanical and chemical phenomena on a molecular scale and includes mechanical breakage chemical behaviour of mechanically stressed solids e g stress corrosion cracking or enhanced oxidation tribology polymer degradation under shear cavitation related phenomena e g sonochemistry and sonoluminescence shock wave chemistry and physics and even the burgeoning field of molecular machines mechanochemistry can be seen as an interface between chemistry and mechanical engineering it is possible to synthesize chemical products by using only mechanical action the mechanisms of mechanochemical transformations are often complex and different from usual thermal or photochemical mechanisms the method of ball milling is a widely used process in which mechanical force is used to achieve chemical processing and transformations the special issue of chemical society review vol is dedicated to the theme of mechanochemistry fundamentals and applications ranging from nano materials to technology have been reviewed the mechanochemical process was used recently as a green one to synthesize pharmaceutically attractive phenol hydrazones
the term mechanochemistry is sometimes confused with mechanosynthesis which refers specifically to the machine controlled construction of complex molecular products
mechanochemical phenomena have been utilized since time immemorial for example in making fire the oldest method of making fire is to rub pieces of wood against each other creating friction and hence heat allowing the wood to undergo combustion at a high temperature another method involves the use of flint and steel during which a spark a small particle of pyrophoric metal spontaneously combusts in air starting fire instantaneously
references
lenhardt j m ong m t choe r evenhuis c r martinez t j craig s l trapping a diradical transition state by mechanochemical polymer extension science

atom

an atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element every solid liquid gas and plasma is made up of neutral or ionized atoms atoms are very small typical sizes are around pm a ten billionth of a meter in the short scale however atoms do not have well defined boundaries and there are different ways to define their size which give different but close values
atoms are small enough that classical physics give noticeably incorrect results through the development of physics atomic models have incorporated quantum principles to better explain and predict the behavior
every atom is composed of a nucleus and one or more electrons bound to the nucleus the nucleus is made of one or more protons and typically a similar number of neutrons none in hydrogen protons and neutrons are called nucleons over of the atom's mass is in the nucleus the protons have a positive electric charge the electrons have a negative electric charge and the neutrons have no electric charge if the number of protons and electrons are equal that atom is electrically neutral if an atom has more or less electrons than protons then it has an overall negative or positive charge respectively and it is called an ion
electrons of an atom are attracted to the protons in an atomic nucleus by this electromagnetic force the protons and neutrons in the nucleus are attracted to each other by a different force the nuclear force which is usually stronger than the electromagnetic force repelling the positively charged protons from one another under certain circumstances the repelling electromagnetic force becomes stronger than the nuclear force and nucleons can be ejected from the nucleus leaving behind a different element nuclear decay resulting in nuclear transmutation
the number of protons in the nucleus defines to what chemical element the atom belongs for example all copper atoms contain protons the number of neutrons defines the isotope of the element the number of electrons influences the magnetic properties of an atom atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules the ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature and is the subject of the discipline of chemistry
not all the matter of the universe is composed of atoms dark matter comprises more of the universe than matter and is composed not of atoms but of particles of a currently unknown type
history of atomic theory
atoms in philosophy
the idea that matter is made up of discrete units is a very old one appearing in many ancient cultures such as greece and india the word atom in fact was coined by ancient greek philosophers however these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation as a result their views on what atoms look like and how they behave were incorrect they also couldn't convince everybody so atomism was but one of a number of competing theories on the nature of matter it wasn't until the th century that the idea was embraced and refined by scientists when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain
first evidence based theory
in the early s john dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers the law of multiple proportions for instance there are two types of tin oxide one is tin and oxygen and the other is tin and oxygen tin ii oxide and tin dioxide respectively this means that g of tin will combine either with g or g of oxygen and form a ratio of a ratio of small whole numbers this common pattern in chemistry suggested to dalton that elements react in whole number multiples of discrete units in other words atoms in the case of tin oxides one tin atom will combine with either one or two oxygen atoms
dalton also believed atomic theory could explain why water absorbs different gases in different proportions for example he found that water absorbs carbon dioxide far better than it absorbs nitrogen dalton hypothesized this was due to the differences in mass and complexity of the gases respective particles indeed carbon dioxide molecules co are heavier and larger than nitrogen molecules n
brownian motion
in botanist robert brown used a microscope to look at dust grains floating in water and discovered that they moved about erratically a phenomenon that became known as brownian motion this was thought to be caused by water molecules knocking the grains about in albert einstein produced the first mathematical analysis of the motion french physicist jean perrin used einstein's work to experimentally determine the mass and dimensions of atoms thereby conclusively verifying dalton's atomic theory
discovery of the electron
the physicist j j thomson measured the mass of cathode rays showing they were made of particles but were around times lighter than the lightest atom hydrogen therefore they were not atoms but a new particle the first subatomic particle to be discovered which he originally called corpuscle but was later named electron after particles postulated by george johnstone stoney in he also showed they were identical to particles given off by photoelectric and radioactive materials it was quickly recognized that they are the particles that carry electric currents in metal wires and carry the negative electric charge within atoms thomson was given the nobel prize for physics for this work thus he overturned the belief that atoms are the indivisible ultimate particles of matter thomson also incorrectly postulated that the low mass negatively charged electrons were distributed throughout the atom in a uniform sea of positive charge this became known as the plum pudding model
discovery of the nucleus
in hans geiger and ernest marsden under the direction of ernest rutherford bombarded a metal foil with alpha particles to observe how they scattered they expected all the alpha particles to pass straight through with little deflection because thomson's model said that the charges in the atom are so diffuse that their electric fields could not affect the alpha particles much however geiger and marsden spotted alpha particles being deflected by angles greater than which was supposed to be impossible according to thomson's model to explain this rutherford proposed that the positive charge of the atom is concentrated in a tiny nucleus at the center of the atom
discovery of isotopes
while experimenting with the products of radioactive decay in radiochemist frederick soddy discovered that there appeared to be more than one type of atom at each position on the periodic table the term isotope was coined by margaret todd as a suitable name for different atoms that belong to the same element j j thomson created a technique for separating atom types through his work on ionized gases which subsequently led to the discovery of stable isotopes
bohr model
in the physicist niels bohr proposed a model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of a photon this quantization was used to explain why the electrons orbits are stable given that normally charges in acceleration including circular motion lose kinetic energy which is emitted as electromagnetic radiation see synchrotron radiation and why elements absorb and emit electromagnetic radiation in discrete spectra
later in the same year henry moseley provided additional experimental evidence in favor of niels bohr's theory these results refined ernest rutherford's and antonius van den broek's model which proposed that the atom contains in its nucleus a number of positive nuclear charges that is equal to its atomic number in the periodic table until these experiments atomic number was not known to be a physical and experimental quantity that it is equal to the atomic nuclear charge remains the accepted atomic model today
chemical bonding explained
chemical bonds between atoms were now explained by gilbert newton lewis in as the interactions between their constituent electrons as the chemical properties of the elements were known to largely repeat themselves according to the periodic law in the american chemist irving langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner groups of electrons were thought to occupy a set of electron shells about the nucleus
further developments in quantum physics
the stern gerlach experiment of provided further evidence of the quantum nature of the atom when a beam of silver atoms was passed through a specially shaped magnetic field the beam was split based on the direction of an atom's angular momentum or spin as this direction is random the beam could be expected to spread into a line instead the beam was split into two parts depending on whether the atomic spin was oriented up or down
in louis de broglie proposed that all particles behave to an extent like waves in erwin schr dinger used this idea to develop a mathematical model of the atom that described the electrons as three dimensional waveforms rather than point particles a consequence of using waveforms to describe particles is that it is physically impossible to obtain precise values for both the position and momentum of a particle at the same time this became known as the uncertainty principle formulated by werner heisenberg in in this concept for a given accuracy in measuring a position one could only obtain a range of probable values for momentum and vice versa this model was able to explain observations of atomic behavior that previous models could not such as certain structural and spectral patterns of atoms larger than hydrogen thus the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed
discovery of the neutron
the development of the mass spectrometer allowed the mass of atoms to be measured with increased accuracy the device uses a magnet to bend the trajectory of a beam of ions and the amount of deflection is determined by the ratio of an atom's mass to its charge the chemist francis william aston used this instrument to show that isotopes had different masses the atomic mass of these isotopes varied by integer amounts called the whole number rule the explanation for these different isotopes awaited the discovery of the neutron an uncharged particle with a mass similar to the proton by the physicist james chadwick in isotopes were then explained as elements with the same number of protons but different numbers of neutrons within the nucleus
fission high energy physics and condensed matter
in the german chemist otto hahn a student of rutherford directed neutrons onto uranium atoms expecting to get transuranium elements instead his chemical experiments showed barium as a product a year later lise meitner and her nephew otto frisch verified that hahn's result were the first experimental nuclear fission in hahn received the nobel prize in chemistry despite hahn's efforts the contributions of meitner and frisch were not recognized
in the s the development of improved particle accelerators and particle detectors allowed scientists to study the impacts of atoms moving at high energies neutrons and protons were found to be hadrons or composites of smaller particles called quarks the standard model of particle physics was developed that so far has successfully explained the properties of the nucleus in terms of these sub atomic particles and the forces that govern their interactions
structure
subatomic particles
though the word atom originally denoted a particle that cannot be cut into smaller particles in modern scientific usage the atom is composed of various subatomic particles the constituent particles of an atom are the electron the proton and the neutron all three are fermions however the hydrogen atom has no neutrons and the hydron ion has no electrons
the electron is by far the least massive of these particles at with a negative electrical charge and a size that is too small to be measured using available techniques it is the lightest particle with a positive rest mass measured under ordinary conditions electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges if an atom has more or fewer electrons than its atomic number then it becomes respectively negatively or positively charged as a whole a charged atom is called an ion electrons have been known since the late th century mostly thanks to j j thomson see history of subatomic physics for details
protons have a positive charge and a mass times that of the electron at the number of protons in an atom is called its atomic number ernest rutherford observed that nitrogen under alpha particle bombardment ejects what appeared to be hydrogen nuclei by he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton
neutrons have no electrical charge and have a free mass of times the mass of the electron or the heaviest of the three constituent particles but it can be reduced by the nuclear binding energy neutrons and protons collectively known as nucleons have comparable dimensions on the order of although the surface of these particles is not sharply defined the neutron was discovered in by the english physicist james chadwick
in the standard model of physics electrons are truly elementary particles with no internal structure however both protons and neutrons are composite particles composed of elementary particles called quarks there are two types of quarks in atoms each having a fractional electric charge protons are composed of two up quarks each with charge and one down quark with a charge of neutrons consist of one up quark and two down quarks this distinction accounts for the difference in mass and charge between the two particles
the quarks are held together by the strong interaction or strong force which is mediated by gluons the protons and neutrons in turn are held to each other in the nucleus by the nuclear force which is a residuum of the strong force that has somewhat different range properties see the article on the nuclear force for more the gluon is a member of the family of gauge bosons which are elementary particles that mediate physical forces
nucleus
all the bound protons and neutrons in an atom make up a tiny atomic nucleus and are collectively called nucleons the radius of a nucleus is approximately equal to fm where a is the total number of nucleons this is much smaller than the radius of the atom which is on the order of fm the nucleons are bound together by a short ranged attractive potential called the residual strong force at distances smaller than fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other
atoms of the same element have the same number of protons called the atomic number within a single element the number of neutrons may vary determining the isotope of that element the total number of protons and neutrons determine the nuclide the number of neutrons relative to the protons determines the stability of the nucleus with certain isotopes undergoing radioactive decay
the proton the electron and the neutron are classified as fermions fermions obey the pauli exclusion principle which prohibits identical fermions such as multiple protons from occupying the same quantum state at the same time thus every proton in the nucleus must occupy a quantum state different from all other protons and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud however a proton and a neutron are allowed to occupy the same quantum state
for atoms with low atomic numbers a nucleus that has more neutrons than protons tends to drop to a lower energy state through radioactive decay so that the neutron proton ratio is closer to one however as the atomic number increases a higher proportion of neutrons is required to offset the mutual repulsion of the protons thus there are no stable nuclei with equal proton and neutron numbers above atomic number z calcium and as z increases the neutron proton ratio of stable isotopes increases the stable isotope with the highest proton neutron ratio is lead about
the number of protons and neutrons in the atomic nucleus can be modified although this can require very high energies because of the strong force nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus such as through the energetic collision of two nuclei for example at the core of the sun protons require energies of kev to overcome their mutual repulsion the coulomb barrier and fuse together into a single nucleus nuclear fission is the opposite process causing a nucleus to split into two smaller nuclei usually through radioactive decay the nucleus can also be modified through bombardment by high energy subatomic particles or photons if this modifies the number of protons in a nucleus the atom changes to a different chemical element
if the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles then the difference between these two values can be emitted as a type of usable energy such as a gamma ray or the kinetic energy of a beta particle as described by albert einstein's mass energy equivalence formula e mc where m is the mass loss and c is the speed of light this deficit is part of the binding energy of the new nucleus and it is the non recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate
the fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel a total nucleon number of about is usually an exothermic process that releases more energy than is required to bring them together it is this energy releasing process that makes nuclear fusion in stars a self sustaining reaction for heavier nuclei the binding energy per nucleon in the nucleus begins to decrease that means fusion processes producing nuclei that have atomic numbers higher than about and atomic masses higher than about is an endothermic process these more massive nuclei can not undergo an energy producing fusion reaction that can sustain the hydrostatic equilibrium of a star
electron cloud
the electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force this force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus which means that an external source of energy is needed for the electron to escape the closer an electron is to the nucleus the greater the attractive force hence electrons bound near the center of the potential well require more energy to escape than those at greater separations
electrons like other particles have properties of both a particle and a wave the electron cloud is a region inside the potential well where each electron forms a type of three dimensional standing wave a wave form that does not move relative to the nucleus this behavior is defined by an atomic orbital a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured only a discrete or quantized set of these orbitals exist around the nucleus as other possible wave patterns rapidly decay into a more stable form orbitals can have one or more ring or node structures and they differ from each other in size shape and orientation
each atomic orbital corresponds to a particular energy level of the electron the electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state likewise through spontaneous emission an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon these characteristic energy values defined by the differences in the energies of the quantum states are responsible for atomic spectral lines
the amount of energy needed to remove or add an electron the electron binding energy is far less than the binding energy of nucleons for example it requires only ev to strip a ground state electron from a hydrogen atom compared to million ev for splitting a deuterium nucleus atoms are electrically neutral if they have an equal number of protons and electrons atoms that have either a deficit or a surplus of electrons are called ions electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms by this mechanism atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals
properties
nuclear properties
by definition any two atoms with an identical number of protons in their nuclei belong to the same chemical element atoms with equal numbers of protons but a different number of neutrons are different isotopes of the same element for example all hydrogen atoms admit exactly one proton but isotopes exist with no neutrons hydrogen by far the most common form also called protium one neutron deuterium two neutrons tritium and more than two neutrons the known elements form a set of atomic numbers from the single proton element hydrogen up to the proton element ununoctium all known isotopes of elements with atomic numbers greater than are radioactive
about nuclides occur naturally on earth of which about have not been observed to decay and are referred to as stable isotopes however only of these nuclides are stable to all decay even in theory another bringing the total to have not been observed to decay even though in theory it is energetically possible these are also formally classified as stable an additional radioactive nuclides have half lives longer than million years and are long lived enough to be present from the birth of the solar system this collection of nuclides are known as primordial nuclides finally an additional short lived nuclides are known to occur naturally as daughter products of primordial nuclide decay such as radium from uranium or else as products of natural energetic processes on earth such as cosmic ray bombardment for example carbon
for of the chemical elements at least one stable isotope exists as a rule there is only a handful of stable isotopes for each of these elements the average being stable isotopes per element twenty six elements have only a single stable isotope while the largest number of stable isotopes observed for any element is ten for the element tin elements and all elements numbered or higher have no stable isotopes
stability of isotopes is affected by the ratio of protons to neutrons and also by the presence of certain magic numbers of neutrons or protons that represent closed and filled quantum shells these quantum shells correspond to a set of energy levels within the shell model of the nucleus filled shells such as the filled shell of protons for tin confers unusual stability on the nuclide of the known stable nuclides only four have both an odd number of protons and odd number of neutrons hydrogen deuterium lithium boron and nitrogen also only four naturally occurring radioactive odd odd nuclides have a half life over a billion years potassium vanadium lanthanum and tantalum m most odd odd nuclei are highly unstable with respect to beta decay because the decay products are even even and are therefore more strongly bound due to nuclear pairing effects
mass
the large majority of an atom's mass comes from the protons and neutrons that make it up the total number of these particles called nucleons in a given atom is called the mass number it is a positive integer and dimensionless instead of having dimension of mass because it expresses a count an example of use of a mass number is carbon which has nucleons six protons and six neutrons
the actual mass of an atom at rest is often expressed using the unified atomic mass unit u also called dalton da this unit is defined as a twelfth of the mass of a free neutral atom of carbon which is approximately hydrogen the lightest isotope of hydrogen which is also the nuclide with the lowest mass has an atomic weight of u the value of this number is called the atomic mass a given atom has an atomic mass approximately equal within to its mass number times the atomic mass unit for example the mass of a nitrogen is roughly u however this number will not be exactly an integer except in the case of carbon see below the heaviest stable atom is lead with a mass of
as even the most massive atoms are far too light to work with directly chemists instead use the unit of moles one mole of atoms of any element always has the same number of atoms about this number was chosen so that if an element has an atomic mass of u a mole of atoms of that element has a mass close to one gram because of the definition of the unified atomic mass unit each carbon atom has an atomic mass of exactly u and so a mole of carbon atoms weighs exactly kg
shape and size
atoms lack a well defined outer boundary so their dimensions are usually described in terms of an atomic radius this is a measure of the distance out to which the electron cloud extends from the nucleus however this assumes the atom to exhibit a spherical shape which is only obeyed for atoms in vacuum or free space atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond the radius varies with the location of an atom on the atomic chart the type of chemical bond the number of neighboring atoms coordination number and a quantum mechanical property known as spin on the periodic table of the elements atom size tends to increase when moving down columns but decrease when moving across rows left to right consequently the smallest atom is helium with a radius of pm while one of the largest is caesium at pm
when subjected to external forces like electrical fields the shape of an atom may deviate from spherical symmetry the deformation depends on the field magnitude and the orbital type of outer shell electrons as shown by group theoretical considerations aspherical deviations might be elicited for instance in crystals where large crystal electrical fields may occur at low symmetry lattice sites significant ellipsoidal deformations have recently been shown to occur for sulfur ions and chalcogen ions in pyrite type compounds
atomic dimensions are thousands of times smaller than the wavelengths of light nm so they cannot be viewed using an optical microscope however individual atoms can be observed using a scanning tunneling microscope to visualize the minuteness of the atom consider that a typical human hair is about million carbon atoms in width a single drop of water contains about sextillion atoms of oxygen and twice the number of hydrogen atoms a single carat diamond with a mass of contains about sextillion atoms of carbon if an apple were magnified to the size of the earth then the atoms in the apple would be approximately the size of the original apple
radioactive decay
every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay causing the nucleus to emit particles or electromagnetic radiation radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force which only acts over distances on the order of fm
the most common forms of radioactive decay are
other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus or more than one beta particle an analog of gamma emission which allows excited nuclei to lose energy in a different way is internal conversion a process that produces high speed electrons that are not beta rays followed by production of high energy photons that are not gamma rays a few large nuclei explode into two or more charged fragments of varying masses plus several neutrons in a decay called spontaneous nuclear fission
each radioactive isotope has a characteristic decay time period the half life that is determined by the amount of time needed for half of a sample to decay this is an exponential decay process that steadily decreases the proportion of the remaining isotope by every half life hence after two half lives have passed only of the isotope is present and so forth
magnetic moment
elementary particles possess an intrinsic quantum mechanical property known as spin this is analogous to the angular momentum of an object that is spinning around its center of mass although strictly speaking these particles are believed to be point like and cannot be said to be rotating spin is measured in units of the reduced planck constant with electrons protons and neutrons all having spin or spin in an atom electrons in motion around the nucleus possess orbital angular momentum in addition to their spin while the nucleus itself possesses angular momentum due to its nuclear spin
the magnetic field produced by an atom its magnetic moment is determined by these various forms of angular momentum just as a rotating charged object classically produces a magnetic field however the most dominant contribution comes from electron spin due to the nature of electrons to obey the pauli exclusion principle in which no two electrons may be found in the same quantum state bound electrons pair up with each other with one member of each pair in a spin up state and the other in the opposite spin down state thus these spins cancel each other out reducing the total magnetic dipole moment to zero in some atoms with even number of electrons
in ferromagnetic elements such as iron cobalt and nickel an odd number of electrons leads to an unpaired electron and a net overall magnetic moment the orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other a spontaneous process known as an exchange interaction when the magnetic moments of ferromagnetic atoms are lined up the material can produce a measurable macroscopic field paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present but the magnetic moments of the individual atoms line up in the presence of a field
the nucleus of an atom will have no spin when it has even numbers of both neutrons and protons but for other cases of odd numbers the nucleus may have a spin normally nuclei with spin are aligned in random directions because of thermal equilibrium however for certain elements such as xenon it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction a condition called hyperpolarization this has important applications in magnetic resonance imaging
energy levels
the potential energy of an electron in an atom is negative its dependence of its position reaches the minimum the most absolute value inside the nucleus and vanishes when the distance from the nucleus goes to infinity roughly in an inverse proportion to the distance in the quantum mechanical model a bound electron can only occupy a set of states centered on the nucleus and each state corresponds to a specific energy level see time independent schr dinger equation for theoretical explanation an energy level can be measured by the amount of energy needed to unbind the electron from the atom and is usually given in units of electronvolts ev the lowest energy state of a bound electron is called the ground state i e stationary state while an electron transition to a higher level results in an excited state the electron's energy raises when n increases because the average distance to the nucleus increases dependence of the energy on is caused not by electrostatic potential of the nucleus but by interaction between electrons
for an electron to transition between two different states e g grounded state to first excited level ionization it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels according to niels bohr model what can be precisely calculated by the schr dinger equation
electrons jump between orbitals in a particle like fashion for example if a single photon strikes the electrons only a single electron changes states in response to the photon see electron properties
the energy of an emitted photon is proportional to its frequency so these specific energy levels appear as distinct bands in the electromagnetic spectrum each element has a characteristic spectrum that can depend on the nuclear charge subshells filled by electrons the electromagnetic interactions between the electrons and other factors
when a continuous spectrum of energy is passed through a gas or plasma some of the photons are absorbed by atoms causing electrons to change their energy level those excited electrons that remain bound to their atom spontaneously emit this energy as a photon traveling in a random direction and so drop back to lower energy levels thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output an observer viewing the atoms from a view that does not include the continuous spectrum in the background instead sees a series of emission lines from the photons emitted by the atoms spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined
close examination of the spectral lines reveals that some display a fine structure splitting this occurs because of spin orbit coupling which is an interaction between the spin and motion of the outermost electron when an atom is in an external magnetic field spectral lines become split into three or more components a phenomenon called the zeeman effect this is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons some atoms can have multiple electron configurations with the same energy level which thus appear as a single spectral line the interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels resulting in multiple spectral lines the presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels a phenomenon called the stark effect
if a bound electron is in an excited state an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level for this to occur the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon the emitted photon and the interacting photon then move off in parallel and with matching phases that is the wave patterns of the two photons are synchronized this physical property is used to make lasers which can emit a coherent beam of light energy in a narrow frequency band
valence and bonding behavior
valency is the combining power of an element it is equal to number of hydrogen atoms that atom can combine or displace in forming compounds the outermost electron shell of an atom in its uncombined state is known as the valence shell and the electrons in
that shell are called valence electrons the number of valence electrons determines the bonding
behavior with other atoms atoms tend to chemically react with each other in a manner that fills or empties their outer valence shells for example a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one electron more than a filled shell and others that are one electron short of a full shell such as occurs in the compound sodium chloride and other chemical ionic salts however many elements display multiple valences or tendencies to share differing numbers of electrons in different compounds thus chemical bonding between these elements takes many forms of electron sharing that are more than simple electron transfers examples include the element carbon and the organic compounds
the chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties and elements with the same number of valence electrons form a group that is aligned in the same column of the table the horizontal rows correspond to the filling of a quantum shell of electrons the elements at the far right of the table have their outer shell completely filled with electrons which results in chemically inert elements known as the noble gases
states
quantities of atoms are found in different states of matter that depend on the physical conditions such as temperature and pressure by varying the conditions materials can transition between solids liquids gases and plasmas within a state a material can also exist in different allotropes an example of this is solid carbon which can exist as graphite or diamond gaseous allotropes exist as well such as dioxygen and ozone
at temperatures close to absolute zero atoms can form a bose einstein condensate at which point quantum mechanical effects which are normally only observed at the atomic scale become apparent on a macroscopic scale this super cooled collection of atoms
then behaves as a single super atom which may allow fundamental checks of quantum mechanical behavior
identification
the scanning tunneling microscope is a device for viewing surfaces at the atomic level it uses the quantum tunneling phenomenon which allows particles to pass through a barrier that would normally be insurmountable electrons tunnel through the vacuum between two planar metal electrodes on each of which is an adsorbed atom providing a tunneling current density that can be measured scanning one atom taken as the tip as it moves past the other the sample permits plotting of tip displacement versus lateral separation for a constant current the calculation shows the extent to which scanning tunneling microscope images of an individual atom are visible it confirms that for low bias the microscope images the space averaged dimensions of the electron orbitals across closely packed energy levels the fermi level local density of states
an atom can be ionized by removing one of its electrons the electric charge causes the trajectory of an atom to bend when it passes through a magnetic field the radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom the mass spectrometer uses this principle to measure the mass to charge ratio of ions if a sample contains multiple isotopes the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry both of which use a plasma to vaporize samples for analysis
a more area selective method is electron energy loss spectroscopy which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample the atom probe tomograph has sub nanometer resolution in d and can chemically identify individual atoms using time of flight mass spectrometry
spectra of excited states can be used to analyze the atomic composition of distant stars specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms these colors can be replicated using a gas discharge lamp containing the same element helium was discovered in this way in the spectrum of the sun years before it was found on earth
origin and current state
atoms form about of the total energy density of the observable universe with an average density of about atoms m within a galaxy such as the milky way atoms have a much higher concentration with the density of matter in the interstellar medium ism ranging from to atoms m the sun is believed to be inside the local bubble a region of highly ionized gas so the density in the solar neighborhood is only about atoms m stars form from dense clouds in the ism and the evolutionary processes of stars result in the steady enrichment of the ism with elements more massive than hydrogen and helium up to of the milky way's atoms are concentrated inside stars and the total mass of atoms forms about of the mass of the galaxy the remainder of the mass is an unknown dark matter
formation
electrons are thought to exist in the universe since early stages of the big bang atomic nuclei forms in nucleosynthesis reactions in about three minutes big bang nucleosynthesis produced most of the helium lithium and deuterium in the universe and perhaps some of the beryllium and boron
ubiquitousness and stability of atoms relies on their binding energy which means that an atom has a lower energy than an unbound system of the nucleus and electrons where the temperature is much higher than ionization potential the matter exists in the form of plasma a gas of positively charged ions possibly bare nuclei and electrons when the temperature drops below the ionization potential atoms become statistically favorable atoms complete with bound electrons became to dominate over charged particles years after the big bang an epoch called recombination when the expanding universe cooled enough to allow electrons to become attached to nuclei
since the big bang which produced no carbon or heavier elements atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium and via the triple alpha process the sequence of elements from carbon up to iron see stellar nucleosynthesis for details
isotopes such as lithium as well as some beryllium and boron are generated in space through cosmic ray spallation this occurs when a high energy proton strikes an atomic nucleus causing large numbers of nucleons to be ejected
elements heavier than iron were produced in supernovae through the r process and in agb stars through the s process both of which involve the capture of neutrons by atomic nuclei elements such as lead formed largely through the radioactive decay of heavier elements
earth
most of the atoms that make up the earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the solar system the rest are the result of radioactive decay and their relative proportion can be used to determine the age of the earth through radiometric dating most of the helium in the crust of the earth about of the helium from gas wells as shown by its lower abundance of helium is a product of alpha decay
there are a few trace atoms on earth that were not present at the beginning i e not primordial nor are results of radioactive decay carbon is continuously generated by cosmic rays in the atmosphere some atoms on earth have been artificially generated either deliberately or as by products of nuclear reactors or explosions of the transuranic elements those with atomic numbers greater than only plutonium and neptunium occur naturally on earth transuranic elements have radioactive lifetimes shorter than the current age of the earth and thus identifiable quantities of these elements have long since decayed with the exception of traces of plutonium possibly deposited by cosmic dust natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore
the earth contains approximately atoms although small numbers of independent atoms of noble gases exist such as argon neon and helium of the atmosphere is bound in the form of molecules including carbon dioxide and diatomic oxygen and nitrogen at the surface of the earth an overwhelming majority of atoms combine to form various compounds including water salt silicates and oxides atoms can also combine to create materials that do not consist of discrete molecules including crystals and liquid or solid metals this atomic matter forms networked arrangements that lack the particular type of small scale interrupted order associated with molecular matter
rare and theoretical forms
superheavy elements
while isotopes with atomic numbers higher than lead are known to be radioactive an island of stability has been proposed for some elements with atomic numbers above these superheavy elements may have a nucleus that is relatively stable against radioactive decay the most likely candidate for a stable superheavy atom unbihexium has protons and neutrons
exotic matter
each particle of matter has a corresponding antimatter particle with the opposite electrical charge thus the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton when a matter and corresponding antimatter particle meet they annihilate each other because of this along with an imbalance between the number of matter and antimatter particles the latter are rare in the universe the first causes of this imbalance are not yet fully understood although theories of baryogenesis may offer an explanation as a result no antimatter atoms have been discovered in nature however in the antimatter counterpart of the hydrogen atom antihydrogen was synthesized at the cern laboratory in geneva
other exotic atoms have been created by replacing one of the protons neutrons or electrons with other particles that have the same charge for example an electron can be replaced by a more massive muon forming a muonic atom these types of atoms can be used to test the fundamental predictions of physics

soft chemistry

soft chemistry also known as chimie douce is a type of chemistry that uses reactions at ambient temperature in open reaction vessels with reactions similar to those occurring in biological systems
aims
the aim of the soft chemistry is to synthesize materials drawing capacity of living beings more or less basic such as diatoms capable of producing glass from silicates dissolved it is a new branch of materials science that differs from conventional solid state chemistry and its application to the intense energy to explore the chemical inventiveness of the living world this specialty emerged in the s around the label of chimie douce which was first published by the french chemist in le monde october french hits the term soft chemistry is employed as such in the early twenty first century in publications scientific english and others his mode of synthesis is similar generally for reactions involved in the polymerizations based on organic and the establishment of solutions reactive energy intake without essential polycondensation the fundamental interest of this kind of polymerization mineral obtained at room temperature is to preserve organic molecules or microorganisms that wishes to fit the products obtained by means of the so called soft chemistry sol gel can be stored in several types
the early results have included the creation of glasses and ceramic with new properties these different structures are more or less composite mobilized a wide range of applications ranging from health to the needs of the conquest of space beyond its mode of synthesis a compound with the label soft chemistry combines the advantages of the mineral resistance transparency repetition patterns etc and now exploring the potential of the biochemistry and organic chemistry interface with the organic world reactivity synthesis capability etc according to its practitionners the soft chemistry is only in its early success and opens up vast prospects

microscale chemistry

microscale chemistry often referred to as small scale chemistry in german is an analytical method and also a teaching method widely used at school and at university levels working with small quantities of chemical substances while much of traditional chemistry teaching centers on multi gramme preparations milligrammes of substances are sufficient for microscale chemistry in universities modern and expensive lab glassware is used and modern methods for detection and characterization of the produced substances are very common in schools and in many countries of the southern hemisphere small scale working takes place with low cost and even no cost material there has always been a place for small scale working in qualitative analysis but the new developments can encompass much of chemistry a student is likely to meet
history
there are two main strands of the modern approach
one is based on the idea that many of the experiments associated with general chemistry acids and bases oxidation and reduction electrochemistry etc can be carried out in equipment much simpler injection bottles dropper bottles syringes wellplates plastic pipettes and therefore cheaper than the traditional glassware in a laboratory thus enabling the expansion of the laboratory experiences of students in large classes and to introduce laboratory work into institutions too poorly equipped for standard type work pioneering development in this area was carried out by egerton c grey mahmoud k el marsafy in egypt stephen thompson in the us and others a further application of these ideas was the devising by bradley of the radmaste kits in south africa designed to make effective chemical experiments possible in developing countries in schools that lack the technical services electricity running water taken for granted in many places
the other strand is the introduction of this approach into synthetic work mainly in organic chemistry here the crucial breakthrough was achieved by mayo pike and butcher and by williamson who demonstrated that inexperienced students were able to carry out organic syntheses on a few tens of milligrams a skill previously thought to require years of training and experience these approaches were accompanied by the introduction of some specialised equipment which was subsequently simplified by breuer without great loss of versatility
there is a great deal of published material available to help in the introduction of such a scheme providing advice on choice of equipment techniques and preparative experiments and the flow of such material is continuing through a column in the journal of chemical education called the microscale laboratory that has been running for many years
scaling down experiments when combined with modern projection technology opened up the possibility of carrying out lecture demonstrations of the most hazardous kind in total safety
the approach has been adopted world wide it has become a major presence on the educational scene in the us it is used to a lesser extent in the uk and it is used in many countries in institutions with staff who are enthusiastic about it
microscale chemistry conferences
st international symposium on microscale chemistry
may at universidad iberoamericana ciudad de mexico
nd international symposium on microscale chemistry
december at hong kong baptist university hong kong
rd international symposium on microscale chemistry
may at universidad iberoamericana ciudad de mexico

phytochemistry

phytochemistry is in the strict sense of the word the study of phytochemicals these are chemicals derived from plants in a narrower sense the terms are often used to describe the large number of secondary metabolic compounds found in plants many of these are known to provide protection against insect attacks and plant diseases they also exhibit a number of protective functions for human consumers
phytochemistry can be considered sub fields of botany or chemistry activities can be led in botanical gardens or in the wild with the aid of ethnobotany the applications of the discipline can be for pharmacognosy or the discovery of new drugs or as an aid for plant physiology studies
techniques
techniques commonly used in the field of phytochemistry are extraction isolation and structural elucidation ms d and d nmr of natural products as well as various chromatography techniques mplc hplc lc ms
constituent elements
the list of simple elements of which plants are primarily constructed carbon oxygen hydrogen calcium phosphorus etc is not different from similar lists for animals fungi or even bacteria the fundamental atomic components of plants are the same as for all life only the details of the way in which they are assembled differs
the following tables list elements essential to plants uses within plants are generalized
eastern medicine
phytochemistry is widely used in the field of chinese medicine especially in the field of herbal medicine
phytochemical technique mainly applies to the quality control of chinese medicine ayurvedic medicine indian traditional medicine or herbal medicine of various chemical components such as saponins alkaloids volatile oils flavonoids and anthraquinones in the development of rapid and reproducible analytical techniques the combination of hplc with different detectors such as diode array detector dad refractive index detector rid evaporative light scattering detector elsd and mass spectrometric detector msd has been widely developed
in most cases biologically active compounds in chinese medicine ayurveda or herbal medicine have not been determined therefore it is important to use the phytochemical methods to screen and analyze bioactive components not only for the quality control of crude drugs but also for the elucidation of their therapeutic mechanisms modern pharmacological studies indicate that binding to receptors or ion channels on cell membranes is the first step of some drug actions a new method in phytochemistry called biochromatography has been developed this method combines human red cell membrane extraction and high performance liquid chromatography to screen potential active components in chinese medicine

